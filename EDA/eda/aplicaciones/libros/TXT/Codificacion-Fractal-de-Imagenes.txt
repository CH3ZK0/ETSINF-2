CODIFICACIÓN FRACTAL
DE IMÁGENES

Juan Antonio Pérez Ortiz
septiembre 1997 – julio 1998

Esta obra está bajo una licencia Reconocimiento-No comercial 2.5 de
Creative Commons. Para ver una copia de esta licencia, visite
http://creativecommons.org/licenses/by-nc/2.5/ o envı́e una carta a
Creative Commons, 559 Nathan Abbott Way, Stanford, California 94305,
USA.

Prólogo
La geometrı́a fractal, cuyos primeros desarrollos datan de finales del siglo
pasado, ha recibido durante los últimos veinte años, desde la publicación de
los trabajos de Mandelbrot, una atención y un auge crecientes. Lejos de
ser simplemente una herramienta de generación de impresionantes paisajes
virtuales, la geometrı́a fractal viene avalada por la teorı́a geométrica de la
medida y por innumerables aplicaciones en ciencias tan dispares como la
Fı́sica, la Quı́mica, la Economı́a o, incluso, la Informática.
Dentro de esta corriente, la teorı́a matemática denominada sistemas de
funciones iteradas, desarrollada en 1981 por Hutchinson, se convirtió a finales de la década de los 80 con los trabajos de Barnsley en una de las técnicas
más innovadoras y prometedoras en el campo de la compresión de imágenes.
Aunque las expectativas iniciales fueron de alguna manera exageradas (situación ésta que también sobreestimó en sus inicios a técnicas como la lógica
difusa o las redes neuronales), el paso de los años ha ido configurando una
teorı́a factible que comienza ya a dar sus primeros productos comerciales.
Aunque el principal cometido de este trabajo es conocer el estado actual
de la codificación fractal y los principios matemáticos que la configuran, no
por ello deja de ser también una revisión detallada de los fundamentos de
la geometrı́a fractal.
Al mundo de los fractales muchos llegan (el autor incluido) atraidos por
el estallido de color de alguna representación del conjunto de Mandelbrot o
de un conjunto de Julia. Sin embargo, una vez que se profundiza en la magia
de los fractales, uno no sabe que admirar más, si las cascadas multicolor o
la belleza de las matemáticas que las engendran. En esta obra los turistas
que se asomen por primera vez a este mundo encontrarán una introducción
desde cero a los principios elementales de la revolución fractal.
Hay también quienes disfrutan desde hace ya tiempo generando intrincados bosques fractales o fotografı́as a profundidades abismales tras iterar
fórmulas sencillas mediante programas confeccionados, incluso, por ellos mismos. Algunos serán los geómetras del siglo XXI armados esta vez de ordenai

ii

PRÓLOGO

dores cada vez más rápidos que, aun ası́, cuando se trata de fractales, siempre
parecen quedarse pequeños. Con todo, y quizá debido al nivel superficial con
el que muchas publicaciones divulgativas afrontan el tema, muchas veces se
desconoce la teorı́a matemática que aguarda tras cada fractal. Conceptos
como los de dimensión de Hausdorff o conjunto autosemejante son de vital
importancia para abordar con ciertas garantı́as de éxito la exploración de
nuevos continentes fractales y se mostrarán aquı́ detalladamente.
Por último, hay quienes pueden acercarse a este proyecto para ampliar
sus conocimientos sobre la compresión de imágenes con pérdidas. En realidad, la primera parte de la obra trata de crear un clima adecuado para
poder abordar la codificación fractal de imágenes, una de las tecnologı́as de
compresión más en boga en los últimos años. Aunque el mayor peso recae
sobre el esquema de compresión fractal, no quedan sin analizar con cierto
detalle otros enfoques alternativos, principalmente los basados en wavelets
y en la transformada discreta del coseno.

Contenido de la obra
Los requisitos para acceder a esta obra son mı́nimos ya que basta con que
el lector conozca suficientemente algunas de las herramientas proporcionadas
por un curso inicial de cálculo. Aun ası́, se presentan cuando es necesario
la mayor parte de los conceptos utilizados y cuando esto no es posible,
por requerir una gran cantidad de información, se remite al lector a las
referencias oportunas. De cualquier forma lo anterior ocurre con aspectos
nunca básicos del trabajo cuya no completa asimilación no debe repercutir
en la comprensión del resto de la obra. Se ha tratado de realizar un trabajo
lo más autocontenido posible.
El capı́tulo 1 presenta una introducción a las ideas básicas de la geometrı́a fractal. Se realiza allı́ una revisión histórica del estudio de los conjuntos
fractales a la vez que se presenta el comportamiento de los sistemas caóticos,
primos hermanos de los fractales. Como elementos ineludibles se presentan
la constante de Feigenbaum, los conjuntos de Julia y el conjunto de Mandelbrot.
En el segundo capı́tulo se realiza un primer acercamiento serio al concepto de autosemejanza, crucial en la geometrı́a fractal. Este acercamiento
se realiza de la mano de uno de los enfoques estructurales más elegantes con
los que describir los fractales, los sistemas L.
El tercer capı́tulo muestra la teorı́a de conjuntos autosemejantes de Hutchinson, quizá la más consolidada hoy dı́a. Antes de afrontarla se describen

PRÓLOGO

iii

todos los conceptos fundamentales de la teorı́a de espacios métricos en los
que se sustenta, especialmente el teorema del punto fijo.
Los sistemas de funciones iteradas del capı́tulo 4 son la base de las técnicas actuales de compresión fractal. Estos sistemas generalizan la concepción
de autosemejanza del capı́tulo anterior, constituyendo la herramienta básica
para la aproximación mediante fractales de figuras reales. El teorema del
collage, como culminación del capı́tulo, asegura que bajo ciertas condiciones
esto es posible.
Antes de utilizar los fractales para la compresión de imágenes, el capı́tulo 5 estudia los conceptos generales de tal compresión a la vez que presenta
esquemas alternativos como la cuantización vectorial o los basados en transformadas como la del coseno o la transformada con wavelets.
Por fin es posible abordar con seguridad la compresión fractal de imágenes, denominada también transformada fractal. Esto se lleva a cabo en el
capı́tulo 6 donde se presenta un esquema básico, de fácil comprensión, pero
de dudosa eficiencia.
El capı́tulo 7, el último, estudia alguna de las posibles mejoras que pueden llevarse a cabo sobre la técnica básica del capı́tulo 6 para hacer factible
su implementación. Aquı́ la oferta es amplı́sima y a falta de determinar
qué alternativas son las más adecuadas, se presentan las más interesantes
propuestas durante los últimos años.
Algunos aspectos secundarios del trabajo, pero no por ello menos interesantes, se han relegado a los apéndices. El apéndice A aborda la medida
de Lebesgue y la dimensión de Hausdorff, ésta última como herramienta
imprescindible para medir y comparar fractales. El apéndice B presenta un
resumen de la teorı́a de los wavelets, fundamento de uno de los más duros
adversarios de la compresión fractal como se explica en el capı́tulo 5.
Finalmente, se presenta una bibliografı́a comentada que evita la mera
descripción catalográfica de muchos trabajos y que es de gran importancia
para orientarse entre las numerosas fuentes existentes. Cierran la obra el
ı́ndice de materias y un vocabulario bilingüe al que se puede recurrir al
estudiar alguna de las referencias (prácticamente todas) escritas en inglés.

Créditos
Este trabajo se realizó como memoria del proyecto desarrollado por
el autor para la obtención del tı́tulo de Ingeniero en Informática. Es el
resultado de cientos de horas de trabajo desde septiembre de 1997 a julio de

iv

PRÓLOGO

1998 bajo la tutela del profesor José Oncina Carratalá del Departamento
de Lenguajes y Sistemas Informáticos de la Universidad de Alicante. El
fuente de esta obra fue realizado en LATEX. El proyecto cumple, además, las
esperanzas del autor de adentrarse en la dimensión siempre fascinante de
los fractales.

Juan Antonio Pérez Ortiz
Alicante, 8 de julio de 1998

Índice general
Prólogo
1. Monstruos matemáticos
1.1. Fractales . . . . . . . . . . .
1.2. El caos y el orden . . . . . .
1.3. Conjuntos de Julia . . . . .
1.4. El conjunto de Mandelbrot

I

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

1
1
11
15
16

2. Lenguajes fractales
2.1. Teorı́a de lenguajes . . . . . . . .
2.2. Fractales sintácticos . . . . . . .
2.3. Sistemas D0L . . . . . . . . . . .
2.4. Curvas fractales y sistemas D0L .
2.5. Instrumentación . . . . . . . . .
2.6. Un poco de Botánica . . . . . . .
2.7. Más allá de los sistemas D0L . .

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

23
23
24
25
26
28
29
30

3. Conjuntos autosemejantes
3.1. Modelo matemático de autosemejanza . . . . .
3.2. Conjuntos autosemejantes famosos . . . . . . .
3.3. Espacios métricos . . . . . . . . . . . . . . . . .
3.4. Invarianza respecto a un sistema de semejanzas
3.5. Transformación de un sistema de semejanzas .
3.6. Espacio (H(Rn ), dH ) . . . . . . . . . . . . . . .
3.7. Teorema del punto fijo . . . . . . . . . . . . . .
3.8. Condición de abierto . . . . . . . . . . . . . . .
3.9. Red de recubrimientos básicos . . . . . . . . . .
3.10. Dimensión de conjuntos autosemejantes . . . .

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

33
33
35
37
42
43
45
47
49
49
52

4. Sistemas de funciones iteradas
4.1. El espacio de los fractales . . . . .
4.2. Aplicaciones contractivas . . . . .
4.3. Obtención del fractal asociado a un
4.4. El teorema del collage . . . . . . .

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

55
56
56
67
70

.
.
.
.

v

.
.
.
.

. . .
. . .
SFI
. . .

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

vi

ÍNDICE GENERAL
4.5. Fractales en movimiento . . . . . . . . . . . . . . . . . . . . .
4.6. Los conjuntos de Julia como SFI . . . . . . . . . . . . . . . .

5. Compresión de imágenes
5.1. Dos pájaros de un tiro . . . . . . . . . . . . .
5.2. Calidad de la compresión con pérdidas . . . .
5.3. Compresión de imágenes en color . . . . . . .
5.4. Cuantización vectorial . . . . . . . . . . . . .
5.5. El estándar JPEG . . . . . . . . . . . . . . .
5.6. Compresión basada en wavelets . . . . . . . .
5.7. Compresión fractal . . . . . . . . . . . . . . .
5.8. Comparación de los esquemas de compresión

76
77

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

79
80
81
82
84
85
90
92
92

6. La transformada fractal
6.1. Historia y fundamentos . . . . . . . . . . . . . . . .
6.2. Modelo de imagen . . . . . . . . . . . . . . . . . . .
6.3. Sistemas de funciones iteradas particionadas . . . . .
6.4. Cuantización vectorial y codificación fractal . . . . .
6.5. Obtención de los coeficientes de los códigos fractales
6.6. Compactación de los códigos fractales . . . . . . . .
6.7. Ejemplos . . . . . . . . . . . . . . . . . . . . . . . .

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

99
99
101
102
105
108
110
110

7. Mejoras en la codificación fractal
7.1. Segmentación de la imagen . . .
7.2. Transformación geométrica de los
7.3. Postprocesamiento . . . . . . . .
7.4. Clasificación de los dominios . . .
7.5. Compresión sustituyente . . . . .
7.6. Independencia de la resolución .
7.7. Mejora de la resolución . . . . . .
7.8. Aceleración de la compresión . .
7.9. Aceleración de la descompresión
7.10. Enfoques hı́bridos . . . . . . . . .

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

. . . . . .
dominios
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

115
. 115
. 118
. 120
. 122
. 126
. 127
. 129
. 129
. 132
. 133

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

Apéndices
A. Medida de conjuntos
A.1. La medida de Lebesgue
A.2. Problema del área . . .
A.3. Dimensión . . . . . . . .
A.4. Dimensión de homotecia
A.5. Medida de Haussdorf . .
A.6. Dimensión de Hausdorff

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

135
135
137
137
138
138
139

ÍNDICE GENERAL

vii

A.7. Dimensión fractal . . . . . . . . . . . . . . . . . . . . . . . . . 140
B. La teorı́a de los wavelets
B.1. Limitaciones de la transformada de Fourier
B.2. La transformada de Fourier a corto plazo .
B.3. Análisis multirresolución . . . . . . . . . . .
B.4. La transformada continua con wavelets . . .
B.5. La transformada discreta con wavelets . . .

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

145
145
147
149
150
154

C. Imágenes originales

159

Bibliografı́a

165

Índice de Materias

175

Vocabulario bilingüe (inglés-español)

179

Capı́tulo 1

Monstruos matemáticos
A pesar de que la historia de los fractales comienza en los últimos dı́as
del siglo XIX, gran parte del XX permanece ajena a ellos. En las últimas
décadas del siglo, y casi paralelamente a la evolución de la investigación de
los sistemas caóticos, los fractales van cobrando un auge creciente, hasta
convertirse en un concepto cada vez más extendido en todas las ciencias.
En este capı́tulo nos introduciremos en la temática mostrando algunos
de los fractales más famosos. También abordaremos brevemente la aparición
del caos en sistemas dinámicos y el importante descubrimiento que supuso la
constante de Feigenbaum. Por último, descubriremos uno de los más bellos
y complejos objetos matemáticos, el conjunto de Mandelbrot, que puede
considerarse una enciclopedia en la que cada una de sus entradas es un
conjunto de Julia.
Este capı́tulo se basa en información obtenida de muy diversas fuentes
y medios. Por citar algunas, puede considerarse [BAR 93b, GUZ 93]. Referencias adicionales aparecen donde se considera oportuno a lo largo del
texto. Los objetivos de este capı́tulo son puramente descriptivos por lo que
se omitirá toda demostración de los resultados obtenidos.

1.1.

Fractales

A finales del siglo pasado, el matemático Charles Hermite tildaba de
“plaga lamentable” la fascinación que algunos otros matemáticos sentı́an
por determinadas curvas que desafiaban los cimientos de la geometrı́a de
la época. Muchos como él consideraban patológicas aquel tipo de curvas,
desentendiéndose de sus insólitas propiedades. Uno de aquellos primeros
1

2

CAPÍTULO 1. MONSTRUOS MATEMÁTICOS

monstruos geométricos era el denominado conjunto de Cantor. Su definición
es muy sencilla: se toma un segmento de determinada longitud (por ejemplo
el intervalo [0, 1] de la recta real) y se divide en tres subsegmentos de igual
longitud, se suprime el segmento central y el proceso se repite con los dos
nuevos segmentos resultantes. El resultado de iterar este proceso infinitas
veces (paso al lı́mite) es el conjunto de Cantor.
Ahora bien, ¿tiene elementos el conjunto de Cantor? Un espectador infinitesimal que contemplase la iteración anterior durante una eternidad, ¿no
terminarı́a por ver desaparecer la totalidad de los puntos? El consolidado
sistema de medidas de la época (medida Lebesgue) daba para dicho conjunto longitud nula. Tarde o temprano se tuvo que aceptar que aquel sistema
de medidas era insuficiente.
En 1890, Peano ideó otro de tales monstruos: una curva que rellenaba el
plano ¿Cómo podı́a una región cuadrada del plano ser una curva? Años más
tarde, Hilbert ideó una curva con idéntica propiedad pero de más sencilla
elaboración.
Otro ejemplo lo constituye la curva ideada por el matemático sueco Helge
von Koch en 1904. Un segmento se divide en tres partes iguales, sustituyendo la central por los dos segmentos que junto a dicha parte formarı́an un
triángulo equilátero. El proceso se repite ad infinitum con los cuatro segmentos resultantes. La curva de Koch oculta otra caracterı́stica sorprendente:
un perı́metro infinito aloja un área finita.
Todas estas formas que se retuercen sobre sı́ mismas terminaron por revolucionar muchos de los conceptos dados por válidos hasta el siglo pasado,
desembocando en la denominada teorı́a geométrica de la medida desarrollada
en las primeras décadas de nuestro siglo. Uno de los aspectos más relevantes
surgidos de esta teorı́a es la redefinición del concepto de dimensión a cargo
de Hausdorff, que permite que estas curvas tengan dimensión fraccionaria.
Ası́ la curva de Koch tiene una dimensión de Hausdorff de 1,2618 lo cual
indica que está más cerca de ser una recta (dimensión 1) que un área (dimensión 2). La curva de Hilbert, por tanto, tiene una dimensión de Hausdorff
de 2. Los trabajos de Haussdorf fueron continuados durante la década de los
años 20 por Besicovitch derivando en la teorı́a geométrica de la medida.
Hoy en dı́a todas las curvas anteriores se incluyen dentro de una clase
más amplia de objetos matemáticos denominados fractales. El término fue
acuñado por Benoit Mandelbrot (descubridor de uno de los más bellos y
complejos conjuntos matemáticos, que lleva su nombre) hace apenas veinte
años como un neologismo derivado de la palabra latina fractus1 , estando aún
1
Aunque Madelbrot definió el sustantivo fractal con genero femenino, son raras las
referencias en castellano que se refieren a las fractales y gran mayorı́a las que lo hacen

1.1. FRACTALES

3

por establecer una definición exacta y definitiva del término. Sin embargo,
de algo no hay duda: las curvas descritas anteriormente son genuinamente
fractales.
Básicamente los fractales se caracterizan por dos propiedades: autosemejanza (o autosimilitud) y autorreferencia. La autorreferencia determina
que el propio objeto aparece en la definición de sı́ mismo, con lo que la forma
de generar el fractal necesita algún tipo de algoritmo recurrente. La autosemejanza implica invarianza de escala, es decir, el objeto fractal presenta
la misma apariencia independientemente del grado de ampliación con que
lo miremos. Por más que se amplı́e cualquier zona de un fractal, siempre
hay estructura, hasta el infinito, apareciendo muchas veces el objeto fractal
inicial, contenido en sı́ mismo.
De todas formas, no fue hasta los años 70 que comenzaron a vislumbrarse las aplicaciones de los fractales. En su tan citada obra The Fractal
Geometry of Nature2 , Mandelbrot razonó que la naturaleza entiende mucho
más de geometrı́a fractal que de geometrı́a diferenciable3 . La geometrı́a fractal aborda el estudio de formas geométricas no diferenciables o quebradas
a cualquier escala que se miren. La geometrı́a diferenciable, por otra parte,
asume que a pequeña escala formas que no lo son se suavizan, con lo que se
pierde la perspectiva global del objeto. La geometrı́a fractal ofrece un modelo alternativo que busca una regularidad en las relaciones entre un objeto
y sus partes a diferentes escalas. El objeto se expresa como el lı́mite de un
proceso geométrico iterativo, el cual puede provocar en cada iteración una
ruptura (fractura o quebramiento) de la suavidad que lleva a la ausencia de
diferenciabilidad en el objeto lı́mite.
También fue crucial la publicación por Hutchinson en 1981 de un trabajo en el que se desarrolla el concepto de conjunto autosemejante, de gran
trascendencia en el desarrollo posterior de la geometrı́a fractal.
A partir de ahı́, muchos cientı́ficos se han encontrado fractales en sus
campos de estudio (el tı́tulo de uno de los libros sobre el tema es bastante
sugerente, Fractals Everywhere). La distribución de las galaxias, los procesos
fı́sicos de ramificación, agregación y turbulencia, la aparición de ruido en
señales eléctricas (precisamente una especie de conjunto de Cantor en su
distribución) e incluso los fenómenos económicos o sociológicos son algunos
de los lugares en los que se esconde el serpenteo incansable de los fractales.
a los fractales. Por ello, y siguiendo esta tendencia, utilizaremos en esta obra el genero
masculino.
2
Editada en castellano en 1997, veinte años después de su publicación original, por la
editorial Tusquets.
3
Es más correcto contraponer la geometrı́a fractal a la geometrı́a diferenciable que a la
euclidiana, aunque muchas fuentes la opongan a esta última.

4

CAPÍTULO 1. MONSTRUOS MATEMÁTICOS

Un congreso multidisciplinar sobre fractales (Fractal 98, Valletta, Malta)
incluye entre los temas a tratar los siguientes:
Aplicaciones en Biologı́a, Medicina, Ingenierı́a, Economı́a y Sociologı́a
Autómatas celulares
Estructuras coherentes
Difusión
Sistemas desordenados
Superficies y volúmenes fractales
Fenómenos de crecimiento
Sistemas de funciones iteradas
Análisis y sı́ntesis de imágenes
Sistemas L
Multifractales
Sistemas dinámicos no lineales
Formación de estructuras
Transiciones de fase
Autoorganización y fenómenos de cooperación
Turbulencia
Visualización
Ondas e interacciones
Resulta curioso que los matemáticos que sentaron las bases de la teorı́a
geométrica de la medida a comienzos de este siglo, lo hicieron desde un
punto de vista completamente teórico, sin intuir las tremendas consecuencias
que sus trabajos tendrı́an varias décadas después en multitud de disciplinas
cientı́ficas. Aunque no es correcto atribuir a Mandelbrot la paternidad de la
geometrı́a fractal, no puede negarse su vital aportación al renacimiento de
ésta y su visión de la potencia de los fractales para modelizar la realidad.

La naturaleza es fractal
Es muy común encontrar afirmaciones como la que titula este apartado
en la literatura sobre el tema. Sin embargo, es necesario advertir aquı́ que,
realmente, la naturaleza no es fractal. Cuando decimos que un objeto real
como una costa o la red capilar del sistema venoso es un fractal estamos

1.1. FRACTALES

5

queriendo decir que existe un modelo fractal que aproxima con bastante
precisión dicho objeto.
En el mundo real no existen fractales, como tampoco existen rectas ni
esferas. Hablar de la dimensión fractal de una costa no es más absurdo que
hablar del radio de la Tierra. La ciencia avanza gracias a todas estas aproximaciones, aunque probablemente las cosas no comprendan en su esencia
nuestros modelos matemáticos.
Los fractales, además, abren la puerta a numerosas conjeturas sobre la
complejidad del mundo. Las pautas de generación de fractales son extremadamente sencillas si se comparan con los resultados obtenidos. Es posible
que numerosos comportamientos de la naturaleza que hoy dı́a se nos antojan extremadamente complicados respondan de igual forma a mecanismos
de enome sencillez. La geometrı́a fractal es una rama muy joven cuyos progresos deben repercutir muy directamente en una creciente utilidad de la
geometrı́a fractal para el estudio de la realidad.

El conjunto de Cantor
El conjunto de Cantor es un ejemplo clásico de conjunto no numerable
con el mismo cardinal que el continuo, pero, a pesar de ello, con medida
de Lebesgue unidimensional (longitud) nula. Una breve descripción de esta
medida puede encontrarse en el apéndice A.
Para construir el conjunto de Cantor partiremos del intervalo unidad
E0 = [0, 1] ⊂ R. Dividimos dicho intervalo en tres partes iguales y consideramos los intervalos cerrados de los extremos
·

E11 = 0,

1
3

¸

·

E12 =

¸

2
,1
3

cada uno de ellos de longitud 1/3.
El proceso anterior se repite sobre los nuevos conjuntos obtenidos. Cada
uno de estos intervalos se divide en tres intervalos de igual longitud para
prescindir del intervalo central y considerar los cuatro intervalos cerrados
·

E21 = 0,

1
9

¸

·

E22 =

2 1
,
9 3

¸

·

E23 =

2 7
,
3 9

¸

·

E24 =

¸

8
,1
9

cada uno de ellos de longitud 1/9.
Si continuamos indefinidamente de esta forma, en la etapa k-ésima tendremos 2k intervalos cerrados Ekj con j = 1, 2, . . . , 2k cada uno de ellos de
longitud 3−k .

6

CAPÍTULO 1. MONSTRUOS MATEMÁTICOS
1

E0
1/3

E1
1/9

E2

Figura 1.1: El conjunto ternario de Cantor se obtiene de manera inductiva comenzando por el segmento unidad y quitando en cada etapa a cada intervalo el
segmento medio resultante de dividirlo en tres partes iguales.

Consideremos ahora para cada k = 1, 2, . . . el conjunto
k

Ek =

2
[

Ekj

j=1

Observamos que los conjuntos Ek , k = 1, 2, . . ., forman una sucesión
monótonamente decreciente, esto es
Ek+1 ⊂ Ek

∀k

El conjunto lı́mite de este proceso
E=

∞
\

Ek

k=1

se denomina conjunto ternario de Cantor . En la figura 1.1 se muestran las
primeras etapas de la generación del conjunto de Cantor.
Las propiedades asombrosas de este conjunto son abundantes. Veamos
unas cuantas. En primer lugar observamos que E no es vacı́o ya que en cada
Ek están, como mı́nimo, los extremos de los 2k intervalos cuya unión nos da
Ek y, por lo tanto, también están en E. Además, el conjunto de Cantor es
cerrado por ser intersección de cerrados.
Con todo, éstos no son los únicos puntos de E; si ası́ fuera, se tratarı́a
de un conjunto numerable. Pero E es no numerable. Veámoslo.
Cada punto de E es representable de forma única mediante
a=

a1 a2
an
+ 2 + ··· + n + ···
3
3
3

donde cada ai es 0 ó 2. Podemos entonces escribirlo en base tres como
a = 0.a1 a2 . . . an . . .

1.1. FRACTALES

7

Recı́procamente, cada expresión de este tipo corresponde a un punto de
E. Si E fuera numerable4 podrı́amos ordenar sus elementos. Supongamos
que es cierto lo anterior y que E es numerable
a1 = 0.a11 a12 . . .
a2 = 0.a21 a22 . . .
a3 = 0.a31 a32 . . .
...............
y formemos un punto 0.b1 b2 . . . a partir de la sucesión anterior con la regla
siguiente
si ann = 0, bn = 2
si ann = 2, bn = 0
El número ası́ formado no está en la sucesión anterior y, sin embargo,
pertenece claramente a E y, por tanto, E no puede ser numerable.
Este procedimiento es muy similar a la famosa técnica utilizada por
Cantor para demostrar la no numerabilidad de R.
Aun ası́, el conjunto de Cantor tiene medida Lebesgue unidimensional
nula. Esta medida se discute en el apéndice A. Para cualquier etapa k, la
familia de intervalos {Ekj }, j = 1, . . . , 2k , es un recubrimiento de E formado
por intervalos disjuntos. Ası́ se tiene, por las propiedades de la medida de
Lebesgue, que

1

1

L (E) ≤ L

k

2
[



Ekj  =

j=1

k

2
X
j=1

L1 (Ekj ) = 2k 3−k =

µ ¶k

2
3

Puesto que la desigualdad es cierta para todo k y (2/3)k tiende a cero cuando
k tiende a infinito, se obtiene L1 (E) = 0.
Aunque aquı́ no lo demostraremos, puede comprobarse, además, que el
conjunto E no contiene intervalos, es decir, es infinitamente poroso.

Curvas de Peano y Hilbert
En 1890 Peano construyó una curva continua que pasa por todos los
puntos del cuadrado unidad [0, 1]2 . Era el primer ejemplo de una curva que
4

Un conjunto es infinito si tiene el mismo cardinal que una parte estricta suya, esto es,
si puede establecerse una aplicación biyectiva entre el conjunto y un subconjunto propio
suyo. Un conjunto es numerable si tiene el mismo cardinal que N. Cantor demostró que
Q es numerable y que R no es numerable.

8

CAPÍTULO 1. MONSTRUOS MATEMÁTICOS

Figura 1.2: Primeras etapas de la generación de la curva de Hilbert. La curva de
Hilbert es un ejemplo de curva que llena el plano, por lo que su dimensión fractal
es 2.

llena un espacio. Años más tarde, Hilbert construye otra del mismo tipo con
una construcción geométrica más simple de describir.
La curva de Hilbert se construye iterando el procedimiento que puede
observarse en la figura 1.2. En cada etapa cada segmento se sustituye por
otros cuatro con la mitad de longitud. La curva lı́mite de tales poligonales
llena el cuadrado unidad.

Curva de Kock
Esta curva fue construida en 1904 por el matemático Helge von Kock.
Se parte del segmento unidad [0, 1] y se divide en tres partes iguales, sustituyendo la parte central por los dos segmentos que junto con dicha parte
formarı́an un triángulo equilatero. Con cada uno de los cuatro segmentos
que ası́ queden determinados se repite la operación anteriormente descrita.
Se procede indefinidamente de esta forma obteniendo en cada etapa k
una poligonal de longitud (4/3)k . La curva de Kock se define como la curva
lı́mite a que converge la sucesión cuando k tiende a infinito. Se trata, por
tanto, de una curva de longitud infinita pues (4/3)k tiende a infinito con
k. Más aún, la longitud de la parte de curva comprendida entre dos puntos
cualesquiera de la misma también es infinita. El área bajo la curva, por otra
parte, viene dada por la serie
µ ¶

1+

4
9

µ ¶2

+

4
9

µ ¶3

+

4
9

+ ...

1.1. FRACTALES

9

Figura 1.3: Primeros pasos del proceso de construcción de la curva de Koch. En
el lı́mite, dados dos puntos cualesquiera de la curva es imposible llegar a uno de
ellos desde el otro por encima de la curva. La longitud de cualquier tramo de curva
es infinita.

que converge a 9/3 asumiendo que el área bajo el triángulo de la primera iteración es 1. En la figura 1.3 pueden verse las primeras etapas de la
generación de la curva de Koch.

Funciones de Weierstrass
Weierstrass dio otro ejemplo de una curva con comportamiento análogo
a las anteriores, pero definida de forma analı́tica
f (x) =

∞
X

λ(s−2)i sen λi x

i=1

con 1 < s < 2 y λ < 1. Esta función es continua, pero no es diferenciable en
ningún punto.

Otros fractales
Los apartados anteriores han mostrado algunos conjuntos fractales de
reconocida fama y prestigio. Sin embargo, no son, ni mucho menos, los únicos.
La figura 1.4 muestra el triángulo de Sierpinski. Este fractal se genera
a partir de un triángulo equilatero relleno de lado l del que se extrae el
subtriángulo formado por los tres puntos medios de los lados del triángulo.
El proceso se repite con los tres nuevos triángulos de lado l/2 ası́ obtenidos. Si continuamos de esta manera, en la etapa k tendremos 3k triángulos
equilateros con lados de longitud l2−k . La figura 1.4 muestra el conjunto
obtenido.
El proceso seguido para la construcción del conjunto de Cantor puede
generalizarse a dimensiones superiores. La generalización a tres dimensiones

10

CAPÍTULO 1. MONSTRUOS MATEMÁTICOS

Figura 1.4: Imagen final aproximada del triángulo de Sierpinski. Sabiendo que el
conjunto inicial es un triángulo equilatero relleno, no es difı́cil deducir el proceso
iterativo que permite obtenerlo.

Figura 1.5: Esponja de Menger. Puede considerarse una generalización del conjunto de Cantor. Se comienza por un cubo y se divide en 27 cubos más pequeños,
extrayendo el cubo central y los situados en el centro de cada cara del cubo original.
El proceso se repite con cada uno de los cubos restantes y ası́ sucesivamente. La
dimensión de la esponja de Menger es 2, 727 lo que indica que está más cerca de
ser un cuerpo solido que una curva suave.

produce la denominada esponja de Menger que puede verse en la figura 1.5.
Unos años antes de los primeros desarrollos de Mandelbrot, algunos
cientı́ficos comenzaron a ponerse de acuerdo en la explicación de ciertos
fenómenos irregulares que surgı́an en multitud de sistemas dinámicos. Eran
los primeros intentos de descubrir algunos viejos trucos de magia de la naturaleza.

1.2. EL CAOS Y EL ORDEN

1.2.

11

El caos y el orden

El descubrimiento y formalización del caos se ha dado en considerar como
una nueva revolución en la Fı́sica del siglo XX, comparable a la que en su
dı́a provocaron la relatividad y la teorı́a cuántica.
Un sistema dinámico (siempre no lineal) se considera caótico5 si presenta
un comportamiento aperiódico (esto es, resultado de oscilaciones regulares
que no se repiten nunca, de periodo infinito) resultado de un modelo totalmente determinista y que presenta gran sensibilidad a las condiciones
iniciales.
La sensibilidad a las condiciones iniciales implica que existe una divergencia exponencial de trayectorias inicialmente muy próximas en el espacio
de fases, fenómeno que se conoce como estirado. Otra propiedad existente
sobre el espacio de fases y opuesta al estirado es el plegamiento que conlleva
que dos trayectorias muy lejanas pueden eventualmente acercarse.
Si representamos el retrato fase de un sistema dinámico, veremos que
las dos fuerzas anteriores entran en acción de forma que se genera una estructura confinada en una región del espacio de fases que se conoce como
atractor extraño. Antes del descubrimiento del caos, los ciclos lı́mite eran los
atractores más complejos que se conocı́an. Hoy dı́a se puede decir que cada
sistema caótico lleva asociado un atractor de caracterı́sticas peculiares.
Las trayectorias del espacio de fases nunca intersectan entre sı́, pues esto
supondrı́a un comportamiento periódico. Como la región en la que está ubicado el atractor es finita, se tiene, al seguir una trayectoria cualquiera, una
curva de longitud infinita encerrada en un área finita o, dicho de otra forma,
un atractor extraño posee estructura fractal.
El ordenador facilita el proceso iterativo de los sistemas dinámicos y es
un arma imprescindible para aproximarse a la geometrı́a de los atractores
extraños.

Duplicación de periodo y constante de Feigenbaum
La ecuación logı́stica se ha convertido en la manera usual de introducir
las caracterı́sticas del caos. Se trata de una ecuación en diferencias que fue
formulada por Verhulst en el siglo pasado para explicar el crecimiento de
5

Para este apartado puede consultarse Dinámica clásica de Antonio Rañada, editado
por Alianza. También Biofı́sica: procesos de autoorganización en Biologı́a de Francisco
Montero y Federico Morán, editado por Eudema, donde se discute ampliamente la teorı́a
de bifurcaciones y la constante de Feigenbaum.

12

CAPÍTULO 1. MONSTRUOS MATEMÁTICOS

una población perteneciente a la misma especie y que se reproduce en un
entorno cerrado sin ningún tipo de influencia externa. Pese a su aparente
sencillez, constituye un buen ejemplo para mostrar el comportamiento de
los sistemas caóticos. La ecuación se puede escribir como
xn+1 = rxn (1 − xn )
donde el parámetro r es una constante denominada parámetro de crecimiento
(generalmente entre 0 y 4) y la variable xn puede verse como la fracción
máxima de población que el ambiente puede soportar en el instante tn .
Considerando que la población lı́mite
x∞ = lı́m xn
n→∞

existe, queremos investigar la forma en la cual xn depende del parámetro
de crecimiento r. Si estudiamos experimentalmente el sistema, observaremos
que para valores de r < 3 el sistema converge a un punto fijo estable, que es
cero cuando r < 1. Cuando r > 3, el punto fijo se hace inestable y el valor
de x∞ oscila entre dos valores; se ha obtenido lo que se conoce como una
duplicación de periodo.
Si se aumenta r ligeramente, por ejemplo r = 3,44 . . ., el número de
puntos sobre los que oscila x∞ es de 4. Si se sigue aumentando el valor de
r, aparece una nueva duplicación de periodo para r = 3,544 . . ., obteniendo
un periodo de 8. Y ası́ sucesivamente hasta llegar a obtener una sucesión de
infinitos valores para x∞ correspondiente al caos. Nótese cómo los valores
de r para los que se producen las sucesivas duplicaciones de periodo están
cada vez más cerca unos de otros.
El comportamiento de la ecuación logı́stica en función de r puede observarse visualmente a través de un diagrama de bifurcación. En el eje horizontal se representa un cierto intervalo de valores de r y entonces se dibujan los
valores de x generados por la iteración en el eje vertical. La figura 1.6 muestra el diagrama de bifurcación de la ecuación logı́stica en el rango 2 ≤ r ≤ 4.
La duplicación de periodo es un signo ineludible del comportamiento
caótico de un sistema. Son muchos, y cada dı́a más, los sistemas dinámicos
en los que se observa este fenómeno y que desembocan, variando alguno
de sus parámetros, en caos. Es más, un famoso artı́culo publicado en los
inicios de la teorı́a del caos demostró que cualquier sistema en el que, para
algún valor de sus parámetros, se registrara una periodicidad de periodo
3, desembocará para otros valores de sus parámetros en comportamiento
caótico.
Lo anterior hace pensar en una universalidad del caos todavı́a no muy
bien conocida que hace que sistemas muy diferentes muestren pautas similares de comportamiento. Un hecho que vino a corroborar esto y a mostrarnos

1.2. EL CAOS Y EL ORDEN

13

Figura 1.6: Diagrama de bifurcación de la ecuación logı́stica xn+1 = rxn (1 − xn )
en el rango 2 ≤ r ≤ 4. Puede observarse la ruta del caos: sucesivos desdoblamientos
de periodo que desembocan en un periodo infinito.

que existe un cierto orden en el caos fue el descubrimiento por parte de
Feigenbaum a mediados de los setenta de la constante que lleva su nombre.
Una vez obtenido el diagrama de bifurcación de la ecuación logı́stica, se
puede calcular el incremento del parámetro entre dos bifurcaciones contiguas
∆i = ri − ri−1
y dividiendo por el incremento en el siguiente intervalo
ri − ri−1
∆i
=
∆i + 1
ri+1 − ri
Feigenbaum encontró que la fracción anterior convergı́a hacia un valor determinado al ir haciendo i mayor, de modo que en el lı́mite se obtenı́a
∆i
= 4,6692016091029906718532038204662 . . .
i→∞ ∆i + 1

δ = lı́m

Feigenbaum calculó el lı́mite anterior para otras ecuaciones en diferencias
y obtuvo el mismo valor para δ. Posteriormente se ha encontrado el mismo
valor de δ en algunos sistemas continuos e incluso en sistemas experimentales, todos de muy diversa procedencia. Hoy sabemos que δ, conocida como
constante de Feigenbaum, es una constante universal tan fundamental como
π o e y que ha provocado una nueva forma de ver el mundo.
El comportamiento caótico descrito anteriormente no sólo surge bajo sistemas discretos. Multitud de sistemas dinámicos de ecuaciones diferenciales
presentan fenómenos caóticos que generan atractores extraños. Por mostrar
uno de ellos, veremos como el caos puede anidar incluso en sistemas clásicos
aparentemente sencillos.

14

CAPÍTULO 1. MONSTRUOS MATEMÁTICOS

Figura 1.7: Cuando se considera que la fuerza ejercida por un muelle sobre una
masa m no sigue la ley de Hooke, sino que esta fuerza es función no lineal de x
y, además, hacemos que una fuerza externa actúe sobre la masa, el sistema puede
comportarse de forma caótica.

Ecuación forzada de Duffing
La ecuación diferencial de segundo orden
mẍ + cẋ + kx + βx3 = 0
puede utilizarse para modelar las vibraciones libres amortiguadas por la
velocidad de una masa m sobre un muelle no lineal como se muestra en la
figura 1.7. El término kx representa la fuerza ejercida sobre la masa por un
muelle lineal, mientras que el término βx3 representa la no linealidad de un
muelle real.
Vamos a analizar las vibraciones forzadas que resultan cuando una fuerza
externa F (t) = F0 cos ωt actúa sobre la masa. Con esta fuerza sumada al
sistema obtenemos la ecuación forzada de Duffing
mẍ + cẋ + kx + βx3 = F0 cos ωt
para el desplazamiento x(t) de la masa de su posición de equilibrio. Para
simplificar el modelo supondremos que k = −1 y que m = c = β = ω = 1,
con lo que la ecuación diferencial es
ẍ + ẋ − x + x3 = F0 cos t.
Pasando el sistema anterior a variables de estado obtenemos
Ã

ẋ
ẏ

!

Ã

=

0
1
(1 − x2 ) −1

!Ã

x
y

!

Ã

+

0
F0 cos t

!

que puede integrarse mediante el método de Euler para obtener el retrato
fase asociado al sistema.

1.3. CONJUNTOS DE JULIA

15

(a) F0 = 0,6

(b) F0 = 0,7

(c) F0 = 0,75

(d) F0 = 0,8

Figura 1.8: Ruta hacia el caos de la ecuación forzada de Duffing. Las figuras
muestran las duplicaciones de periodo directamente sobre el atractor extraño (también podrı́a haberse hecho con un diagrama de bifurcación). En algún punto entre
F0 = 0,75 y F0 = 0,8 el caos irrumpe en el sistema obligándolo a un comportamiento
aperiódico. Las duplicaciones de periodo respetan la constante de Feigenbaum.

Variando el valor de F0 cuidadosamente desde F0 = 0,6 a F0 = 0,8, como
en la figura 1.8, pueden observarse las sucesivas duplicaciones de periodo
que llevan al caos. Es curioso que aunque esta ecuación se estudió durante
décadas, sin ordenadores nadie pudo vislumbrar en ella los signos del caos.

1.3.

Conjuntos de Julia

En las secciones anteriores hemos estudiado la evolución de sistemas
dinámicos sobre el plano real. Sin embargo, algunos de los resultados más
espectaculares obtenidos con la iteración de un sistema dinámico se dan
cuando consideramos funciones de variable compleja. Esta espectacularidad
se muestra en dos frentes distintos: el estético y el matemático.
La teorı́a de sistemas dinámicos complejos data de comienzos de este
siglo, con los trabajos de los matemáticos franceses Gaston Julia y Pierre
Fatou. Aquı́ nos centraremos en el estudio de sistemas dinámicos complejos cuadráticos. La discusión de otros sistemas se sale del objetivo de esta
introducción.
Podemos definir el conjunto de Julia de un polinomio de variable compleja como la frontera del conjunto de puntos que escapan al infinito al iterar
dicho polinomio. Esto significa que la órbita de un elemento del conjunto
de Julia no escapa al infinito, pero existen puntos arbitrariamente cerca de

16

CAPÍTULO 1. MONSTRUOS MATEMÁTICOS

él que sı́ lo hacen. La órbita de un punto x bajo un sistema dinámico de
función f es la sucesión de puntos {f n (x)∞
n=0 }.
Para simplificar las cosas nuestro estudio girará en torno a funciones
polinómicas cuadráticas de la forma:
f (z) = z 2 + c
donde c y z son números complejos. A pesar de su sencillez, la iteración de
la función anterior genera estructuras muy complicadas, hecho éste que ya
fue vislumbrado por Julia y Fatou a comienzos de siglo.
Los valores que no tienden a infinito los representaremos dibujando en
negro su pixel asociado. Con este procedimiento se han obtenido los conjuntos de la figura 1.9. Como se dijo antes, es la frontera de las regiones
dibujadas en negro lo que constituye realmente el conjunto de Julia. A la
región completa se le suele denominar conjunto de Julia relleno.
Aunque algunos matemáticos intuı́an la diversidad de conjuntos de Julia
que se derivaba de la utilización de distintos valores para c, la llegada de los
primeros ordenadores con capacidades gráficas y a precios asequibles a finales
de la década de los setenta hizo que se sobrepasara cualquier expectativa.
Si observamos los distintos conjuntos de Julia rellenos representados en
esta sección veremos que pueden clasificarse en dos grandes grupos según
su estructura. Algunos de ellos parecen estar formados por una única pieza,
de manera que parece factible poder caminar por su frontera interminablemente. Otros, sin embargo, parecen fragmentados o disconexos. Esta clasificación de hecho no es arbitraria y su estudio dio lugar a uno de los objetos
matemáticos más complejos que existen: el conjunto de Mandelbrot.

1.4.

El conjunto de Mandelbrot

Julia probó que la órbita de z = 0 juega un papel esencial para saber si
un conjunto de Julia es conexo. Si esta órbita escapa al infinito, el conjunto
aparece fragmentado como polvo fractal. En caso contrario, el conjunto de
Julia es conexo.
El resultado anterior nos proporciona un método preciso y cómodo para
determinar la conectividad de un conjunto de Julia. Ahora bien, ¿cuándo
podemos considerar que la órbita de z = 0 diverge a infinito? Esta pregunta
queda resuelta por la teorı́a de iteraciones que nos asegura que la órbita
divergirá a infinito si en algún momento uno de sus puntos tiene módulo
mayor o igual a 2.

1.4. EL CONJUNTO DE MANDELBROT

(a) c = −0,67 + 0,31j

17

(b) c = −0,8 + 0,4j

(c) c = 0,27

(d) c = −1

(e) c = −0,48 − 0,53j

(f) c = −1,312

Figura 1.9: Conjuntos de Julia para distintos valores del parámetro c. Estos conjuntos se pueden dividir en dos grupos, los que están formados por una sola pieza y
los que parecen estar fragmentados en muchas piezas. Los tres primeros pertenecen
a la última clase.

Aunque no lo demostraremos, la clasificación anterior es todavı́a de
carácter más fuerte, ya que, según el valor del parámetro c, el conjunto
de Julia es o bien conexo o bien completamente disconexo, es decir, formado
por una nube de puntos dispersos con la misma estructura que los conjuntos
de Cantor. Estos puntos aparecen dispuestos en grupos densos de forma que
cualquier disco finito que envuelva a un punto contiene, como mı́nimo, otro
punto del conjunto.

18

CAPÍTULO 1. MONSTRUOS MATEMÁTICOS

Figura 1.10: Una de las primeras fotografı́as del nuevo continente descubierto por
Mandelbrot. Se trata de una de las primeras imágenes de una cardioide distinta
a la principal y fue tomada en 1980. En tan sólo unos años se ha hecho posible
obtener imágenes a enormes resoluciones y con millones de colores con tan solo un
ordenador personal. Aun ası́ esta representación ya significaba mucho: en los años
sesenta los primeros atractores extraños se representaron por impresora mediante
caracteres alfanuméricos.

Dada esta división de los conjuntos de Julia, parece natural preguntarse
qué valores de c de la ecuación f (z) = z 2 +c generan conjuntos de uno u otro
tipo. Esta cuestión tan simple no fue resuelta hasta 1978 cuando Mandelbrot
representó en un plano todos los valores de c que producı́an conjuntos de
Julia conexos, consiguiendo la primera representación del conjunto que hoy
lleva su nombre. Por aquellas fechas y con los medios disponibles las primeras
y toscas impresiones de ordenador que obtuvo Mandelbrot eran del tipo de
la de la figura 1.10.
Una representación más visible es la mostrada en la figura 1.11. Lo primero que salta a la vista es la gran región a la derecha que conforma una
cardioide.6 A la izquierda de la gran cardioide podemos observar un disco
tangente a ella. Éste, no obstante, no es el único disco tangente a la cardioide, ya que pueden apreciarse multitud de pequeños discos tangentes a
6

Una cardioide es la curva engendrada por el movimiento de un punto de una circunferencia que rueda exteriormente sobre otra fija de igual radio. La ecuación de la cardioide
en coordenadas cartesianas es (x2 + y 2 )2 − 4ax(x2 + y 2 ) = 4a2 y 2 donde a es el radio de
la circunferencia fija.

1.4. EL CONJUNTO DE MANDELBROT

19

Figura 1.11: El conjunto de Mandelbrot. Puede apreciarse la cardioide y la serie
de cı́rculos cada vez más pequeños pegados a ella.

ella rodeándola. Si ampliáramos7 algunas zonas de la imagen, verı́amos que
unidos a estos discos existen otros aún más pequeños, a los que se unen otros
y ası́ sucesivamente. Si se estudia detenidamente la sucesión de circulos cada
vez más pequeños que se extiende horizontalmente en el sentido negativo del
eje de las x y obtenemos los diámetros de los sucesivos cı́rculos d1 , d2 , . . .,
podemos obtener el lı́mite
di
= 4,66920160910299067185320382 . . .
i→∞ di+1

δ = lı́m

cuyo valor es la constante de Feigenbaum. La universalidad de la constante
de Feigenbaum es un hecho fascinante que hoy por hoy desafı́a a la ciencia.
Existen muchas otras cardioides repartidas por el plano, realmente infinitas. Todas estas cardioides están unidas a la cardioide principal por medio
de filamentos cargados de nuevas cardioides. Estos filamentos se ramifican
siguiendo pautas muy complejas haciendo que el conjunto de Mandelbrot
sea conexo. La demostración de la conectividad del conjunto de Mandelbrot
es una labor compleja que todavı́a presenta gran cantidad de cuestiones
abiertas.
Existen series dudas sobre la autosemejanza del conjunto de Mandelbrot.
Aunque es posible encontrar pequeñas copias en miniatura del conjunto por
7

Cuando hablamos de ampliar una zona de la imagen de un conjunto fractal, nos
referimos a representar el conjunto entre unos intervalos más pequeños que los de la imagen
inicial y no a nada parecido a un zoom fotográfico que no aportarı́a ninguna información
adicional.

20

CAPÍTULO 1. MONSTRUOS MATEMÁTICOS

Figura 1.12: De izquierda a derecha y de arriba a abajo, sucesivas imágenes
de una inmersión en el conjunto de Mandelbrot que demuestran el carácter casi autosemejante del conjunto. El centro de las imágenes corresponde al punto
−0,74650920 + 0,09848028j y la última muestra una escala de casi tres millones
de aumentos sobre la primera. Para generarlas se utilizó el algoritmo de tiempo de
escape.

todo el plano, éstas siempre están envueltas en filamentos cuyo aspecto varı́a
notablemente según donde observemos. A diferencia de los conjuntos de Julia, que sı́ son autosemejantes, dadas dos copias del conjunto de Mandelbrot,
podrı́amos identificar, en teorı́a, bajo qué escala del plano se han obtenido.
Podemos, por tanto, considerar por ahora al conjunto de Mandelbrot como
casi autosemejante.
Las imágenes de la figura 1.12 pueden dar una idea de la variedad de
estructuras que es posible encontrar en el conjunto de Mandelbrot. Estas
imágenes muestran de izquierda a derecha y de arriba a abajo un zoom
sobre la imagen precedente a cada una.

1.4. EL CONJUNTO DE MANDELBROT

21

1. Sea ∆p = (x2 − x1 )/(x̃ − 1)
2. Sea ∆q = (y2 − y1 )/(ỹ − 1)
3. Hacer desde p = 0 hasta x̃
3.1. Hacer desde q = 0 hasta ỹ
3.1.1. Hacer p0 = x1 + p · ∆p
3.1.2. Hacer q0 = y1 + q · ∆q
3.1.3. Hacer z = 0 + 0j
3.1.4. Desde k = 1 hasta iteraciones
3.1.4.1. Hacer z = z 2 + (p0 + q0 j)
3.1.4.2. Si |z| > 2, pintar el punto (p, q) y
salir del bucle de la variable k
4. Fin
Figura 1.13: Algoritmo para representar el conjunto de Mandelbrot. Las dimensiones del modo de vı́deo utilizado son x̃ × ỹ. La parte a representar del conjunto es
la comprendida entre las coordenadas (x1 , y1 ) y (x2 , y2 ) que conformarán, respectivamente, la esquina superior izquierda y la esquina inferior derecha de la imagen
obtenida.

Explosiones cromáticas
La representación del conjunto de Mandelbrot en una pantalla de ordenador es extremadamente sencilla. El único problema que puede surgir es la
discretización que impone la pantalla. El programa en pseudocódigo de la
figura 1.13 asume que las dimensiones del modo de vı́deo utilizado son x̃ × ỹ
y que se desea representar la parte del conjunto de Mandelbrot comprendida
entre las coordenadas (x1 , y1 ) y (x2 , y2 ) que conformarán, respectivamente,
la esquina superior izquierda y la esquina inferior derecha de la imagen.
Aunque el valor de iteraciones del programa de la figura 1.13 puede mantenerse en torno a 200, deberá incrementarse conforme se vaya reduciendo
el intervalo del conjunto de Mandelbrot a representar.
Una de las formas de representar el conjunto de Mandelbrot que proporciona las imágenes de mayor belleza es mediante el denominado algoritmo
de tiempo de escape. Para representar el conjunto de Mandelbrot mediante
este procedimiento seguimos pintando, como hasta ahora, los puntos pertenecientes al conjunto de color negro. Los puntos que divergen a infinito,
sin embargo, se pintan con distintos colores según la velocidad de escape
hacia infinito de su órbita asociada. Lo anterior se lleva a cabo en la práctica considerando una paleta de colores en la que cada uno lleva asociado un
número distinto. Un punto no perteneciente al conjunto se pinta de un color
n si son necesarias n iteraciones para que el módulo de su órbita sea mayor
que 2 (condición ésta, como ya vimos, suficiente para que la orbita se fugue
hacia infinito). El algoritmo de tiempo de escape también puede aplicarse,

22

CAPÍTULO 1. MONSTRUOS MATEMÁTICOS

Figura 1.14: El algoritmo de tiempo de escape convierte en un arte la representación de conjuntos de Julia. Hoy dı́a es casi tan importante la selección de la zona de
visualización como la de una adecuada paleta de colores. El conjunto aquı́ mostrado
corresponde a c = −0,204812 − 0,663316j.

Figura 1.15: Otro conjunto de Julia merecedor de una visita. El parámetro c de
este conjunto es −0,833062 + 0,194798j.

por supuesto, a la representación de conjuntos de Julia. Un par de conjuntos
de Julia obtenidos con esta técnica se muestran en las figuras 1.14 y 1.15.
El algoritmo de tiempo de escape, en la sencilla versión aquı́ comentada y
en variaciones más sofisticadas, ha convertido en un arte la representación
de conjuntos fractales: se venden pósters, camisetas o postales con estos
motivos y se organizan exposiciones por todo el planeta.

Capı́tulo 2

Lenguajes fractales
En el capı́tulo anterior se describió la forma de construir varios fractales de muy diversa ı́ndole. Sin embargo, salvo en el caso de los sistemas
cuadráticos complejos, no se mostró un método sencillo para generarlos.
Los sistemas D0L proporcionan las pautas para la obtención de multitud
de fractales, basándose en la interpretación de ciertos códigos que almacenan
la información que permite la construcción de una sucesión de conjuntos
convergentes al fractal. No es el único enfoque posible (veremos más adelante
los sistemas de funciones iteradas y existen otros métodos como los basados
en automatas celulares o en teragones), pero sı́ es uno de los más sencillos
y potentes. Los contenidos de este capı́tulo pueden encontrarse también en
[BAR 93b] y especialmente en [BLA 94].

2.1.

Teorı́a de lenguajes

Antes de presentar los sistemas D0L es necesaria una pequeña introducción a los conceptos básicos de la teorı́a de lenguajes. Unas cuantas definiciones nos darán las herramientas básicas para comprender el resto del
capı́tulo.
Para empezar consideremos un conjunto finito Σ al que denominaremos
alfabeto. A los elementos de Σ también los denominaremos sı́mbolos. Las
letras y los dı́gitos son ejemplos de sı́mbolos utilizados frecuentemente. Una
cadena (o palabra) es una secuencia posiblemente infinita de sı́mbolos yuxtapuestos. Por ejemplo, partiendo del alfabeto Σ = {a, b, c} podemos construir
cadenas como abbc o aaaaaa . . .
Consideremos ahora el conjunto de todas las cadenas de longitud finita
23

24

CAPÍTULO 2. LENGUAJES FRACTALES

sobre Σ, incluyendo la cadena vacı́a ² ; dicho conjunto recibe a menudo el
nombre de lenguaje universal y se denota por Σ? . Con Σ+ nos referiremos
al conjunto de todas las cadenas no vacı́as (esto es, Σ? = Σ+ ∪ {²}).
Para x ∈ Σ? , |x| es la longitud de x, esto es, el número de elementos que
forman la cadena x. Por lo tanto, |²| = 0.
El conjunto de cadenas infinitas sobre Σ se escribe Σω y el conjunto de
cadenas posiblemente infinitas Σ∞ = Σ? ∪ Σω . El concepto más importante
y el que da utilidad y sentido a la teorı́a de lenguajes es precisamente el de
lenguaje. Un lenguaje es cualquier subconjunto de Σ? . Ası́, L1 = {aa, ², bca}
y L2 = {c, cc, ccc, cccc, . . .} son lenguajes sobre el alfabeto anterior.

2.2.

Fractales sintácticos

No daremos aquı́ intencionadamente una definición detallada de fractal,
idea ésta que se irá concretando a lo largo de sucesivos capı́tulos. Preferimos
que el lector maneje la idea intuitiva de conjunto fractal que adquirió en
el capı́tulo anterior y que en éste se concretará todavı́a más sin llegar a
una definición rigurosa. Puede considerarse, por tanto, un fractal como un
subconjunto de Rn con propiedades peculiares, especialmente la de autosemejanza.
Las técnicas sintácticas para generar fractales que se discuten a continuación son una forma agradable y casi natural de familiarizarse con los
conjuntos fractales bajo R2 , aunque su generalización a espacios mayores es
casi inmediata. Una de las razones de su popularidad es que los objetos que
se procesan realmente son sı́mbolos relacionados con primitivas geométricas
y no con desarrollos numéricos que pueden ser menos sencillos de entender.
La idea es generar mediante ciertas reglas predeterminadas una secuencia de
cadenas convergente a un cierto fractal. El estudio de los fractales se traslada de esta forma, independientemente de la dimensión del espacio inicial,
al dominio de las palabras infinitas.
Aquı́ estudiaremos los sistemas D0L, que son un tipo particular de sistemas L. Los sistemas L fueron introducidos en 1968 por el matemático y
biólogo danés Aristid Lindenmeyer con el propósito de simular el crecimiento
de organismos vivos. El modelado de organismos a través de los sistemas L
permite comprobar ciertas hipótesis relativas a los mecanismos existentes
tras determinados fenómenos biológicos.
Tanto los sistemas L como los D0L son estructuras claramente discretas,
por lo que cabe preguntarse por su utilidad para aproximarse a conjuntos

2.3. SISTEMAS D0L

25

fractales. En general, no hay una dualidad directa entre un fractal en Rn y
un modelo discreto; es más, uno espera intuitivamente que muchos conjuntos (los conjuntos de Julia, por ejemplo) no puedan describirse mediante tal
modelo discreto. Con todo, los estudios realizados sobre sistemas L aseguran
que se pueda capturar mediante modelos sintácticos la estructura fractaliforme de multitud de conjuntos autosemejantes.

2.3.

Sistemas D0L

Dado un alfabeto finito Σ, los sistemas D0L generan cadenas autosemejantes al iterar un morfismo respecto a la operación de concatenación
φ : Σ? −→ Σ? (endomorfismo de Σ? ) sobre una cadena inicial s para construir la secuencia φ(s), φ2 (s) = φ(φ(s)), φ3 (s) . . . Por ser un morfismo, φ viene completamente definido por el conjunto de sus valores φ(x) para x ∈ Σ.
A pesar de su sencillez, este modelo computacional permite el cálculo de
cadenas con propiedades topológicas complejas como veremos más adelante.
Ahora formalicemos la idea anterior.

Definición 2.1 (Sistema DT0L) Un sistema DT0L es un triplete D =
(Σ, Φ, s) donde Σ es un conjunto finito, Φ es un conjunto de p morfismos
Σ? −→ Σ? y s es una cadena inicial o axioma.

Consideremos el conjunto de todas las cadenas que es posible generar
mediante la aplicación de cualquier posible secuencia de los morfismos de Φ
sobre la cadena inicial s. A este lenguaje lo designaremos por L(D).
Ejemplo Sea el sistema DT0L Γ = ({a, b}, {φ1 , φ2 }, a) con los morfismos definidos por
φ1 (a) = aba,
φ2 (a) = bab,

φ1 (b) = aa
φ2 (b) = b

Tendremos entonces
L(Γ) =

{φ1 (a), φ2 (a), φ1 (φ1 (a)), φ2 (φ1 (a)),
φ1 (φ2 (a)), φ2 (φ2 (a)), φ1 (φ1 (φ1 (a))), . . .}

= {aba, bab, abaaaaba, babbbab, aaabaaa,
bbabb, abaaaabaabaabaabaaaaba, . . .}
que es el lenguaje asociado al sistema.

26

CAPÍTULO 2. LENGUAJES FRACTALES

Un sistema D0L es un sistema DT0L con p = 1, esto es, con un único
morfismo.1 El 0 en el acrónimo D0L significa que la reescritura es independiente del contexto (la reescritura de un sı́mbolo es independiente de su
ubicación en la cadena), la D significa determinista y la L es por el apellido
del creador de los sistemas L, Lindenmeyer. En lo siguiente sólo consideraremos sistemas D0L y designaremos por φ al morfismo (único) del sistema.
Para dibujar las cadenas de L(D) utilizaremos una aplicación K : Σ? −→
R2 seguida normalmente de una función de reescalado L : R2 −→ R2 . Por
simplificar nos centraremos en representaciones sobre el plano.
Una posibilidad para K es que traduzca cada sı́mbolo de la cadena a
vectores unitarios con, posiblemente, diferentes sentidos y cada uno con origen o punto de aplicación en el extremo del vector inmediatamente anterior.
La forma de K influirá decisivamente en el tipo de conjuntos que se puedan
generar.
El cometido de L es meramente estético. La función L provoca una reducción de escala en cada iteración sucesiva de manera que el conjunto generado
queda confinado en una determinada zona del plano. De lo contrario, la expansión de la cadena inicial aumentarı́a, posiblemente exponencialmente, el
tamaño del conjunto generado.
Si la secuencia de curvas K(φ(s)), K(φ2 (s)), K(φ3 (s)), . . . converge a una curva particular C (según una métrica apropiada), entonces es razonable
considerar la cadena infinita w = lı́mn→∞ φn (s) en lugar de C e intentar encontrar propiedades combinatorias y topológicas de w en vez de caracterı́sticas geométricas en C. Se ha demostrado que bajo ciertas condiciones esta
convergencia puede ser garantizada. Nosotros no llegaremos tan lejos. En la
siguiente sección mostraremos un enfoque para caracterizar a la aplicación
K, ligeramente distinto al sugerido anteriormente con vectores unitarios,
que nos permitirá alcanzar el objetivo inicial: generar fractales mediante
sistemas D0L.

2.4.

Curvas fractales y sistemas D0L

La aplicación K, que transforma las cadenas del lenguaje asociado a un
sistema D0L en un conjunto geométrico sobre el plano, nos da la clave para
convertir cadenas autosemejantes en fractales. Una de las aproximaciones
más sencillas a la modelización de K consiste en interpretar algunos de los
1

Si en un sistema DT0L la aplicación de los distintos morfismos se lleva a cabo de forma
aleatoria puede obtenerse algo similar a los denominados fractales aleatorios, pero aquı́ nos
contentaremos con sistemas D0L y fractales autosemejantes en un sentido estricto.

2.4. CURVAS FRACTALES Y SISTEMAS D0L

27

sı́mbolos de las cadenas del lenguaje generado por un sistema D0L como
pautas de comportamiento para una tortuga geométrica al estilo de la del
lenguaje de programación Logo.
Ampliemos la definición de sistema D0L para incluir la determinación
de un ángulo cuyo significado se verá más adelante. Definamos, por tanto,
un sistema D0L modificado como D = (Σ, φ, s, α) donde todo es como antes
excepto por la aparición de α, que indica un ángulo de giro en radianes.
Además, Σ incluirá como mı́nimo el sı́mbolo F y opcionalmente alguno de
los sı́mbolos del conjunto {G, +, −, [, ]}, que tienen para nuestra aplicación
K el significado especial mostrado en la tabla 2.1, aunque pueden utilizarse
en el morfismo φ como cualquier otro sı́mbolo.
Nótese que es posible que φ mantenga invariable algún sı́mbolo de Σ
haciendo φ(x) = x para algún x ∈ Σ. De hecho, este es el comportamiento
más habitual con los sı́mbolos del conjunto {+, −, [ , ] }. De forma contraria,
es frecuente que φ(F ) 6= F y que φ(G) 6= G.

Ejemplo El sistema D0L Γ1 = ({F, G}, φ1 , F, π) con φ1 (F ) = F GF
y φ1 (G) = GGG genera cadenas que cuando son interpretadas según
la aplicación K descrita anteriormente convergen al conjunto ternario
de Cantor. El lector puede comprobarlo generando manualmente las
primeras cadenas del lenguaje. Nótese que en este caso, por tratarse
de un fractal plano, el valor del ángulo es indiferente. Se ha mantenido
en Γ1 por consistencia con la definición.

Sı́mbolo
F
G
+
−
[
]

Función
avanza un paso la tortuga dibujando
avanza un paso la tortuga sin dibujar
gira la tortuga a la izquierda α radianes
gira la tortuga a la derecha α radianes
almacena en una pila la posición y ángulo actual de
la tortuga
saca de la pila nuevos valores para la posición y el
ángulo de la tortuga

Cuadro 2.1: Algunos sı́mbolos del alfabeto del sistema D0L modificado tienen un
significado especial cuando son interpretados por la aplicación K. El número de
sı́mbolos especiales puede aumentarse para dotar de mayor poder de representación
al sistema.

28

2.5.

CAPÍTULO 2. LENGUAJES FRACTALES

Instrumentación

Las curvas fractales pueden generarse en la pantalla de un ordenador de
muy distintas formas. Dada la autorreferencia que las caracteriza, una forma
evidente (y utilizada con bastante frecuencia) es mediante algún algoritmo
recursivo. Esta es una solución bastante potente en muchas situaciones, pero
implica la elaboración de un programa para cada curva distinta y el aburrido enfrentamiento con errores inherentes a la propia programación y no a la
curva en sı́. Los sistemas D0L brindan un mecanismo elegante para representar ciertas formas fractales, permitiendo obtener con un único programa
multitud de fractales según el sistema D0L suministrado como entrada.
El mecanismo de actuación del programa serı́a el siguiente: introducido
el sistema D0L como entrada al programa, se genera la cadena derivable
tras el número de pasos de derivación indicados por el usuario. Dicha cadena se interpreta sı́mbolo a sı́mbolo según la tabla 2.1, generando la curva
en pantalla. Aquellos sı́mbolos que aparezcan en la cadena y no sean alguno de los sı́mbolos especiales, simplemente no se interpretan, procediendo
automáticamente con el sı́mbolo siguiente de la cadena.
Ejemplo El sistema D0L Γ2 = ({F, +, − [, ]}, φ2 , + + + + F, π/8) con
φ2 (F ) = F F −[−F +F +F ]+[+F −F −F ] y la identidad como imagen
de φ2 para los sı́mbolos distintos de F genera cadenas que convergen
a una especie de arbusto fractal.
Vamos a analizar los tres primeros niveles de derivación del sistema
D0L Γ2 . En el nivel cero (todavı́a no se ha realizado ninguna sustitución) la cadena es + + + + F (la cadena inicial), lo que provoca que la
tortuga gire un poco y pinte una recta. Tras la primera derivación la
cadena a interpretar es + + + + F F − [−F + F + F ] + [+F − F − F ].
La segunda derivación hace la cadena un poco más larga, resultando
+ + + + F F − [−F + F + F ] + [+F − F − F ]F F − [−F + F . . .
El lector puede generar la cadena completa e intentar dibujar su interpretación en papel. La cadena generada en el nivel 4 permite obtener
la sorprendente imagen que se muestra en la figura 2.1.

Ejemplo El sistema D0L Γ3 = ({F, X, +, −}, φ3 , F XF − −F F −
−F F, π/3) con φ3 (F ) = F F , φ3 (X) = −−F XF ++F XF ++F XF −−
y la identidad como imagen de φ3 para los sı́mbolos restantes genera
cadenas que cuando son interpretadas según la aplicación K descrita
en la tabla 2.1 convergen al triángulo de Sierpinski.

Otros fractales famosos se generan también de manera sencilla mediante
sistemas D0L. La curva de Koch o la de Hilbert son ejemplos de ello.

2.6. UN POCO DE BOTÁNICA

29

Figura 2.1: Los sistemas D0L son ideales para la modelización de plantas como ésta, obtenida del sistema Γ2 tras 4
iteraciones. Una de las primeras aplicaciones de estos sistemas fue la representación gráfica de estructuras presentes
en la naturaleza.

Ejemplo El sistema D0L Γ4 = ({F, +, −}, φ4 , F, π/3) con φ4 (F ) =
F + F − −F + F , y la identidad como imagen de φ4 para los sı́mbolos restantes genera cadenas que cuando son interpretadas según la
aplicación K descrita en la tabla 2.1 convergen a la curva de Koch.

Ejemplo El sistema D0L Γ5 = ({F, X, Y, +, −}, φ5 , X, π/2) con
φ5 (X) = −Y F + XF X + F Y −, φ5 (Y ) = +XF − Y F Y − F X+ y
la identidad como imagen de φ5 para los sı́mbolos restantes genera
cadenas que cuando son interpretadas según la aplicación K de la tabla 2.1 convergen a la curva de Hilbert.

2.6.

Un poco de Botánica

El arbusto fractal de la figura 2.1 no es un ejemplo aislado de la aproximación a la naturaleza de los fractales. Aunque operan perfectamente con
muchas de las clásicas curvas fractales, los sistemas D0L también producen
modelizaciones de plantas, árboles y arbustos de aspecto casi real. Precisamente, éste fue el primer uso que se hizo de los sistemas L (recordemos,
un superconjunto de los sistemas D0L) asociado a gráficos por ordenador.
Fueron A. R. Smith en 1984 y P. Prusinkiewicz en 1986 los creadores de este
método.
En la figura 2.2 se muestran dos plantas más generadas con sistemas D0L.
El lector puede intentar encontrar los morfismos que las generan. Aunque
no obtenga un sistema exacto, seguro que es capaz de crear un modelo muy
similar.
Dentro de los procesos de crecimiento fractal existe uno que emula con
gran realismo el crecimiento de muchas especies: la ramificación. La ramificación puede observarse en un gran número de árboles, plantas, algas, musgos,
lı́quenes y corales. Los sistemas D0L permiten generar muchas de estas pautas de ramificación tales como la ramificación dicotómica, la monopódica o
la simpódica mediante sistemas extremadamente sencillos.

30

CAPÍTULO 2. LENGUAJES FRACTALES

Figura 2.2: Las estructuras fractaliformes modelan con bastante
realismo muchos tipos de vegetación. Estas dos plantas, generadas con sistemas D0L como los
discutidos en este capı́tulo, son
un ejemplo de ello.

Dentro del cuerpo humano abundan también las estructuras fractaliformes. Las ramificaciones fractales amplı́an notablemente la superficie de las
áreas de absorción como en el intestino, de distribución o recolección como
ocurre en los vasos sanguı́neos o en los tubos bronquiales, y de proceso de
información como en las terminaciones nerviosas. Además, debido a su estructura fractal, la redundancia de operadores similares dota a estas partes
de una gran resistencia ante las lesiones. Evidentemente, aun en estos casos,
la estructura no es totalmente fractal (la ramificación no se extiende hasta el
infinito pues existe un lı́mite determinado, por ejemplo, por el nivel atómico)
pero el modelo fractal supone una excelente aproximación.
En el sentido anterior, también es imposible representar curvas fractales
por medio de un ordenador (o por cualquier medio) ya que la resolución
de pantalla o la memoria disponible imponen un lı́mite al nivel de profundización. En el caso de los sistemas D0L, una cadena w generada será un
fractal si y sólo si su longitud es infinita o, lo que es lo mismo, si y sólo si se
deriva de la cadena inicial en un número infinito de derivaciones. Esto tiene
la consecuencia de que la función que genera un fractal es no computable.
De nuevo, las aproximaciones gráficas que podemos obtener por medio de
un ordenador son más que suficientes para hacernos una idea del aspecto
final del fractal.

2.7.

Más allá de los sistemas D0L

Los sistemas utilizados pueden complicarse todo lo que uno quiera. Pueden hacerse dependientes del contexto para permitir, por ejemplo, que en la
generación de un árbol, una rama demasiado profunda se convierta en una
explosión de hojas. Pueden utilizarse distintas aplicaciones K con nuevos
sı́mbolos para manejo de color o saltar a los sistemas DT0L. Una de las
modificaciones más espectaculares permitirı́a la generación de curvas tridimensionales: la generalización de muchas de las curvas fractales presentadas
a tres dimensiones es casi inmediata. Usamos la expresión tres dimensiones
en un sentido amplio, ya que la mayor parte de dichas curvas tendrı́an una
dimensión fractal comprendida entre 2 y 3.

2.7. MÁS ALLÁ DE LOS SISTEMAS D0L

31

Los sistemas DT0L pueden considerarse como una especie de gramáticas
independientes del contexto2 en las que no hay distinción entre sı́mbolos
terminales y no terminales. Una gramática independiente del contexto GD en
forma normal de Chomsky puede derivarse fácilmente a partir de un sistema
DT0L D sustituyendo para cada a ∈ Σ el valor φ(a) = w por una regla de
derivación a −→ φ(a). GD genera codificaciones más realistas que el sistema
DT0L inicial; es más, el modelo puede mejorarse añadiendo probabilidades
a las reglas (gramáticas estocásticas).
El problema inverso todavı́a permanece poco explorado. El problema inverso consiste en calcular el sistema D0L que genera un conjunto fractal dado. Algunos desarrollos se han realizado utilizando gramáticas independientes del contexto (véase, por ejemplo, [BLA 94]). Sin embargo, esta técnica
se encuentra muy lejos en cuanto a su aplicación a la compresión de imágenes del modelo matemático en el que se centra este trabajo, los sistemas de
funciones iteradas, una evolución de la teorı́a de conjuntos autosemejantes.

2

No se describirán aquı́ las gramáticas. Puede encontrarse más información en el libro
de Hopcroft y Ullman, Introduction to Automata Theory, Languages and Computation,
Addison-Wesley, 1979.

Capı́tulo 3

Conjuntos autosemejantes
Nadie ha escapado nunca a la divertida sensación que producen algunos
libros en cuya portada un personaje muestra un libro cuya portada es igual
a la del primer libro y en la que, por tanto, aparece el mismo personaje
sosteniendo un libro con una portada igual a la del libro... Aunque, evidentemente, se trata de un montaje fotográfico y el nivel de profundización no
es infinito, no nos resulta complicado imaginar una sucesión interminable
del personaje sosteniendo un libro en el que aparece él mismo mostrando la
misma portada.
La situación anterior posee en cierta forma estructura fractal, ya que la
invarianza a escala y la autosemejanza se manifiestan de manera notoria. Las
matemáticas de los conjuntos autosemejantes modelizan el comportamiento
anterior y son la pista de despegue hacia nuestro destino: la compresión de
imágenes mediante sistemas de funciones iteradas.
En este capı́tulo se presenta también la teorı́a de los espacios métricos y
las nociones de topologı́a imprescindibles para enfrentarse a cualquier exposición seria sobre fractales. La referencia principal para todo este capı́tulo
es [GUZ 93], aunque también se presentarán ideas de [FIS 95] y [BAR 93a].
Muchas otras de las referencias que aparecen en la bibliografı́a discuten con
mayor o menor profundidad algunos de los conceptos aquı́ estudiados.

3.1.

Modelo matemático de autosemejanza

Desde un punto de vista intuitivo, un conjunto autosemejante es aquél
que puede ser descompuesto en partes, cada una de las cuales es semejante
al conjunto total.
33

34

CAPÍTULO 3. CONJUNTOS AUTOSEMEJANTES

La autosemejanza es una propiedad universalmente extendida en la naturaleza. Se han reconocido rasgos de autosemejanza en fenómenos como las
variaciones climatológicas, los flujos en régimen de turbulencia, los precios
de un mercado o la formación de masas coralinas. Los fractales que presentan
propiedades de autosemejanza en la naturaleza lo suelen hacer en un sentido
aleatorio. Aquı́, sin embargo, sólo trataremos el caso de autosemejanza en
sentido estricto o autodeterminista.
A pesar de que los conjuntos autosemejantes se encuentran entre los
primeros tipos de conjuntos fractales conocidos, su estudio sistemático no
se produjo hasta la década de los 80. Existen diferentes aproximaciones
matemáticas a la noción de autosemejanza. En este capı́tulo estudiaremos
el enfoque más extendido, que parte del crucial trabajo Fractals and selfsimilarity desarrollado por J. Hutchinson en 1981.
Una semejanza es la correspondencia que transforma una figura en otra
semejante. Una transformación Ψ de Rn es una semejanza si y sólo si para
cierto r ∈ R y para cualesquiera x, y ∈ Rn se tiene
d(Ψ(x), Ψ(y)) = rd(x, y)
donde la función d expresa la distancia entre puntos de Rn . Al factor r se le
denomina razón de semejanza y expresa la reducción o dilatación operada
sobre el tamaño de las figuras sobre las que actúa la semejanza. En el capı́tulo
siguiente se dará una descripción detallada de las ecuaciones analı́ticas de
las semejanzas del plano.
Definición 3.1 Un conjunto E ⊆ Rn es autosemejante si existe una colección Ψ1 , Ψ2 , . . . , Ψm de semejanzas de Rn , todas ellas con razones menores
a la unidad (es decir, contractivas), tales que
a) E =

Sm

i=1 Ψi (E)

b) Para cierto s (no necesariamente entero) se tiene que H s (E) > 0 y
que H s (Ψi (E) ∩ Ψj (E)) = 0, si i 6= j
La condición a) indica que el conjunto se obtiene como unión de partes semejantes al total (cada Ψi (E) es una de tales partes). La condición
b) precisa la forma en que los trozos Ψi (E) pueden solapar entre sı́ (solapamiento de las piezas que componen E), exigiendo que este solapamiento
sea despreciable en relación a la medida total de E cuando medimos en
cierta dimensión s. En la sección A.5 de los apéndices se define la medida
de Hausdorff H s (E); aunque no es un concepto trascendental para el resto
del capı́tulo, serı́a interesante su estudio detallado, ya que es una idea que
subyace en toda la teorı́a fractal.

3.2. CONJUNTOS AUTOSEMEJANTES FAMOSOS

35

La condición b) de la definición 3.1 se verifica automáticamente cuando no existe solapamiento (por ejemplo, en el conjunto de Cantor en R2 ),
pero muchas curvas autosemejantes presentan un cierto contacto entre sus
piezas (por ejemplo, en la curva de Koch los solapamientos consisten en un
único punto) por lo que no podemos relajar la condición b) por la de no
solapamiento.
Más adelante veremos que la condición b) puede ser substituida por la
condición de abierto, de utilización más simple que b), pero menos drástica
que la exigencia de solapamiento vacı́o.

3.2.

Conjuntos autosemejantes famosos

Como muestra de conjuntos autosemejantes veremos tres conjuntos célebres. En el primer caso, el conjunto de Cantor, los resultados obtenidos en
el capı́tulo 1 nos permitirán demostrar la autosemejanza del conjunto.

Conjunto de Cantor
Consideremos un sistema {Ψ1 , Ψ2 } de dos contracciones de R de ecuaciones Ψ1 (x) = x/3 y Ψ2 (x) = x/3 + 2/3. La primera transforma I = [0, 1]
en el intervalo [0, 1/3], mientras que Ψ2 (I) = [2/3, 1].
Sabemos que el conjunto de Cantor E construido en el capı́tulo 1 no es
otro que el conjunto de los números reales incluidos en I tales que en sus
expresiones decimales en base tres sólo figuran ceros y doses. Observemos
que si x es uno de tales números, x/3 también los es (la división por 3 en
base 3 se efectúa corriendo la coma decimal un lugar a la izquierda). También
x/3 + 2/3 estará en el conjunto de Cantor, ya que ahora tras correr la coma
un lugar a la izquierda sumaremos 0,2 (en base 3).
Los conjuntos Ψ1 (E) y Ψ2 (E) resultan ser aquı́, respectivamente, aquellos puntos del conjunto de Cantor cuyas expresiones decimales comienzan
por 0,0 y aquéllos que comienzan por 0,2. Entre ambos reúnen todos los
puntos de E, siendo vacı́a su intersección. Hemos probado que el conjunto
de Cantor es autosemejante con arreglo a la definición dada anteriormente.

Conjunto de Cantor en R2
Para obtener el conjunto de Cantor en R2 mostrado en la figura 3.1
partiendo de un cuadrado utilizamos un sistema de cuatro semejanzas: las

36

CAPÍTULO 3. CONJUNTOS AUTOSEMEJANTES

Figura 3.1: El conjunto de Cantor en R2 es un conjunto autosemejante bajo el
sistema de cuatro semejanzas que transforman el cuadrado inicial en cada uno de
los cuatro cuadrados de las esquinas.

I3

I2

I4

I1

I

Figura 3.2: La curva de Koch se puede construir sustituyendo el segmento I
por los segmentos I1 , I2 , I3 , I4 y repitiendo en cada uno de ellos este proceso
indefinadamente.

que transforman el cuadrado inicial en cada uno de los cuadrados pequeños
que ocupan sus cuatro esquinas. Más concretamente, si Ψ1 y Ψ2 son las
semejanzas del ejemplo anterior, nuestras cuatro semejanzas son ahora
Φij = (Ψi (x), Ψj (y)),

1 ≤ i, j ≤ 2

El conjunto E se descompone en la unión de cuatro copias semejantes,
que son precisamente las Φij (E). En este ejemplo tampoco existe solapamiento entre las copias.

Curva de Koch
Consideremos las cuatro semejanzas del plano que transforman el segmento unitario I en los cuatro segmentos de la figura 3.2.
Puede demostrarse que la curva de Koch es autosemejante respecto a es-

3.3. ESPACIOS MÉTRICOS

37

tas cuatro semejanzas, cada una de las cuales tiene razón 1/3. En el capı́tulo
siguiente se dan las ecuaciones exactas de tales semejanzas.

3.3.

Espacios métricos

Antes de profundizar en las caracterı́sticas de los conjuntos autosemejantes, es necesario mostrar algunos conceptos sobre topologı́a y espacios
métricos cuya comprensión es vital no sólo para éste, sino para el resto de
capı́tulos del libro. Aunque en un principio puedan parecer conceptos excesivamente abstractos, se verá más adelante su gran utilidad a la hora de
conformar una base teórica sólida de la geometrı́a fractal. El lector con conocimientos suficientes sobre espacios métricos, completitud, compacidad y
el teorema del punto fijo puede saltar directamente a la sección 3.4.
Se dice que d es una métrica o distancia definida en un conjunto X si
a cada par de puntos x, y ∈ X se les puede asignar un número real d(x, y)
tal que
1. Para todo x, y ∈ X, d(x, y) ≥ 0 y d(x, y) = 0 ⇔ x = y
2. Para todo x, y ∈ X, d(x, y) = d(y, x)
3. Para todo x, y, z ∈ X, d(x, z) ≤ d(x, y) + d(y, z) (desigualdad triangular)
Al par (X, d) se le denomina espacio métrico. Un ejemplo caracterı́stico
de espacio métrico es el espacio Rn con la distancia euclı́dea habitual
v
u n
uX
d(x, y) = |x − y| = t (x2k − yk2 )
k=1

con x, y ∈ Rn .
Si (X, d) es un espacio métrico, todo A ⊂ X admite de forma natural
una métrica dA , dada para x, y ∈ A por
dA (x, y) = d(x, y)
lo que convierte (A, dA ) en un espacio métrico del que se dice es subespacio
métrico de X.
En un espacio métrico (X, d), dado un punto x ∈ X y un número real
r > 0 se define bola abierta de centro x y radio r como el conjunto
B(x, r) = {y ∈ X : d(x, y) < r}

38

CAPÍTULO 3. CONJUNTOS AUTOSEMEJANTES

Si en esta definición se cambia < por ≤ se obtiene la definición de bola
cerrada con centro x y radio r.
Un subconjunto A de un espacio métrico se dice acotado si está incluido
en alguna bola del espacio métrico.
Definición 3.2 Dado un conjunto acotado A ⊂ R no vacı́o, el supremo de
A, que representaremos por sup A, cumple las dos condiciones siguientes:
a) para cualquier x ∈ A se verifica x ≤ sup A
b) dado ² > 0, por pequeño que sea, existe x ∈ A tal que x > (sup A) − ²,
es decir, un punto x tan próximo al supremo de A como queramos.
Si el conjunto A es cerrado (incluye a su frontera), no sólo ocurre lo anterior,
sino que de hecho existe x ∈ A tal que x = sup A, y entonces sup A = máx A,
esto es, el supremo se convierte en máximo. A partir de esta definición de
supremo es sencillo obtener la de ı́nfimo y mı́nimo de un conjunto.
Dado un subconjunto acotado A en un espacio métrico (X, d) se define
diámetro de A como
|A| = sup {d(x, y)}
x,y ∈A

Si A y B son conjuntos acotados de X (en particular cuando alguno de
ellos se reduce a un punto), se define distancia1 entre A y B como
d(A, B) =

ı́nf

x∈A,y∈B

{d(x, y)}

En un espacio métrico (X, d) un conjunto A se llama abierto si para
cada x ∈ A hay una bola B(x, r) ⊆ A. Un conjunto B se llama cerrado si
su complementario X − B es abierto.
En un espacio métrico (X, d) dado A ⊆ X se llama adherencia de A al
conjunto
adh(A) = {x ∈ X : para toda bola B(x, r), B(x, r) ∩ A 6= ∅}
La adherencia de un conjunto es el mı́nimo conjunto cerrado que lo contiene
y también
adh(A) = {x : d(A, x) = 0}
1

Esta distancia no coincide con la métrica de Hausdorff dH que veremos más adelante;
de hecho, ni siquiera es una métrica según la definición anterior ya que no cumple el
apartado 1.

3.3. ESPACIOS MÉTRICOS

39

Un conjunto es cerrado si y sólo si coincide con su adherencia.
Dado A ⊆ X se llama interior de A al conjunto
Int(A) = {x ∈ A : existe B(x, r) ⊆ A}
El interior de un conjunto es el mayor conjunto abierto contenido en él. Un
conjunto es abierto si y sólo si coincide con su interior.
Una sucesión {xn } de puntos de un espacio métrico (X, d) es convergente
si existe un número x que verifique que para cualquier ² > 0 existe un natural
N tal que si n > N , d(x, xn ) < ². Entonces se escribe x = lı́mn→∞ xn .
Una aplicación f : X −→ Y entre dos espacios métricos es continua en
x ∈ X si para todo ² > 0 existe δ tal que
d(x, y) < δ =⇒ d(f (x), f (y)) < ²
Si f es continua en todo punto de X, se dice que es continua en X. Una
condición necesaria y suficiente para que f sea continua en x es que, para
toda {xn } convergente a x, sea
lı́m f (xn ) = f (lı́m xn ) = f (x)
Una condición necesaria y suficiente para que f sea continua en X es que
para todo A ⊆ X sea
f (adh(A)) ⊆ adh(f (A))

Espacios métricos completos y compactos
En un espacio métrico (X, d) una sucesión {xn } se llama de Cauchy si
para todo ² > 0 existe un N tal que si p, q > N , d(xp , xq ) < ². Toda sucesión
convergente es de Cauchy, pero puede haber sucesiones de Cauchy que no
sean convergentes. Cuando toda sucesión de Cauchy es convergente a un
punto de X, al espacio métrico se le denomina completo .
Un espacio métrico es compacto si toda sucesión {xn } de puntos de
X admite una subsucesión convergente a un punto de X. Son ejemplos
caracterı́sticos de conjuntos compactos los conjuntos cerrados y acotados de
Rn . La imagen de un conjunto compacto por una aplicación continua entre
espacios métricos es un conjunto compacto.

Aplicaciones contractivas en espacios métricos
Una aplicación f : X −→ X, donde (X, d) es un espacio métrico, es
contractiva si para x, y ∈ X, d(f (x), f (y)) ≤ k · d(x, y) para cierto k < 1

40

CAPÍTULO 3. CONJUNTOS AUTOSEMEJANTES

llamado constante de la contracción, módulo de la contracción o razón de
contractividad . En estas condiciones se verifica la siguiente proposición.
Proposición 3.1 Si la aplicación f : X −→ X sobre el espacio métrico X
es contractiva, entonces se cumple:
a) f es continua
b) Si g : X −→ X es contractiva de módulo k 0 , entonces g ◦ f es contractiva de módulo k · k 0
c) f n es contractiva de módulo k n
La demostración es sencilla y puede encontrarse en [GUZ 93, p. 150].

Teorema del punto fijo
Estamos en condiciones ahora de dar un teorema vital para nuestro trabajo, ya que sin él la compresión fractal (al menos tal como hoy la conocemos) no serı́a posible. Se trata, además, de un resultado muy util en muchas
áreas de las matemáticas.
Teorema 3.1 (Teorema del punto fijo) Si X es un espacio métrico
completo y f : X −→ X es una aplicación contractiva de módulo k, entonces existe un único x ∈ X denominado punto fijo de la contracción tal
que f (x) = x.
Demostración No pueden existir x e y tales que f (x) = x y f (y) = y,
ya que en tal caso
d(f (x), f (y)) = d(x, y)
y, sin embargo, la contractividad de f impone
d(f (x), f (y)) < d(x, y)
Esto prueba que si x existe, es único.

2

Teorema 3.2 Si X es un espacio métrico completo y f : X −→ X es una
aplicación contractiva de módulo k, entonces, si x es el punto fijo de la
contracción tal que f (x) = x, se tiene que para cualquier y ∈ X
a) x = lı́mn→∞ f n (y)

3.3. ESPACIOS MÉTRICOS

41

1 d(y, f (y))
b) d(x, y) ≤ 1 −
k
Demostración Veamos en primer lugar la demostración de a). Probaremos a continuación que, dado un y ∈ X arbitrario, la sucesión cuyo término
general es yn = f n (y) es de Cauchy.
Para p ≥ 1 arbitrario
d(yp , yp+1 ) = d(f (yp−1 ), f (yp )) ≤ k · d(yp−1 , yp )
Aplicando esta fórmula repetidas veces se tiene
d(yp , yp+1 ) ≤ k p d(y0 , y1 )
Para p < q arbitrarios, en virtud de la desigualdad triangular
d(yp , yq ) ≤ d(yp , yp+1 ) + d(yp+1 , yp+2 ) + · · · + d(yq−1 , yq )
≤

∞
X

d(yi , yi+1 )

i=p

≤ d(y0 , y1 )
= d(y0 , y1 )

∞
X

ki

i=p
kp

1−k

y esta última expresión se hace más pequeña que un ² arbitrario tomando p
suficientemente grande.
Como {yn } es de Cauchy en el espacio completo (X, d), debe ser convergente. Si x es su lı́mite, en virtud de la continuidad de f se tiene
f (x) = f ( lı́m yn )
n→∞

=
=

lı́m f (yn )

n→∞

lı́m yn+1

n→∞

= x

y esto prueba que x es el punto fijo de la contracción y concluye la demostración de a).
Para demostrar b), tomando p = 0 en
d(yp , yq ) ≤ d(y0 , y1 )
se obtiene
d(y0 , yq ) ≤

kp
1−k

1
d(y0 , y1 )
1−k

42

CAPÍTULO 3. CONJUNTOS AUTOSEMEJANTES

Tomando lı́mites cuando q tiende a infinito
µ

¶

lı́m d(y0 , yq ) = d y0 , lı́m yq

q→∞

q→∞

= d(y0 , x) ≤

donde y0 ∈ X es arbitrario.

3.4.

1
d(y0 , f (y0 ))
1−k
2

Invarianza respecto a un sistema de semejanzas

Expondremos ahora un resultado que proporciona un criterio más eficaz para probar la autosemejanza de conjuntos al permitir su construcción
directa a partir de sistemas de autosemejanzas. En los ejemplos de la sección 3.2 hemos encontrado el sistema de semejanzas a posteriori basándonos
en el conocimiento que tenı́amos del proceso de construcción de los conjuntos. Además, la demostración rigurosa de la autosemejanza del conjunto de
Cantor fue posible porque disponı́amos de las expresiones decimales de sus
puntos.
Conseguiremos ahora, por tanto, un método flexible para la construcción y caracterización de autosemejantes (con el que se puede probar de
forma elemental la autosemejanza de todos los fractales mencionados en el
apartado 3.2).
Teorema 3.3 Dado un sistema S = {Ψ1 , Ψ2 , . . . , Ψm } de semejanzas contractivas de Rn (esto es, todas ellas de razón menor a la unidad) existe un
único compacto y no vacı́o E ⊂ Rn tal que
E=

m
[

Ψi (E)

i=1

Observemos que el conjunto E cuya existencia conocemos a partir de
cualquier sistema de semejanzas contractivas, dado a priori, satisface la condición a) de la definición de autosemejante, pero nada podemos asegurar
respecto de la condición b). La demostración de este teorema se irá construyendo durante las próximas páginas.

Construcción de teragones
Como ejemplo de la utilización del teorema anterior se muestra la construcción de unas curvas denominadas teragones. Se empieza con el llamado

3.5. TRANSFORMACIÓN DE UN SISTEMA DE SEMEJANZAS

43

Figura 3.3: Primeras etapas de la construcción de un teragón que llena la isla de
Koch. La primera figura es el conjunto generador.

conjunto generador F que es una curva poligonal formada por segmentos
rectilı́neos colocados de forma consecutiva en el plano.
Sean x1 , x2 , . . . , xn+1 los extremos de estos segmentos. Entonces se selecciona un sistema de semejanzas {Ψ1 , Ψ2 , . . . , Ψn } tales que Ψk transforma la
poligonal F en una poligonal semejante con extremos en xk y xk+1 , siendo
Ψk contractiva.2
Según el teorema 3.3, sabemos que debe existir un conjunto invariante para este sistema de semejanzas. De nuevo es necesario advertir que las
curvas ası́ obtenidas no tienen por qué ser conjuntos autosemejantes en el
sentido estricto de la definición, ya que, aunque se verifica la condición a)
nada sabemos de la condición b). Sólo si conseguimos probar que tal condición se verifica (o si se verifica la condición de abierto que veremos más
adelante), podremos estar seguros de la autosemejanza de la curva fractal.
La figura 3.3 muestra las primeras etapas de la generación de un teragón.

3.5.

Transformación asociada a un sistema de semejanzas

La demostración del teorema 3.3 requiere ideas y métodos fundamentales
para la geometrı́a fractal.
2
Los teragones pueden también construirse con ayuda de los sistemas D0L del capı́tulo 2.

44

CAPÍTULO 3. CONJUNTOS AUTOSEMEJANTES

Figura 3.4: La sucesión de imágenes obtenidas mediante una transformación SΨ
se estabiliza hacia el mismo conjunto independientemente del conjunto inicial como
puede observarse comparando esta figura con la 3.5.

Comenzaremos definiendo a partir del sistema de semejanzas contractivas S = {Ψ1 , Ψ2 , . . . , Ψm } una transformación de conjuntos SΨ tal que a
cada F ⊂ Rn le hace corresponder el conjunto SΨ(F ) definido por
SΨ(F ) =

m
[

Ψi (F )

i=1

En estos términos el teorema afirma la existencia de un único compacto
E tal que E = SΨ(E). Por tal razón suele llamarse a E conjunto invariante
respecto a la transformación de conjuntos S o también respecto al sistema
de semejanzas S.
La idea central que conduce a una demostración constructiva del teorema
consiste en explotar las propiedades de la transformación SΨ. La propiedad
que nos interesa puede verse en la figuras 3.4 y 3.5. Si partiendo de un
conjunto compacto F arbitrario (que puede reducirse a un único punto),
obtenemos en un ordenador imágenes de los conjuntos
SΨ(F ), SΨ2 (F ) = SΨ(SΨ(F )), . . . , SΨn (F ) = SΨ(SΨn−1 (F ))
podemos observar cómo, al aumentar n, la sucesión de imágenes se va estabilizando rápidamente hacia una forma fractal cuyo aspecto es siempre el
mismo con independencia del conjunto F de partida.
Supongamos por un momento que, tal como sugieren estos experimentos,
tuviera sentido la expresión
lı́m SΨn (F )

n→∞

y que el lı́mite conmutara con S. En tal caso
³

SΨ

´

lı́m SΨn (F ) = lı́m SΨn+1 (F ) = lı́m SΨn (F )

n→∞

n→∞

n→∞

3.6. ESPACIO (H(RN ), DH )

45

Figura 3.5: La sucesión de imágenes proporcionadas por la iteración de SΨ converge hacia el mismo fractal con independencia del conjunto inicial sobre el que se
aplicó la transformación (¡incluso aunque éste sea fractal!) como puede apreciarse
comparando esta figura y la 3.4.

lo que supondrı́a que lı́mn→∞ SΨn (F ) serı́a precisamente el conjunto invariante para el sistema de semejanzas S.

3.6.

Espacio (H(Rn ), dH )

¿Cómo formalizamos la idea anterior? ¿Qué significa lı́mite de una sucesión de conjuntos? La noción de lı́mite está muy vinculada a la de distancia.
Para poder definir el lı́mite de una sucesión de conjuntos es necesario hablar
previamente de distancia entre conjuntos. No nos vale la distancia usada
normalmente en geometrı́a como la distancia entre los puntos más próximos
de los conjuntos, ya que si los conjuntos tienen algún punto en común su
distancia es cero, aunque sean muy diferentes. La distancia que nosostros
precisamos debe ser tal que dos conjuntos próximos respecto a ella sean parecidos entre sı́. Tal requisito lo cumple la llamada distancia de Hausdorff .

Definición 3.3 Dados A y B, subconjuntos compactos y no vacı́os de Rn ,
definimos la distancia de Hausdorff entre A y B como
dH (A, B) = máx {máx {mı́n d(x, y)}, máx {mı́n d(x, y)}}
x∈B

y∈A

x∈A

y∈B

donde d(x, y) expresa la distancia habitual entre puntos de Rn .
Una forma alternativa para definir esta distancia se basa en la noción
de cuerpo paralelo-δ a un conjunto. Dado un conjunto A ⊆ Rn se define su

46

CAPÍTULO 3. CONJUNTOS AUTOSEMEJANTES

CP (A, δ)

A

δ

Figura 3.6: El cuerpo paralelo-δ de un conjunto A se define como el conjunto de
puntos cuya distancia a A es menor o igual a δ.

cuerpo paralelo-δ CP (A, δ) como el conjunto de puntos cuya distancia a A
es menor o igual a δ como se muestra gráficamente en la figura 3.6. Si el
conjunto A es compacto, CP (A, δ) es la unión de todas las bolas cerradas
centradas en A y con radio δ.
Dados dos conjuntos compactos A y B ⊆ Rn , si considaremos el menor
δ1 tal que A está incluido en el cuerpo paralelo-δ1 a B, y el menor δ2 tal
que B está incluido en el cuerpo paralelo-δ2 a A, entonces la distancia de
Hausdorff entre A y B es el mayor de los dos números δ1 y δ2 .
Ejemplo La distancia de Hausdorff entre un cı́rculo C de radio r y
un punto x de su borde es el diámetro del cı́rculo.

δ

r
C

x

x ∈ CP (C, 0)
C ⊆ CP (x, δ) ⇒ δ ≥ 2r

¾
⇒ dH (x, C) = 2r

La distancia de Hausdorff tiene un significado sencillo: dos conjuntos
están próximos cuando tienen parecida forma, tamaño y ubicación.
Ejemplo Sea C ⊂ [0, 1] ⊂ R el conjunto clásico de Cantor y sea Ck =
S2 k k
k
−k
i=1 Ii , k ≥ 1, la unión de los 2 intervalos cerrados de longitud 3
que se obtienen en el paso k de la construcción inductiva del conjunto
de Cantor.
Puesto que C ⊂ [0, 1], la distancia de cualquier punto de C al conjunto
[0, 1] es cero. Luego, si queremos calcular dH (C, [0, 1]) tendremos que
hallar la distancia del punto de [0, 1] que esté más alejado del conjunto
C a dicho conjunto. A la vista de la construcción de C, este punto es

3.7. TEOREMA DEL PUNTO FIJO

47

el punto central del intervalo que se elimina en la construcción de C1 ,
puesto que el resto de puntos de [0, 1] que se van quitando en el proceso
de construcción de C distan menos que él de dicho conjunto. Luego,
¯
µ
¶
µ
¶ ¯
¯1 1¯ 1
1
1 1
dH (C, [0, 1]) = d
,C = d
,
= ¯¯ − ¯¯ =
2
2 3
2 3
6
Un razonamiento análogo se puede aplicar para calcular d(C, Ck ).
Puesto que C ⊂ Ck , tendremos que buscar el punto de Ck que diste una mayor distancia del conjunto C. Este punto será el centro de
cualquiera de los intervalos que se eliminen en el proceso de construcción de Ck+1 . Puesto que todos estos intervalos tienen longitud 3−(k+1)
y sus extremos pertenecen a C se tiene
dH (C, Ck ) =

1 −(k+1)
1
·3
=
2
6 · 3k

Puede demostrarse fácilmente que la distancia de Hausdorff cumple las
propiedades esenciales de toda distancia.
La distancia de Hausdorff nos permite utilizar el espacio métrico
(H(Rn ), dH ) cuyos puntos serán los subconjuntos compactos y no vacı́os
de Rn , separados por la distancia dH .
Podemos ahora hablar de lı́mite de sucesiones, con lo que ya tiene todo
su sentido formular nuestra conjetura: el conjunto invariante para el sistema
de semejanzas es el lı́mite de la sucesión {SΨn (F )}, donde F es un compacto
no vacı́o arbitrario.
El espacio (H(Rn ), dH ) posee, además, una valiosa propiedad, la completitud; no en todos los espacios métricos las sucesiones que se estabilizan
convergen a un lı́mite.
Ejemplo La sucesión 1, 3/2, 7/5, . . . , p/q, (p + 2q)/(p + q), . . . en el
espacio métrico (Q, d(x, y) = |x − y|) se estabiliza (sus términos son
√
cada vez más parecidos), pero converge a 2 que no es racional.

Los espacios métricos en los que las sucesiones que se estabilizan (sucesiones de Cauchy) convergen a un lı́mite, se llaman espacios métricos completos. El llamado teorema de selección de Blaschke garantiza que el espacio
(H(Rn ), dH ) es completo.

3.7.

Teorema del punto fijo

Una aplicación contractiva f : Rn −→ Rn , por ser continua, transforma cada compacto A en un compacto f (A), induciendo una aplicación

48

CAPÍTULO 3. CONJUNTOS AUTOSEMEJANTES

f : H(Rn ) −→ H(Rn ). Se verifica la siguiente proposición.
Proposición 3.2 Si f : Rn −→ Rn es contractiva de módulo k, la aplicación inducida f : H(Rn ) −→ H(Rn ) es contractiva de módulo k en
(H(Rn ), dH ).
La demostración de esta proposición puede encontrarse en [GUZ 93,
p. 150]. Podemos retomar ahora el teorema 3.1 adecuándolo a nuestros intereses.
Teorema 3.4 (Teorema del punto fijo) Si (X, d) es un espacio métrico
completo y Φ es una transformación contractiva en X, es decir, si existe
una constante de contracción k < 1 tal que para un par arbitrario x, y de
puntos de X es
d(Φ(x), Φ(y)) ≤ kd(x, y)
entonces existe un único x ∈ X tal que Φ(x) = x, y, además, dado cualquier
y∈X
x = lı́m Φn (y)
Bastarı́a ahora con probar que SΨ es una transformación contractiva
en (H(Rn ), dH ) para probar la existencia de un único compacto E tal que
SΨ(E) = E. Puesto que las semejanzas contractivas son aplicaciones contractivas (para las cuales la desigualdad ≤ se convierte en igualdad), este
resultado depende ya exclusivamente de la siguiente proposición.
Proposición 3.3 Dada una colección S = {Ψ1 , Ψ2 , . . . , Ψm } de transformaciones contractivas de Rn , la transformación SΨ : H(Rn ) −→ H(Rn )
definida por
m
[

SΨ(F ) =

Ψi (F )

i=1

es contractiva en (H(Rn ), dH ) con módulo de contracción igual al máximo
de los módulos de contracción de las Ψi , 1 ≤ i ≤ m.
La demostración puede encontrarse en [GUZ 93, p. 151]. En el capı́tulo
siguiente se muestran numerosos ejemplos de aplicaciones contractivas. En
definitiva, podemos concluir el siguiente resultado.
Teorema 3.5 Dada una colección S = {Ψ1 , Ψ2 , . . . , Ψm } de semejanzas
contractivas, el conjunto E tal que
SΨ(E) =

m
[
i=1

Ψi (E) = E

3.8. CONDICIÓN DE ABIERTO

49

verifica
E = lı́m SΨn (F )
n→∞

siendo F cualquier compacto de Rn no vacı́o.
Este resultado nos proporciona un medio constructivo para la obtención
del conjunto invariante para SΨ y será fundamental para la construcción de
conjuntos autosemejantes.

3.8.

Condición de abierto

Retomemos nuestro camino volviendo a considerar la parte b) de la definición de conjunto autosemejante. Daremos una forma más sencilla de asegurar el cumplimiento de la definición de autosemejanza basada en la condición
de abierto.
Definición 3.4 Se dice que el sistema S = {Ψ1 , Ψ2 , . . . , Ψm } de semejanzas
de Rn cumple la condición de abierto si existe un conjunto acotado y abierto
V ∈ Rn tal que
SΨ(V ) ⊂ V

y

Ψi (V ) ∩ Ψj (V ) = ∅ si i 6= j

Resulta sencillo fabricar ejemplos que verifiquen la condición de abierto.
Tómese para ello, en este caso, un triángulo equilatero F como el de la
figura 3.7. Formemos imágenes semejantes a él tales que estén incluidas
en F , pero que se solapen entre sı́ como máximo en puntos del borde. En
nuestro ejemplo consideraremos los tres triángulos F1 , F2 y F3 , que solapan
solamente en tres vértices.
En tales condiciones el abierto V resultante de quitar a F el borde verifica la condición de abierto respecto de las semejanzas que transforman en
los Fi al conjunto F . El conjunto invariante para el sistema de semejanzas
ası́ construido es, en nuestro caso, el triángulo de Sierpinski.

3.9.

Red de recubrimientos básicos

Veamos cómo puede ser utilizada sistemáticamente la condición de abierto para construir el conjunto invariante E en un proceso de selección por
étapas. Utilizamos el compacto F = adh(V ) para obtener el conjunto invariante E como lı́mite de SΨn (F ).

50

CAPÍTULO 3. CONJUNTOS AUTOSEMEJANTES

F2

F1

F3
F

Figura 3.7: El abierto V resultante de quitar el borde a F verifica la condición de
abierto respecto a las semejanzas que transforman F en cada una de las tres partes
indicadas. El conjunto invariante para este sistema de semejanzas es el triángulo de
Sierpinki.

Antes hagamos una observación importante. No es posible obtener E
directamente como lı́mite de SΨn (V ) porque V no es compacto. Podrı́amos
preguntarnos por qué no exigimos directamente que V sea compacto. La
razón es que en muchas ocasiones tal compacto no puede encontrarse (por
ejemplo, en la curva de Koch o en el triángulo de Sierpinski). En estos casos
sucede que las piezas que forman V no tienen solapamiento, pero sı́ existe
solapamiento en sus fronteras.
Ahora, usando la condición de abierto, puede probarse la siguiente proposición.
Proposición 3.4 Dada el sistema de semejanzas S = {Ψ1 , Ψ2 , . . . , Ψm }
que cumple la condición de abierto respecto al conjunto V y dado F =
adh(V ) se cumple que
F ⊃ SΨ(F )
S

Demostración Sabemos por definición que SΨ(F ) = m
i=1 Ψi (F ). Ahora
bien, como cada Ψi es una semejanza y por tanto continua (ver sección 3.3),
es Ψi (F ) = Ψi (adh(V )) ⊆ adh(Ψi (V )), de donde
SΨ(F ) ⊆

m
[

adh(Ψi (V ))

i=1

= adh

Ã m
[

!

Ψi (V )

i=1

⊂ adh(V )
= F
Observemos que SΨ(F ) es un conjunto formado por m piezas de la forma

3.9. RED DE RECUBRIMIENTOS BÁSICOS

51

Ψi (F ) semejantes, por tanto, a F , y todas ellas incluidas en F .

2

Tomando en la relación SΨ(F ) ⊂ F que acabamos de demostrar imágenes por SΨ en ambos miembros sucesivas veces se obtiene F ⊃ SΨ(F ) ⊃
SΨ2 (F ) . . . ⊃ SΨk (F ) ⊃ . . .
Consideremos el conjunto
E=

∞
\

SΨi (F )

i=1

Tomando imágenes por SΨ se obtiene
SΨ(E) =

∞
\

SΨi+1 (F )

i=1
∞
\

= F∩

SΨi (F )

i=1

= F ∩E
= E
de lo que se deduce que E es precisamente el conjunto invariante para el
sistema de semejanzas S. La condición de abierto proporciona pues el tipo
de proceso geométrico de construcción que buscábamos.
Profundizaremos ahora en este proceso, investigando más detalladamente las propiedades de la colección de recubrimientos SΨk (F ) a la que llamaremos red de recubrimientos básicos (sabemos que para todo k es E ⊂ SΨk (F ),
luego tales conjuntos son, efectivamente, recubrimientos de E que actúan
como una colección de filtros con poros cada vez más finos; tras filtrar al
conjunto F a través de todos ellos, lo que resta es el conjunto E). Cada
SΨk (F ) es una aproximación a E (recuérdese que E se obtiene como lı́mite
de los SΨk (F )).
Para SΨ2 (F ) se tiene
2

SΨ (F ) = SΨ

Ã m
[

!

Ψi (F )

i=1

=

m
[

Ψj

Ã m
[

j=1

Ψi (F )

i=1

[

=

!

Ψj ◦ Ψi (F )

1≤i,j≤m

que, si convenimos en definir Fi,j = Ψi ◦ Ψj (F ) para cada i, j, se escribirá
SΨ2 (F ) =

[
1≤i,j≤m

Fi,j

52

CAPÍTULO 3. CONJUNTOS AUTOSEMEJANTES
Esta relación se generaliza para cualquier k
SΨk (F ) =

[

Fi1 ,i2 ,...,ik

i1 ,i2 ,...,ik ∈Ak

donde Ak representa el conjunto de todas las posibles sucesiones de k números enteros comprendidos entre 1 y m, y, como antes,
Fi1 ,i2 ,...,ik = Ψi1 ◦ Ψi2 ◦ . . . Ψik (F )

(3.1)

Este proceso consiste, por tanto, en partir de un conjunto inicial F del
que seleccionamos en una primera etapa los trozos Ψi (F ), 1 ≤ i ≤ m, que
componen el conjunto SΨ(F ). En la siguiente etapa seleccionamos en cada
Ψi (F ) los m trozos que componen SΨ(Ψi (F )), y ası́ sucesivamente.
Los recubrimientos SΨk (F ) están formados por piezas de la forma
Fi1 ,i2 ,...,ik que son semejantes a F a través de la cadena de semejanzas indicadas en la ecuación 3.1. Podemos, por lo tanto, calcular con exactitud
cuál es la razón de la semejanza resultante, ya que en aplicación de sucesivas semejanzas se multiplican las razones. Esto permite el cálculo exacto
del diámetro de cada pieza
|Fi1 ,i2 ,...,ik | = ri1 ri2 · · · rik |F |
y, por consiguiente, los diámetros de todas las piezas deben tender a cero,
ya que las razones son menores que uno.

3.10.

Dimensión de Hausdorff de conjuntos autosemejantes

Obsérvese cómo, en las construcciones en las que se verifica la condición
de abierto, el solapamiento de los trozos Ψi (F ) se produce exclusivamente
en las fronteras de los mismos, es decir, los solapamientos son pequeños
en medida en comparación con F . Es natural conjeturar que en el lı́mite
también será pequeña la medida del solapamiento de los Ψi (E) en relación
a la de E si medimos el conjunto E en la dimensión adecuada; es decir, que
la condición de abierto implica la condición b) de la definición de conjunto
autosemejante.
El siguiente teorema da toda su potencia y utilidad al método de construcción de fractales que hemos desarrollado en este capı́tulo.
Teorema 3.6 Sea S = {Ψ1 , Ψ2 , . . . , Ψm } un sistema de semejanzas contractivas de Rn con razones ri , 1 ≤ i ≤ m, que verifican la condición de

3.10. DIMENSIÓN DE CONJUNTOS AUTOSEMEJANTES

53

abierto. Entonces, el compacto E invariante para S es autosemejante y tanto
su dimensión de Hausdorff como su dimensión fractal (véase el apéndice A)
son iguales y vienen dadas por el único número real no negativo s, tal que
1=

m
X

(ri )s

i=1

verificándose, además, que 0 < H s (E) < ∞.
La demostración de este teorema es bastante laboriosa y puede encontrarse en [GUZ 93, p. 112].
Comprobemos cómo puede ser utilizado este teorema para hallar la dimensión de Hausdorff de fractales autosemejantes que conocemos. En el caso
del triángulo de Sierpinski S tenemos tres semejanzas de razón 1/2 con lo
que la ecuación anterior se convierte en
µ ¶s

1=3

1
2

de donde 1/3 = (1/2)s ⇒ log 1/3 = s log 1/2 ⇒ − log 3 = −s log 2 lo que nos
da una dimensión dim(S) = s = log 3/ log 2. De igual forma puede obtenerse
que la dimensión de la curva de Koch es log 4/ log 3 y que log 2/ log 3 es la
del conjunto de Cantor en R.

Capı́tulo 4

Sistemas de funciones
iteradas
Como ya vimos en el capı́tulo anterior, J. E. Hutchinson fue en 1981
el primer matemático que, estudiando las propiedades comunes (compacidad, autosemejanza etc.) de los fractales ya conocidos, elaboró una teorı́a
unificada para la obtención de una amplia clase de conjuntos fractales: los
conjuntos autosemejantes.
En 1985, M. F. Barnsley generalizó el método de Hutchinson. Mientras que éste utiliza semejanzas contractivas, Barnsley utiliza aplicaciones
contractivas, lo que permite ampliar notablemente la familia de fractales obtenidos, de la que ahora los conjuntos autosemejantes son un subconjunto.
El método de Barnsley para la generación de conjuntos fractales que
vamos a presentar, los sistemas de funciones iteradas, se mostrará sobre Rn ,
que es el espacio natural en que lo vamos a aplicar. Debe quedar claro, en
todo caso, que los desarrollos presentados son aplicables en cualquier espacio
métrico completo.
La referencia principal sobre los sistemas de funciones iteradas es
[BAR 93a]. También seguiremos en este capı́tulo a [GUZ 93]. Además, es
posible encontrar exposiciones más o menos profundas sobre el tema en muchas otras de las referencias de la bibliografı́a.

55

56

4.1.

CAPÍTULO 4. SISTEMAS DE FUNCIONES ITERADAS

El espacio de los fractales

Debe tenerse muy en cuenta que durante este capı́tulo (y sólo durante
él) consideraremos fractal en sentido amplio a todo conjunto compacto, es
decir, a cualquier conjunto no vacı́o acotado y que contenga a su frontera.
Esta consideración surge del hecho de poder unificar bajo un nombre común
a todos los conjuntos que se pueden derivar de un sistema de funciones iteradas, independientemente de que posean o no estructura fractal. De cualquier
modo, los resultados obtenidos serán aplicados únicamente a los auténticos
conjuntos fractales.
Llamaremos, por tanto, fractal a cualquier subconjunto compacto y no
vacı́o de Rn y espacio de los fractales, o espacio donde van a vivir los fractales, de Rn al conjunto de todos los fractales de dicho espacio, es decir, al
conjunto
H(Rn ) = {K : K ⊂ Rn , K 6= ∅ y K es compacto}
Puesto que aquı́ vamos a tratar sobre el problema de aproximar objetos
naturales (fractales en un cierto espacio Rn ) mediante fractales que nosotros podamos generar, es necesario disponer de una métrica que nos dé la
distancia entre elementos del espacio H(Rn ). Nosotros consideraremos la
métrica de Hausdorff dH que ya vimos en 3.6. Como ya se dijo allı́, dH es
una métrica sobre el espacio H(Rn ) y el espacio de los fractales (H(Rn ), dH )
es un espacio métrico completo.

4.2.

Aplicaciones contractivas

Para construir un fractal autosemejante partı́amos de un número finito
de transformaciones que eran semejanzas contractivas; aquı́ vamos a estudiar
lo que ocurre cuando el conjunto de transformaciones está formado por aplicaciones de una clase mucho más amplia: aplicaciones contractivas. Como
ya vimos en el capı́tulo anterior, toda aplicación contractiva es continua e induce una aplicación contractiva en el espacio métrico completo (H(Rn ), dH )
de igual razón.
Intuitivamente una aplicación contractiva f : Rn −→ Rn es aquella que
acerca los puntos y contrae las figuras como se refleja en la figura 4.1.
Entre dos figuras semejantes y distintas del plano euclı́deo siempre existe una aplicación contractiva que transforma la mayor en la menor. Esta
aplicación contractiva es una composición de isometrı́as (traslaciones, giros

4.2. APLICACIONES CONTRACTIVAS

57

a
A
f (a)

f

f (A)

f (b)

b

Figura 4.1: Una aplicación contractiva f acerca los puntos y contrae, por tanto,
los conjuntos sobre los que se aplica.

y simetrı́as) y una homotecia contractiva. A continuación se muestran las
transformaciones elementales del plano euclı́deo. Cualquier otro giro, simetrı́a u homotecia se puede obtener por composición de las transformaciones
elementales siguientes.

1. Traslación de vector (α, β):
y

f (R)
R

Ã

f

x
y

!

Ã

=

1 0
0 1

!Ã

x
y

! Ã

+

α
β

!

(α,β)
x
0

2. Giro del ángulo ϕ y centro en el origen:
y

f (R)

Ã

f
R
ϕ
x
0

x
y

!

Ã

=

cos ϕ − sen ϕ
sen ϕ cos ϕ

!Ã

x
y

!

58

CAPÍTULO 4. SISTEMAS DE FUNCIONES ITERADAS
3. Simetrı́a respecto del eje de abcisas:
y

R

Ã

x

f

0

!

x
y

Ã

=

1 0
0 −1

!Ã

x
y

!

f (R)

4. Homotecia centrada en el origen de razón k:
y

Ã

f (R)

f

x
y

!

Ã

=

k 0
0 k

!Ã

x
y

!

R

x
0

Si las isometrı́as y homotecias que definen una aplicación contractiva
son fáciles de determinar, entonces dicha aplicación se puede obtener como
composición de las mismas. Si, por el contrario, son difı́ciles de determinar, se
puede proceder directamente a calcular la semejanza teniendo en cuenta que
toda semejanza es una transformación afı́n y que, por tanto, sus ecuaciones
son
f (x, y) = f (ax + by + e, cx + dy + f )
o bien

Ã

f

x
y

!

Ã

=

a b
c d

!Ã

x
y

!

Ã

+

e
f

!

Para determinar los coeficientes a, b, c, d, e y f se procede a determinar
las imágenes de tres puntos y a resolver el correspondiente sistema de seis

4.2. APLICACIONES CONTRACTIVAS

59

y

T3

1
T

T1

T2
x
1/3

2/3

1

Figura 4.2: Cada parte Ti , 1 ≤ i ≤ 3, del triángulo de Sierpinski es semejante al
triángulo total T .

ecuaciones con seis incógnitas que nos dará sus valores:
f (x1 , y1 ) = (ax1 + by1 + e, cx1 + dy1 + f ) = (x01 , y10 )
f (x2 , y2 ) = (ax2 + by2 + e, cx2 + dy2 + f ) = (x02 , y20 )
f (x3 , y3 ) = (ax3 + by3 + e, cx3 + dy3 + f ) = (x03 , y30 )
Veamos algunos ejemplos.
Ejemplo Consideremos el triángulo de Sierpinski T ⊂ R2 y la representación de la figura 4.2.
Podemos poner T = T1 ∪ T2 ∪ T3 , siendo T1 , T2 y T3 las partes del
triángulo de Sierpinski que caen dentro de los tres triángulos de lado
1/3 que aparecen en la figura. Cada parte Ti , 1 ≤ i ≤ 3, del triángulo de
Sierpinski es semejante al conjunto total T . Luego existirán semejanzas
contractivas f1 , f2 y f3 tales que fi (T ) = Ti , 1 ≤ i ≤ 3, que hacen que
T = f1 (T ) ∪ f2 (T ) ∪ f3 (T )
Vamos a determinar esas transformaciones. La semejanza f1 es una
homotecia de centro el origen y razón 1/3, luego
³x y´
,
f1 (x, y) =
3 3
o matricialmente
µ
f1

x
y

¶

µ
=

1
3

0

0
1
3

¶µ

x
y

¶

60

CAPÍTULO 4. SISTEMAS DE FUNCIONES ITERADAS
y

K
K2

K3

K1

K

4
x

1/3

2/3

1

Figura 4.3: Cada una de las partes Ki , 1 ≤ i ≤ 4, de la curva de Koch indicadas
es semejante a la curva total K.

La semejanza f2 es una homotecia de centro el origen y razón 1/3,
seguida de una traslación de vector (2/3, 0), luego
³x y ´ µ2 ¶
f2 (x, y) =
,
+
,0
3 3
3
o bien

µ
f2

x
y

¶

µ
=

1
3

0

0

¶µ

1
3

x
y

¶

µ
+

2
3

¶

0

y la semejanza f3 es una homotecia de centro el origen y razón 1/3
seguida de una traslación de vector ((2/3) cos 60◦ , (2/3) sen 60◦ ), luego
¶
³x y ´ µ2
◦ 2
◦
f3 (x, y) =
,
+
cos 60 , sen 60
3 3
3
3
o en forma matricial
µ
¶ µ 1
x
3
f3
=
y
0

0

¶µ

1
3

x
y

¶
+

2
3

µ

cos 60◦
sen 60◦

¶

Es fácil observar que cada una de las aplicaciones contractivas fi , 1 ≤
i ≤ 3, tiene razón 1/3.
Ejemplo

S4
Consideremos la curva de Koch K ⊂ R2 . Entonces K = i=1 Ki
siendo Ki , 1 ≤ i ≤ 4, las partes de la curva de Koch que se indican en
la figura 4.3 y que son semejantes a la curva total K. Luego existirán
semejanzas contractivas fi , 1 ≤ i ≤ 4, tales que fi (K) = Ki y, por
tanto, tales que
4
[
K=
fi (K)
i=1

Vamos a determinar estas transformaciones. La semejanza f1 es una
homotecia de centro el origen y razón 1/3, luego
¶µ
µ
¶
¶ µ 1
0
x
x
3
f1
=
y
y
0 31

4.2. APLICACIONES CONTRACTIVAS

61

o lo que es lo mismo

³x y´
,
3 3
La semejanza f2 es una homotecia de centro el origen y razón 1/3,
seguida de un giro de centro el origen y ángulo 60◦ y seguido de una
traslación de vector (1/3, 0), luego
¶µ 1
¶µ
¶ µ 1 ¶
µ
¶ µ
0
x
x
cos 60◦ − sen 60◦
3
3
+
f2
=
y
y
sen 60◦
cos 60◦
0
0 13
f1 (x, y) =

y desarrollando
µ
f2 (x, y) =

x cos 60◦ − y sen 60◦ + 1 x sen 60◦ + y cos 60◦
,
3
3

¶

La semejanza f3 es una homotecia de centro el origen y razón 1/3,
seguida de un giro de √centro el origen
y ángulo −60◦ y seguida de una
√
√
traslación de vector ( 33 cos 30◦ , 33 sen 30◦ ) = (1/2, 3/6), luego
µ
¶
x
f3
=
y
µ
¶µ 1
¶µ
¶ µ 1 ¶
cos(−60◦ ) − sen(−60◦ )
0
x
√2
3
+
3
sen(−60◦ )
cos(−60◦ )
y
0 13
6
y desarrollando
Ã
√ !
1 −x sen 60◦ + y cos 60◦
x cos 60◦ + y sen 60◦
3
+ ,
+
f3 (x, y) =
3
2
3
6
Por último, la semejanza f4 es una homotecia de centro el origen y
razón 1/3, seguida de una traslación de vector (2/3, 0), luego
µ
¶ µ 1
¶µ
¶ µ 2 ¶
x
0
x
3
3
f4
=
+
y
y
0
0 31
o bien

µ
f4 (x, y) =

x+2 y
,
3
3

¶

Todas las aplicaciones fi , 1 ≤ i ≤ 4, son contractivas de razón 1/3.

Si f : Rn −→ Rn es una aplicación contractiva, entonces la aplicación
f : H(Rn ) −→ H(Rn ) es también contractiva. Aplicando el teorema del
punto fijo de la página 40 a la aplicación f en Rn existirá un único punto
xf ∈ Rn tal que f (xf ) = xf y aplicándolo a f en (H(Rn ), dH ) existirá un
único conjunto Kf ⊂ Rn compacto y no vacı́o Kf ∈ H(Rn ) tal que f (Kf ) =
Kf y
lı́m f k (B) = Kf , para todo B ∈ H(Rn )
k→∞

en la métrica de Hausdorff.

62

CAPÍTULO 4. SISTEMAS DE FUNCIONES ITERADAS

Una familia finita de aplicaciones contractivas definidas sobre un mismo
espacio Rn es lo que llamaremos un sistema de funciones iteradas. Más
concretamente:
Definición 4.1 Llamaremos sistema de funciones iteradas (SFI) en Rn a
n
n
cualquier familia finita {fi }N
i=1 de aplicaciones contractivas fi : R −→ R ,
1 ≤ i ≤ N . Tal sistema de funciones iteradas se representará por
{f1 , f2 , . . . , fN }
y llamaremos razón de contractividad del SFI a
r = máx{r1 , r2 , . . . , rN }
donde ri , 0 ≤ ri < 1, es la razón de contractividad de fi (obviamente,
0 ≤ r < 1).
Ejemplo En R2 , sea fi la aplicación contractiva que transforma el
triángulo de Sierpinski T = T1 ∪ T2 ∪ T3 en Ti , 1 ≤ i ≤ 3, según la
figura 4.2. Estas aplicaciones son las presentadas en el ejemplo de la
pagina 59.
Entonces {f1 , f2 , f3 } es un SFI y, puesto que la razón de cada una
de las aplicaciones contractivas fi , 1 ≤ i ≤ 3, es 1/3, la razón de
contractividad de este SFI es 1/3.
Ejemplo En R2 sea fi la aplicación contractiva que transforma la
curva de Koch K = K1 ∪ K2 ∪ K3 ∪ K4 en Ki , 1 ≤ i ≤ 4, de la
figura 4.3. Más concretamente, cada fi responde a la forma dada en el
ejemplo de la página 60.
Ahora {f1 , f2 , f3 , f4 } es un SFI y, puesto que la razón de cada una
de las aplicaciones contractivas fi , 1 ≤ i ≤ 4, es 1/3, la razón de
contractividad de este SFI es también 1/3.

En los dos ejemplos anteriores existe un conjunto que es igual a la unión
de sus imágenes obtenidas al aplicarle cada una de las aplicaciones contractivas. En el primer caso, si T ⊂ R2 es el triángulo de Sierpinski, se
S
cumple que T = 3i=1 fi (T ); en el segundo, si K ⊂ R2 es la curva de Koch,
S
K = 4i=1 fi (K).
Lo anterior nos sugiere plantearnos las siguientes cuestiones. Dado un
SFI {f1 , f2 , . . . , fN } en Rn ,
S

1. ¿Existirá un conjunto A ⊂ Rn tal que A = N
i=1 fi (A)? (invariante
respecto del SFI) y, si la respuesta a esta pregunta es afirmativa,

4.2. APLICACIONES CONTRACTIVAS

63

2. ¿será único?, ¿cómo se obtendrá?
Si {f1 , f2 , . . . , fN } es un SFI de razón r y K ⊂ Rn es un conjunto compacto no vacı́o, entonces fi (K), 1 ≤ i ≤ N , será también, por la continuidad
de fi , compacto y no vacı́o.
Puede demostrarse que la unión finita de conjuntos compactos es un
conjunto compacto. Con ello se tendrá que la aplicación
F : H(Rn ) −→ H(Rn )
definida por
F (K) =

N
[

fi (K),

∀K ∈ H(Rn )

i=1

está bien definida.
Los cuestiones planteadas anteriormente se traducen ahora, en el contexto de la función F , en el estudio de la existencia y unicidad de algún punto
fijo para esta aplicación, es decir, de algún conjunto A ∈ H(Rn ) tal que
F (A) =

N
[

fi (A) = A

i=1

Como ya vimos en el capı́tulo anterior, la aplicación F es contractiva de
razón r en el espacio métrico completo (H(Rn ), dH ). Aplicando el teorema
del punto fijo existirá un único A ∈ H(Rn ) tal que
F (A) = A
y, además, para todo B ∈ H(Rn ) se cumple que
lı́m F k (B) = A

k→∞

en el espacio métrico (H(Rn ), dH ).
Con todo esto hemos probado el siguiente resultado.
Teorema 4.1 Sea {f1 , f2 , . . . , fN } un sistema de funciones iteradas en Rn
de razón de contractividad r (0 ≤ r < 1). Entonces existe un único fractal
A ∈ H(Rn ) tal que
F (A) =

N
[

fi (A) = A

i=1

Además, para cualquier fractal B ∈ H(Rn ) se cumple
lı́m F k (B) = A

k→∞

en el espacio métrico completo (H(Rn ), dH ).

64

CAPÍTULO 4. SISTEMAS DE FUNCIONES ITERADAS

Definición 4.2 Sea {f1 , f2 , . . . , fN } un SFI sobre Rn . Se llama atractor del
SFI al único fractal A ∈ H(Rn ) que verifica
F (A) =

N
[

fi (A) = A

i=1

cuya existencia y unicidad queda asegurada por el teorema anterior.
Si A es el atractor asociado a un SFI {f1 , f2 , . . . , fN } de razón r, el
teorema anterior nos sugiere un método para la obtención del conjunto A.
Este método consiste en partir de un conjunto compacto y no vacı́o B ⊂
Rn e iterar la aplicación F sobre B, hallando los primeros términos de la
sucesión {F k (B)}∞
k=0 . El teorema 3.2 nos proporciona también un método
para calcular en cada paso, una cota de la distancia de Hausdorff entre el
atractor A y su aproximación F k (B). Esta fórmula es
dH (F k (B), A) ≤

1
· dH (F k (B), F k+1 (B))
1−r

Veamos a continuación unos ejemplos de SFI en los que determinaremos el atractor a través de estas aproximaciones ası́ como la distancia de
Hausdorff entre el atractor y sus aproximaciones.
Ejemplo Sean fi : R −→ R, i = 1, 2, las aplicaciones contractivas
definidas por
x
x+2
f1 (x) =
y
f2 (x) =
3
3
ambas de razón 1/3. Entonces {f1 , f2 } es un SFI de razón r = 1/3 cuyo
atractor es el conjunto clásico de Cantor C ⊂ R, ya que este conjunto,
como hemos visto, verifica que C = f1 (C) ∪ f2 (C).
Vamos a aplicar el proceso iterativo de obtención del atractor, sugerido por el teorema anterior, partiendo del conjunto B = [0, 1] ⊂ R.
Entonces
B =
F (B) =
=
F 2 (B) =
=

[0, 1]
f1 (B) ∪ f2 (B)
·
¸ ·
¸
1
2
0,
∪ ,1
3
3
f1 (F (B)) ∪ f2 (F (B))
¸ ·
¸ ·
¸ ·
¸
·
2 3
6 7
8
1
∪ ,
∪ ,
∪ ,1
0,
9
9 9
9 9
9

que, como se puede observar en la figura 4.4, nos va dando los intervalos
que generan por inducción el conjunto clásico de Cantor.

4.2. APLICACIONES CONTRACTIVAS

65

1

B
0

1
1/3

F(B)
1/9

F2(B)
1/27

F3(B)

Figura 4.4: Intervalos convergentes al conjunto de Cantor obtenidos mediante el
SFI f1 (x) = x/3 y f2 (x) = (x + 2)/3 a partir del intervalo unidad.

Teniendo en cuenta que la distancia de Hausdorff entre dos conjuntos es
la máxima distancia entre un punto de un conjunto y el otro conjunto,
se puede observar que
dH (B, F (B)) =
dH (F (B), F 2 (B)) =
dH (F 2 (B), F 3 (B)) =

1
6
1
18
1
54

y, en general,
dH (F k (B), F k+1 (B)) =

1
,
2 · 3k+1

∀k ≥ 0

Luego, una cota de la distancia de Hausdorff entre el conjunto de Cantor C (atractor) y sus aproximaciones es
1
dH (F k (B), F k+1 (B))
1−r
1
1
=
1 − 13 2 · 3k+1
3
1
=
2 2 · 3k+1
1
=
, k≥0
4 · 3k
lo que nos permite hallar el conjunto de Cantor con la aproximación
deseada. Es obvio que la obtención del atractor la podı́amos haber
abordado desde cualquier conjunto B ⊂ R compacto y no vacı́o.
dH (C, F k (B))

≤

Ejemplo Sean fi : R2 −→ R2 , 1 ≤ i ≤ 3, las aplicaciones contractivas
definidas por
f1 (x, y)
f2 (x, y)

= r(x, y)

= r(x, y) + (1 − r, 0)
Ã
√ ! Ã √ !
1
3
1
3
f3 (x, y) = r x − , y −
+
,
2
2
2 2

66

CAPÍTULO 4. SISTEMAS DE FUNCIONES ITERADAS

B

2

F (B)

1

F (B)

r2

r

Figura 4.5: Primeras iteraciones del SFI asociado al triángulo de Sierpinski a
partir de un triángulo de lado unidad.

con 0 < r ≤ 1/2, siendo r la razón de contractividad de cada una de
ellas. Entonces {f1 , f2 , f3 } es un SFI de razón r, cuyo atractor será un
cierto conjunto Tr ∈ H(Rn ) con n = 2 tal que
Tr =

3
[

fi (Tr )

i=1

Conviene observar que si r = 1/3, entonces el atractor T1/3 es el
triángulo de Sierpinski. Para un r genérico, obtendremos el triángulo generalizado de Sierpinski . Vamos a aplicar el proceso iterativo de
obtención del√atractor al triángulo con vértices en los puntos (0,0),
(1,0) y (1/2, 3/2). Sea B este triángulo. Las primeras iteraciones de
este conjunto B se pueden ver en la figura 4.5.
A partir de la figura 4.5 se puede ver mediante cálculos geométricos
elementales que
v
Ã √ !2
uµ
√ µ
¶
³ r ´2
u 1 − 2r ¶2
3
3 2
t
dH (B, F (B)) =
+
−
=
−r
2
6
2
2
3
En general, y por semejanza, se tiene que
√

k

dH (F (B), F

k+1

3 k
(B)) =
r
2

µ

¶
2
−r ,
3

∀k ≥ 0

Luego una cota de la distancia de Hausdorff entre el atractor Tr y sus
primeras aproximaciones es
dH (Tr , F k (B))

≤
=
=

1
dH (F k (B), F k+1 (B))
1−r
√
1
3 k 2 − 3r
r
1−r 2
3
rk (2 − 3r)
√
2 3 (1 − r)

4.3. OBTENCIÓN DEL FRACTAL ASOCIADO A UN SFI

4.3.

67

Obtención del fractal asociado a un SFI

Sólo consideraremos aquı́ el caso de los SFI definidos sobre R2 por ser
de más sencilla elaboración. Describiremos dos algoritmos distintos, uno
determinista y otro aleatorio. Ambos, sin embargo, proporcionan el mismo
resultado.

Algoritmo determinista
Las pautas anteriores para la obtención del atractor de un SFI pueden
resumirse en el siguiente algoritmo.
1.
2.
3.
4.

Elegir un conjunto arbitrario B ⊂ X compacto y no vacı́o
Hacer Z = B
Representar Z
Hacer desde i = 1 hasta M
4.1. Borrar Z
S
4.2. Hallar F (Z) = N
i=1 fi (Z)
4.3. Hacer Z = F (Z)
4.4. Representar Z
5. Fin

Cuando este algoritmo termine de ejecutarse habremos obtenido F M (B)
que para M = 10 nos da, en general, una muy buena aproximación al atractor A.
El SFI asociado al triángulo de Sierpinski de razón r = 1/2 construido
sobre el triángulo isósceles cuya base y altura coinciden con la base y altura
de una ventana 100 × 100 es {f1 , f2 , f3 } donde
Ã

f1
Ã

f2
Ã

f3

x
y
x
y
x
y

!

Ã

=
!

Ã

=
!

Ã

=

1
2

0

0

1
2

1
2

0

0

1
2

1
2

0

0

1
2

!Ã
!Ã
!Ã

x
y
x
y
x
y

!

Ã

+
!

Ã

+
!

Ã

+

1
1

!

50
1
25
50

!
!

que se puede expresar de forma simplificada como en la tabla 4.1. No debemos fijarnos, por ahora, en la columna marcada como PROB.
Vamos a ocuparnos ahora de la elaboración de un algoritmo aleatorio
para la obtención del fractal determinista. Queremos hacer hincapié en el

68

CAPÍTULO 4. SISTEMAS DE FUNCIONES ITERADAS
f
1
2
3

A
0.5
0.5
0.5

B
0
0
0

C
0
0
0

D
0.5
0.5
0.5

E
1
50
25

F
1
1
50

PROB
0.33
0.33
0.33

Cuadro 4.1: Notación simplificada del sistema de funciones iteradas asociado al
triángulo de Sierpinski. La columna marcada con PROB no es útil todavı́a; su
significado se discute en el apartado siguiente.

hecho de que el fractal que vamos a generar es un fractal absolutamente
determinista (el SFI que genera el fractal está unı́vocamente determinado)
y que la aleatoriedad reside únicamente en el algoritmo que lo genera.

Algoritmo aleatorio
Sea {f1 , f2 , . . . , fN } un sistema de funciones iteradas planas. Asignamos
P
a cada fi , 1 ≤ i ≤ N , una cierta probabilidad pi > 0 tal que N
i=1 pi = 1 y
realizamos el siguiente proceso iterativo.
Se elige x0 ∈ R2 arbitrario. A continuación se elige aleatoriamente
x1 ∈ {f1 (x0 ), . . . , fN (x0 )}
donde fi (x0 ), 1 ≤ i ≤ N , tiene una probabilidad pi de ser elegido. Análoga
e independientemente del paso anterior, se elige aleatoriamente
x2 ∈ {f1 (x1 ), . . . , fN (x1 )}
según la misma distribución de probabilidades. Cuando tenemos construidos
{x0 , x1 , . . . , xp }, se determina xp+1 mediante el mismo proceso anterior, es
decir, eligiendo de manera independiente (de los pasos anteriores) y aleatoria
xp+1 ∈ {f1 (xp ), . . . , fN (xp )}
según la misma distribución de probabilidades. Y ası́ sucesivamente. Entonces con probabilidad uno, el conjunto obtenido {xn }∞
n=0 ⊂ X converge en la
métrica de Hausdorff al atractor A del SFI en el sentido de que dado ² > 0,
existe K = K(²) ∈ N tal que
lı́m dH (A, {xn : K ≤ n ≤ M }) < ²

M →∞

De lo anterior se deduce que los puntos del conjunto {xn }∞
n=0 que pueden
estar a mayor distancia del atractor son los primeros puntos de la sucesión.

4.3. OBTENCIÓN DEL FRACTAL ASOCIADO A UN SFI

69

Por este motivo, cuando se intenta aproximar el atractor mediante este algoritmo se suelen despreciar los primeros términos (con despreciar los 50
primeros suele bastar).
Si podemos asegurar que el punto inicial considerado pertenece al atractor, x0 ∈ A, y puesto que las funciones {fi }N
i=1 no pueden sacar los puntos
del atractor A (A = ∪N
f
(A)),
entonces
podemos
asegurar
i
i=1
{x0 , x1 , . . . , xM } ⊂ A,

∀M ∈ N

y que con probabilidad uno
dH (A, {xn }∞
n=0 ) = lı́m dH (A, {x0 , x1 , . . . , xM }) = 0
M →∞

o, lo que es lo mismo, que con probabilidad uno la sucesión {xn }∞
n=0 es densa
en el atractor A:
A = adh({xn }∞
n=0 )
Estos resultados están basados en la teorı́a ergódica y su fundamentación
puede estudiarse en el capı́tulo cuarto de [BAR 93a].
Cuando se quiere aproximar un atractor mediante este algoritmo, nos
interesa obtener la mejor aproximación con el menor número de puntos. Si
la masa (medida) acumulada en cada fi (A), 1 ≤ i ≤ N , es aproximadamente
la misma, entonces es conveniente elegir pi = 1/N , 1 ≤ i ≤ N . Este es el
caso, por ejemplo, del triángulo de Sierpinski (p1 = p2 = p3 = 1/3). Si no es
ası́, conviene elegir las probabilidades aproximadamente proporcionales a la
cantidad de masa que hay en cada fi (A).
En este último caso se puede elegir un cierto conjunto W ⊂ R2 de área
no nula y fácil de calcular y elegir
área(fi (W ))
,
pi ≈ P N
j=1 área(fj (W ))

1≤i≤N

P

de tal forma que N
i=1 pi = 1. En el caso particular de que las aplicaciones
contractivas sean transformaciones afines, es decir,
Ã

fi

x
y

!

Ã

=

ai bi
ci di

!Ã

x
y

!

Ã

+

ei
fi

!

,

1≤i≤N

entonces se puede elegir
|ai di − bi ci |
pi ≈ P N
j=1 |aj dj − bj cj |
donde ≈ significa aproximadamente igual para indicar también que si algún
pi fuese igual a cero, habrı́a que asignarle algún valor pequeño no nulo, por

70

CAPÍTULO 4. SISTEMAS DE FUNCIONES ITERADAS

ejemplo pi = 0,001, ya que en caso contrario nunca se aplicarı́a la transformación correspondiente.
Una redacción más precisa del algoritmo anterior serı́a la siguiente:
1. Elegir un punto arbitrario x ∈ R2
2. Hacer desde i = 1 hasta M
2.1. Elegir j aleatoriamente entre {1, 2, . . . , N } con
probabilidades {p1 , p2 , . . . , pN }
2.2. Hallar y = fj (x)
2.3. Hacer x = y
2.4. Si i > 50, representar x
3. Fin
Para M = 5000 tendremos, en general, una muy buena aproximación del
atractor A.
Un cambio en las probabilidades asociadas al SFI va a producir un cambio en la distribución de los M puntos que se representan, lo que producirá distintos aspectos de sombras sobre el atractor. Esto puede verificarse
con el SFI para la obtención del triángulo de Sierpinski con probabilidades
p1 = 0,6, p2 = 0,3, p3 = 0,1 mostrado en la tabla 4.2. El conjunto resultante
tras la aplicación del algoritmo aleatorio a este SFI puede observarse en la
figura 4.6.
f
1
2
3

A
0.5
0.5
0.5

B
0
0
0

C
0
0
0

D
0.5
0.5
0.5

E
0
0.5
0.25

F
0
0
0.5

PROB
0.6
0.3
0.1

Cuadro 4.2: SFI asociado a un triángulo de Sierpinski modificado mediante la
variación de las probabilidades asociadas a cada una de sus transformaciones.

4.4.

El teorema del collage

¿Se podrán representar todas las imágenes reales mediante fractales?
¿Cómo se podrá hacer, si esto es posible? Vamos a tratar de responder aquı́ a
estas preguntas. Para nosotros una imagen I será un conjunto compacto y
no vacı́o de puntos de Rn , n = 1, 2, 3. Sea I ∈ H(Rn ) una imagen real y
supongamos que existe un SFI {f1 , f2 , . . . , fN } de razón r tal que F (I) =

4.4. EL TEOREMA DEL COLLAGE

71

Figura 4.6: Triángulo de Sierpinski obtenido tras la aplicación del algoritmo aleatorio al SFI de la tabla 4.2.

SN

i= fi (I)

está suficientemente próximo a I, es decir,
dH (I, F (I)) ≤ ²

Entonces si A ∈ H(Rn ) es el atractor de este SFI, se tiene, aplicando el
teorema del punto fijo, que
dH (A, I) ≤

1
²
dH (I, F (I)) ≤
1−r
1−r

Es decir, que el atractor A del SFI se aproxima bastante a la imagen
real I siempre que ² > 0 sea suficientemente pequeño. Tenemos por tanto el
siguiente corolario del teorema del punto fijo.
Corolario 4.1 (Teorema del collage) Sea I ∈ H(Rn ) una imagen real
y dado ² > 0, sea {f1 , f2 , . . . , fN } un SFI con factor de contractividad r,
0 ≤ r < 1, tal que
dH (I, F (I)) ≤ ²
Entonces
dH (A, I) ≤

²
1−r

donde A es el atractor del SFI.
A la vista del teorema anterior se puede observar que la aproximación
del atractor A a la imagen real I será mejor cuanto más pequeño sea el
valor del factor de contractividad r y que esta aproximación no depende del
número de aplicaciones que forman el SFI.
La gran importancia de este sencillo resultado estriba en la posibilidad de
sustituir la imagen I real por el atractor A, siempre que la aproximación sea

72

CAPÍTULO 4. SISTEMAS DE FUNCIONES ITERADAS

lo suficientemente buena. Si el SFI correspondiente está formado por pocas
transformaciones, almacenándolo en lugar de la imagen I habremos obtenido
una reducción significativa en el espacio ocupado por la imagen. Esta fue la
idea que abrió la investigación en la compresión fractal de imágenes.

Aproximación de imágenes reales mediante SFI
Sea I ∈ H(Rn ) una imagen real. El proceso a seguir para aproximarla
mediante SFI serı́a el siguiente:
1. Encontrar aplicaciones contractivas fi : Rn −→ Rn , 1 ≤ i ≤ N , tales
que
Ã

dH I,

N
[

!

fi (I)

i=1

sea lo más pequeño posible. Sea, por ejemplo, dH (I,
Entonces
²
dH (A, I) ≤
1−r

SN

i=1 fi (I))

≤ ².

donde A es el atractor del SFI y esta aproximación será mejor cuanto
más pequeños sean ² y r. Por ello es conveniente elegir transformaciones contractivas de la menor razón posible, independientemente del
número de ellas, que puede ser tan grande como se quiera.
2. Generar el atractor A mediante cualquiera de los algoritmos descritos
anteriormente.
Una pregunta obvia es por qué no hacer que el SFI F comprima muy
ligeramente I con lo que la distancia dH (I, F (I)) será muy pequeña y
quizá dH (A, I) también lo sea. Esto no funcionará porque para tal SFI el
1
será muy grande y no podremos garantizar que dH (A, I) sea
término 1−r
pequeña (de hecho, no lo es).

Una hoja fractal
Sea I ⊂ R2 la imagen de la figura 4.7 que vamos a tratar de representar
mediante un sistema de funciones iteradas.
Para encontrar el SFI tenemos que descomponer esta imagen I en partes
de tal forma que cada una de ellas se pueda obtener a partir de la imagen
total mediante una aplicación contractiva (a ser posible afı́n). Una posible

4.4. EL TEOREMA DEL COLLAGE

73

Figura 4.7: La hoja de helecho que se intentará aproximar mediante un SFI aplicando el teorema del collage.

I1

I4

I3

I2

Figura 4.8: Cada una de las cuatro partes de la hoja del helecho aquı́ indicadas se
puede considerar como el resultado de una aplicación contractiva sobre la imagen
completa.

descomposición se ilustra en la figura 4.8. En esta descomposición utilizaS
mos 4 partes que llamamos Ii , 1 ≤ i ≤ 4, y se cumple que I = 4i=1 Ii .
Para hallar las aplicaciones que transforman la imagen total I en Ii , 1 ≤
i ≤ 4, tenemos que situar esta imagen en el plano R2 , lo que podemos hacer
como se muestra en la figura 4.9, incluyendo I en el cuadrado [−1/2, 1/2] ×
[0, 1]. De esta forma la imagen queda centrada horizontalmente en el origen
y es más fácil operar.
La aplicación f1 que nos transforma I en I1 es una homotecia centrada
en el origen de razón 3/4 seguida de un leve giro de ángulo π/32 y de una

74

CAPÍTULO 4. SISTEMAS DE FUNCIONES ITERADAS
1

1
4
1
8

0

− 12

1
2

0

Figura 4.9: Para obtener las aplicaciones contractivas que transforman la imagen
completa del helecho en cada una de las partes indicadas en la figura 4.8, tenemos
que situar la hoja en el plano R2 . Si la imagen se centra horizontalmente en el
origen, las transformaciones se obtienen de manera más comoda.

traslación al punto (0, 1/4). Luego
Ã

f1

x
y

!

Ã

=

π
cos 32
π
sen 32

π
− sen 32
π
cos 32

!Ã

3
4

0

0

!Ã

3
4

x
y

!

Ã

+

0

!

1
4

La aplicación f2 que nos transforma I en I2 es una homotecia centrada
en el origen de razón 1/4 respecto al eje de ordenadas y cero respecto al de
abcisas
Ã
! Ã
!Ã
!
x
0 0
x
=
f2
y
0 14
y
La aplicación f3 que nos tranforma I en I3 es una homotecia centrada
en el origen de razón 3/10 respecto al eje de abcisas y 2/5 respecto al de
ordenadas seguida de un giro de π/3, seguida de una traslación de vector
(0, 1/8)
Ã

f3

x
y

!

Ã

=

cos π3
sen π3

− sen π3
cos π3

!Ã

3
10

0

0
2
5

!Ã

x
y

!

Ã

+

0

!

1
8

Por último, la aplicación f4 que transforma I en I4 es una homotecia
centrada en el origen de razón 3/10 respecto al eje de abcisas y 1/2 respecto
al de ordenadas seguida de un giro de −π/4 y seguida de una traslación al

4.4. EL TEOREMA DEL COLLAGE

75

Figura 4.10: Un árbol fractal. El lector puede intentar hallar un SFI que aproxime
esta imagen. La solución en [COL 96].

punto (0, 1/8)
Ã

f4

x
y

!

Ã

=

cos −π
4
sen −π
4

− sen −π
4
cos −π
4

!Ã

3
10

0
1
2

0

!Ã

x
y

!

Ã

+

0

!

1
8

Después de asignar probabilidades, según los criterios establecidos en la
sección anterior, el SFI escrito en forma simplificada serı́a el de la tabla 4.3,
que ejecutado mediante el algoritmo aleatorio y representando 100000 puntos
nos darı́a precisamente la imagen de la figura 4.7.
f
1
2
3
4

A
0.746
0
0.15
0.212

B
-0.073
0
-0.344
0.353

C
0.073
0
0.258
-0.212

D
0.746
0.25
0.2
0.353

E
0
0
0
0

F
0.25
0
0.125
0.125

PROB
0.65
0.03
0.14
0.18

Cuadro 4.3: Aproximación mediante el teorema del collage a la hoja de helecho.
Las probabilidades se asignaron en función del área generada por cada transformación.

Como ejercicio el lector puede intentar ahora obtener un SFI que aproxime el árbol mostrado en la figura 4.10. Pista: un posible SFI tiene cinco
transformaciones de las cuales dos conforman la parte inferior del tronco.

76

4.5.

CAPÍTULO 4. SISTEMAS DE FUNCIONES ITERADAS

Fractales en movimiento

Nos vamos a ocupar aquı́ de la posibilidad de establecer movimiento en
los conjuntos fractales. Puesto que los conjuntos fractales que hemos considerado en este capı́tulo dependen directamente de una familia de funciones
contractivas parece razonable esperar que pequeñas variaciones en estas funciones produzcan pequeñas variaciones en el fractal generado. Si esto fuera
ası́, podrı́amos producir con una sucesión de fractales muy próximos entre
sı́ un efecto de movimiento.
Supongamos que las aplicaciones contractivas que definen un SFI
{f1 , f2 , . . . , fN } no vienen unı́vocamente determinadas, sino que están definidas en función de un parámetro p ∈ [α, β] ⊂ R del que dependen continuamente. El siguiente teorema determina cómo influyen en el atractor
pequeñas variaciones del parámetro p.
Teorema 4.2 Para cada p ∈ [α, β] ⊂ R sea {f1 (p), . . . , fN (p)} un SFI de
razón r(p), 0 ≤ r(p) ≤ r < 1, con atractor A(p) ∈ H(Rn ). Supongamos que
cada transformación fi (p)(x) = fi (p, x) es continua para todo x respecto de
p en [α, β]. Entonces el atractor A(p) depende continuamente de p ∈ [α, β].
La demostración, algo técnica, puede encontrarse en [GUZ 93, p. 201].
Este teorema se puede utilizar para la animación de imágenes. Aunque no
entraremos en más detalles, mostraremos un ejemplo.
Ejemplo Si en la definición del SFI de la hoja dado en la sección
anterior sustituimos la aplicación f1 por
¶µ
¶ µ
¶
µ
¶ µ
¶µ 3
0
0
x
cos α − sen α
x
4
+
f1
=
1
y
y
sen α
cos α
0 34
4
entonces {f1 (α), f2 , f3 , f4 } es un SFI de razón 3/4 que depende continuamente del parámetro α ∈ [−π/2, π/2]. Luego el atractor A(α)
dependerá continuamente de α. Al variar continuamente α se obtiene
un efecto de movimiento de la hoja. Variando α según los valores
α1

= arc sen 0,15

α2
α3
α4
α5

=
=
=
=

α6

= arc sen −0,1

arc sen 0,1
arc sen 0,05
arc sen 0
arc sen −0,05

puede obtenerse la sucesión de imágenes mostrada en la figura 4.11.

4.6. LOS CONJUNTOS DE JULIA COMO SFI

77

(a) ω = 0,15

(b) ω = 0,1

(c) ω = 0,05

(d) ω = 0

(e) ω = −0,05

(f) ω = −0,1

Figura 4.11: La hoja de helecho agitada por el viento mediante distintos valores
del parámetro α. Los valores dados a α son α = arc sen ω donde ω evoluciona según
se indica bajo cada figura. El movimiento de la hoja se puede observar al seguir las
imágenes de izquierda a derecha y de arriba a abajo.

4.6.

Los conjuntos de Julia como SFI

En la sección 1.3 se introdujeron los conjuntos de Julia y se mostró un
algoritmo para su obtención. Al objeto de encontrar el conjunto de Julia
asociado a un sistema dinámico complejo cuadrático fc (z) = z 2 + c para un
valor complejo de c fijo y arbitrario, se puede intentar efectuar el proceso
inverso al de huida de sus puntos, con lo que nos acercarı́amos a él. Es decir,
se tratarı́a de iterar la transformación inversa
√
fc−1 (z) = ± z − c
sobre ciertos conjuntos del plano complejo, hasta que sus órbitas cayeran
sobre el conjunto de Julia.
Esto resulta ser cierto en cualquiera de los casos que consideremos, es
decir, que si c = c1 + c2 j es un número complejo arbitrario, entonces
lı́m f −n (K)
n→∞ c

= J(fc )

para cualquier subconjunto no vacı́o K del plano complejo, donde J(fc )
representa el conjunto de Julia asociado a la función cuadrática de parámetro
c y la convergencia se entiende en la métrica de Hausdorff.

78

CAPÍTULO 4. SISTEMAS DE FUNCIONES ITERADAS

√
La transformación
inversa fc−1 consta de dos funciones g1 (z) = + z − c
√
y g2 (z) = − z − c. Estas funciones se pueden expresar en forma cartesiana
como
g1 (x, y) = (a, b)
y
g2 (x, y) = (−a, −b)
donde, por cálculos elementales,
s

a=

(x − c1 ) +

p

(x − c1 )2 + (y − c2 )2
2

 sq



(x − c1 )2 − (y − c2 )2 − (x − c1 )


,
+


2


b=


sq





(x − c1 )2 − (y − c2 )2 − (x − c1 )

 −
,
2

si y ≥ c2

si y < c2

Es decir, asignamos a g1 la raı́z con parte real positiva y a g2 la opuesta.
Entonces, puesto que fc−1 consta de estas dos funciones, g1 y g2 , se entiende
que
fc−1 (K) = g1 (K) ∪ g2 (K)
³

´

fc−n (K) = fc−1 fc−(n−1) (K) ,

si n > 1

Luego, el proceso de construcción del conjunto de Julia, en el caso cuadrático, resulta ser análogo al proceso de construcción determinista del atractor de un SFI donde las funciones son {g1 , g2 }. Podemos aplicar entonces el
algoritmo determinista, descrito en la página 67, para la determinación del
conjunto de Julia asociado a los sistemas cuadráticos.

Capı́tulo 5

Compresión de imágenes
El volumen de datos electrónicos que maneja la humanidad crece de
forma continua. A la enorme cantidad de información en formato electrónico
que se genera cada dı́a hay que unir todo el fondo documental de la historia
que estará totalmente digitalizado para las primeras décadas del siglo XXI.
El tratamiento de esta información origina unos costes de entre los que
destacan el coste de almacenamiento y el de transmisión.
La compresión de datos trata de reducir el volumen de la información sin
que ésta deje de serlo. En esta obra se trata de la compresión de imágenes
mediante técnicas basadas en fractales.
La compresión de imagen puede ser con pérdidas y sin pérdidas. Las
técnicas sin pérdidas normalmente no logran reducir el tamaño de las imágenes más allá de su tercera parte, pero son necesarias, ya que algunas imágenes (imágenes médicas o con texto, por ejemplo) se vuelven inservibles si se
pierde alguna información en el proceso de descompresión.
Ya que el ojo humano tiene lı́mites, se puede tolerar normalmente alguna
pérdida en la imagen al descomprimirla de forma que la imagen restaurada
sea una aproximación cercana a la original. En algunos casos las imágenes
descomprimidas no parecen tener pérdidas, aunque se haya utilizado alguna técnica con pérdidas en su compresión. Existen en la actualidad varias
técnicas para compresión de imágenes con pérdidas: basadas en la cuantización vectorial [SAU 96a], la transformada discreta del coseno [WAL 91],
la transformada con wavelets [HIL 94, POL 97] o los sistemas de funciones
iteradas [FIS 95, SAU 96a, LU 97].

79

80

5.1.

CAPÍTULO 5. COMPRESIÓN DE IMÁGENES

Dos pájaros de un tiro

Los grupos de discusión de la red sobre compresión de datos se ven asaltados de tarde en tarde por acaloradas discusiones en torno al anuncio de
alguno de sus miembros de un revolucionario mecanismo de compresión sin
pérdidas que no sólo permite obtener enormes ratios de compresión sino
que, además, lo hace sobre cualquier entrada, independientemente de que
ésta proceda de un concierto de música celta, del reportaje en vı́deo sobre las pasadas vacaciones, de la guı́a telefónica o de una fuente de datos
aleatorios. Un sencillo argumento basta para demostrar que lo anterior es
imposible independientemente del método de compresión utilizado. De hecho, es imposible asegurar la compresión, incluso en un bit, de cualquier tipo
de programa.

Teorema 5.1 Es imposible comprimir sin pérdidas todos los archivos de
tamaño mayor o igual a N bits para cualquier entero N ≥ 0.

Demostración Supongamos que existe una técnica capaz de comprimir
sin pérdidas todos los archivos de tamaño igual o superior a N bits. Comprimamos con este programa todos los 2N archivos cuyo tamaño es exactamente
N bits. Todos estos ficheros comprimidos tienen como máximo N − 1 bits
por lo que habrá como máximo 2N −1 ficheros comprimidos diferentes (2N −1
archivos de tamaño N − 1, 2N −2 de tamaño N − 2, y ası́ sucesivamente hasta
llegar a un fichero de tamaño 0). Con ello al menos dos ficheros de entrada
distintos han tenido que ser comprimidos en el mismo archivo de salida.
Por lo tanto, la compresión proporcionada por esta técnica no puede ser sin
pérdidas.
2
Debe tenerse cuidado por otra parte con no caer en las garras de ciertos
programas tramposos que circulan por la red. Este tipo de programas no
comprimen en absoluto, sino que almacenan los datos originales en ficheros
ocultos del disco duro o en clusters sin utilizar. Los datos pueden descomprimirse sólo si tales ficheros ocultos no han sido borrados o si los clusters
no han sido utilizados. Si se copia el fichero comprimido a otra máquina, se
obtiene siempre un error de paridad en la estructura del fichero.
Pese a lo anterior, la historia de la compresión de datos es la historia
del esfuerzo de numerosas personas que con su creatividad y original forma
de abordarla la han llevado a convertirse en una tecnologı́a básica en la
actualidad. Un ejemplo en el que se manifiesta cómo una visión diferente de
las cosas puede llevar a resultados desconcertantes es el siguiente programa

5.2. CALIDAD DE LA COMPRESIÓN CON PÉRDIDAS

81

de autor desconocido que genera las 2400 primeras cifras del número π:1
#include <stdio.h>
long int a= 10000, b, c= 8400, d, e, f[8401], g;
main () {
for ( ;b-c; )
f[b++]= a/5;
for ( ;
d= 0, g= c*2;
c-= 14, printf ("%.4d",e+d/a), e= d%a)
for (b= c;
d+= f[b]*a, f[b]= d%--g, d/=g--, --b;
d*= b)
;
}

Puede comprobarse que el tamaño de este programa es muy inferior al
necesario para codificar la sucesión de tales cifras, incluso admitiendo 4 bits
por cifra en la sucesión y 8 bits por carácter en el programa.
Hasta ahora simplemente hemos presentado algunos hechos curiosos en
torno a la compresión de datos. En la siguiente sección el enfoque deja atrás
lo anecdótico.

5.2.

Calidad de la compresión con pérdidas

Cuando se evalúa la calidad de la imagen restaurada tras una compresión con pérdidas se suelen considerar dos cantidades bastante relacionadas.
La primera es el ratio de compresión que se define como la relación entre el tamaño de la representación original de la imagen y el de su versión
codificada.
La otra cantidad a considerar debe medir la calidad de la imagen. La mayor parte de los autores utilizan la raı́z del error cuadrático medio (RECM)
o la razón señal-ruido máxima (RSRM). Para imágenes en escala de grises
de 8 bits la RSRM se define como
255
255
RSRM = 20 log10
= 20 log10 q
P
1
RECM
(p̂i,j − pi,j )2
#pixels

i,j

donde pi,j y p̂i,j denotan respectivamente las intensidades de los pixels en la
1

Para más información, puede consultarse el número de octubre de 1994 de Investigación y Ciencia. También puede accederse a las páginas del Concurso internacional de
código C ofuscado en http://reality.sgi.com/csp/ioccc/index.html.

82

CAPÍTULO 5. COMPRESIÓN DE IMÁGENES

imagen original y en su aproximación. La RSRM expresa la relación entre
la potencia máxima de la señal y el error y se mide en decibelios.
Si un codificador permite diferentes ratios de compresión para distintas
calidades en la codificación, puede ser interesante poder comparar distintos
codificadores o los resultados de un único codificador con distintos parámetros. Para ello se recogen en un gráfico varios puntos dados por el par (ratio
de compresión,RSRM) de ambos métodos. Al conectar algunos de estos puntos se obtienen las llamadas curvas razón-distorsión. Cuanto más alta en el
gráfico esté una curva, mejor será el codificador.
Aunque la relación señal-ruido máxima y el ratio de compresión son medidas habituales en la mayor parte de artı́culos sobre compresión de imagen,
no son las únicas medidas existentes. Existe una gran controversia sobre
qué métrica es la adecuada para medir el error cometido en la compresión,
ya que ninguna de las existentes parece concordar en todos los casos con
las consideraciones de un observador humano. Un ejemplo simple y paradigmático es el desplazamiento global en las intensidades de una imagen.
Si sumamos 10 a todos los pixels de una imagen, las medidas habituales
indicarán un gran error, aunque la imagen obtenida tiene simplemente un
poco más de brillo. Por otra parte, el uso del logaritmo en la RSRM exagera
las diferencias en el rango de bajas pérdidas y las suprime en el de grandes
pérdidas como puede verse en la figura 5.1 donde se muestra la raı́z del error
cuadrático medio contra la relación señal-ruido máxima.

5.3.

Compresión de imágenes en color

Afortunadamente el sistema visual humano sólo utiliza tres canales de
color para codificar la información sobre éste que llega al cerebro. Esto significa que los colores pueden simularse mediante la superposición de tres
colores primarios, normalmente el rojo, el verde y el azul. Cuando se digitaliza una imagen en color se utilizan tres filtros para extraer las intensidades
de cada uno (estas tres intensidades se conocen como RGB por las iniciales
de los colores en inglés). Al recombinar las tres intensidades, las percibiremos
como algún color.
Cualquier método que pueda codificar imágenes monocromo, puede utilizarse para codificar imágenes en color. Pero codificar cada una de las componentes RGB de forma separada no es una opción muy recomendable: el
sistema visual humano no es especialmente sensible a la información sobre
el color y hay formas para comprimir esta información y sacar partido de
esta baja sensibilidad.

5.3. COMPRESIÓN DE IMÁGENES EN COLOR

83

55
50
45
40
RSRM
(dB)

35
30
25
20
15
10
0

10

20

30

40

50

RECM
Figura 5.1: Representación de la raı́z del error cuadrático medio contra la relación
señal-ruido máxima. Puede apreciarse como para pequeños valores de la RECM,
la RSRM potencia las diferencias, mientras que su comportamiento es el contrario
bajo valores altos de la RECM. En este trabajo se utilizará la RSRM como medida
de la distorsión de una imagen debido a su mayor popularidad, pero debe tenerse
siempre presente que para ratios de compresión elevados una diferencia pequeña
en la RSRM de dos codificaciones de una misma imagen puede significar una gran
diferencia en sus calidades.

Dado un triplete (Ri , Gi , Bi ) que describe la distribuciónes de color en
un pixel i podemos calcular los valores YIQ como










Yi
0,299 0,587
0,114
Ri

 


 Ii  =  0,596 −0,274 −0,322   Gi 
Qi
0,211 −0,523 0,312
Bi
Esta transformación proporciona los tres canales YIQ: la señal Y mide el
brillo del color (luminancia), la señal I mide el color real (matiz ) y la señal Q
la profundidad del color (saturación). Los canales I y Q se denominan también señales de crominancia. Los valores RGB originales pueden obtenerse
mediante la transformación inversa










Yi
1,000 0,956
0,621
Ri


 

 Gi  =  1,000 −0,273 −0,647   Ii 
Qi
1,000 −1,104 1,701
Bi
Es posible comprimir significativamente las señales I y Q sin ninguna
degradación aparente en la calidad de la imagen, ya que el ojo humano no
es tan sensible a la información de la crominancia como lo es para la de
la luminancia. Esta compresión puede realizarse mediante cuantización o

84

CAPÍTULO 5. COMPRESIÓN DE IMÁGENES

mediante submuestreo de los valores I y Q de grupos consecutivos de pixels,
por ejemplo, tomando el valor medio de cada bloque 4 × 4 de la imagen.

5.4.

Cuantización vectorial

La cuantización es uno de los métodos de compresión con pérdidas más
sencillos, aunque no por ello deja de proporcionar resultados aceptables.
La cuantización vectorial es una generalización de la cuantización escalar . En la cuantización escalar se representa cada valor mediante un ı́ndice
a una tabla fija formada por un subconjunto de valores representativos denominado libro de códigos. Por ejemplo, si tenemos una secuencia de datos
cada uno de ellos de 16 bits y consideramos sólo los 8 bits más significativos
de cada elemento, obtendremos una aproximación a los datos originales al
perder precisión. En este caso la tabla de códigos estarı́a formada por todos
los números de 16 bits divisibles por 256. El rango del intervalo que separa los dos valores más próximos que se pueden representar mediante esta
codificación (en este caso 256) se denomina cuanto.
En la cuantización vectorial (CV) el ı́ndice referencia a un libro de códigos formado no por valores individuales, sino por vectores. Un ejemplo tı́pico
es una imagen en color en la que cada pixel viene representado por un triplete con los valores RGB (ver sección 5.3). En la mayor parte de las imágenes
tales tripletes no cubren todo el espacio RGB, sino que tienden a concentrarse en determinadas zonas de él. Por ejemplo, una imagen de un bosque
tendrá normalmente una gran cantidad de verdes. Podemos, entonces, seleccionar un subconjunto relativamente pequeño (por ejemplo, 256 elementos)
de colores representativos (tripletes RGB) y aproximar cada pixel representándolo mediante un ı́ndice al color más cercano del libro de códigos.
El diseño de libros de códigos óptimos es muy difı́cil y en la práctica
sólo pueden obtenerse soluciones subóptimas como la proporcionada por la
iteración generalizada de Lloyd. Además, aunque el enfoque estándar de
CV produce buenos resultados, suele hacerlo con libros de códigos de tamaños prohibitivos y de elevados tiempos de cálculo en la mayor parte de
las ocasiones. Por esta razón existen muchas variaciones de la CV en las
que se utilizan libros de códigos con determinadas estructuras que los hacen
computables, aunque reducen el rendimiento en términos de calidad. Uno
de estos métodos se conoce como cuantización vectorial con eliminación de
media y ganancia de forma (CV-EMGF).
Como su nombre sugiere, un vector R ∈ Rn que va a ser codificado

5.5. EL ESTÁNDAR JPEG

85

mediante CV-EMGF se escribe como
R=s·D+o·1
(1, 1, . . . , 1)T

Rn

donde 1 =
∈
y s, o son escalares. D = (d1 , d2 , . . . , dn )T es
un vector de forma de media cero y varianza uno, esto es
n
X

di = 0,

i=1

n
X

d2i = 1

i=1

Con dos libros de códigos escalares para s y o y un libro de códigos
vectoriales de vectores de forma el vector de entrada R queda, una vez
cuantizado, como
R ≈ sı́nds (R) Dı́ndD (R) + oı́ndo (R) · 1
donde ı́nds (R), ı́ndD (R) e ı́ndo (R) son ı́ndices apropiados generados por el
cuantizador. Puede decirse que este esquema codifica de forma separada
la media, la desviación estándar y la forma de un vector dado, ya que al
considerar los tres libros de códigos simultáneamente se obtiene un libro de
códigos conjunto muy grande. Por ejemplo, si el tamaño de los libros de códigos para s, o y D es 32, 128 y 4096, respectivamente, podemos representar
de forma exacta un total de 224 vectores.

5.5.

El estándar JPEG

Durante los últimos años de la década de los ochenta y los primeros de
la de los noventa, un comité mixto ISO y CCITT conocido como JPEG
(Joint Photographic Experts Group, Grupo Mixto de Expertos Fotográficos)
trabajó para establecer el primer estándar internacional para compresión de
imágenes estáticas de tono continuo tanto en escala de grises como en color.
El denominado estándar JPEG incluye dos métodos de compresión básicos,
cada uno con varios modos de operación. El estándar especifica un método basado en la transformada discreta del coseno (TDC) para compresión
con pérdidas y un método predictivo para compresión sin pérdidas. JPEG
presenta una técnica simple con pérdidas conocida como método base, que
es un subconjunto de los otros modos de operación basados en la TDC. El
método base ha sido con diferencia el más implementado hasta la fecha y es
el que mostraremos aquı́.

Modos de operación
El estándar de compresión JPEG especifica los siguientes modos de operación:

86

CAPÍTULO 5. COMPRESIÓN DE IMÁGENES

Codificación secuencial Cada componente de la imagen se codifica en un
barrido de derecha a izquierda y de arriba a abajo.
Codificación progresiva La imagen se codifica en múltiples barridos para
poder trabajar con aplicaciones en las que el tiempo de transmisión es
largo y se prefiere ver la imagen poco a poco.
Codificación sin pérdidas Para garantizar la recuperación exacta de la
imagen original.
Codificación jerárquica La imagen se codifica bajo múltiples resoluciones
para que se pueda acceder a versiones en baja resolución sin tener que
descomprimir primero la imagen completa en toda su resolución.

A continuación veremos un resumen del método de compresión base del
estándar.

Codificación y descodificación
El estándar JPEG funciona mejor cuando se aplica a imágenes naturales.
No proporciona resultados muy buenos cuando se aplica a imágenes poco
realistas tal como dibujos con lı́neas. Si se pretende utilizar una imagen para
su tratamiento digital, los pequeños errores introducidos por la codificación
pueden ocasionar problemas graves en los algoritmos clásicos de tratamiento de imágenes, aunque dichos errores sean invisibles para el ojo humano.
Esta es una limitación compartida por todos los esquemas de compresión
discutidos en esta obra.
Además las imágenes deben de ser de tono continuo; imágenes con muchos saltos bruscos en sus intensidades no se comprimen bien.
La figura 5.2 muestra la sucesión de etapas claves de los modos de operación basados en la TDC. Ya que aquı́ nos interesa la compresión de imagen
con pérdidas, sólo abordaremos este aspecto del método JPEG. Además,
las explicaciones siguientes se harán considerando que las imágenes están en
escala de grises. Si la imagen es en color, se puede optar por considerarla
como tres imágenes en escala de grises asociadas cada una con las componentes de rojo, verde y azul de la imagen, o bien proceder a la recodificación
propuesta en 5.3. Veamos brevemente cada una de las etapas.

5.5. EL ESTÁNDAR JPEG

87
Compresor

bloques 8x8

TDC

Cuantizador

TDC inversa

Codificador

Especificaciones

Imagen comprimida

Descuantizador

Descodificador

Descompresor

Figura 5.2: Las distintas etapas que conforman el estándar de compresión JPEG
tanto en la compresión como en la descompresión. De todas ellas la principal fuente
de pérdidas es la etapa de cuantización. La codificación por entropı́a no introduce
ningún tipo de pérdida como tampoco lo harı́a el cálculo de la transformada del
coseno de no ser por la imposibilidad de obtener sus coeficientes con total precisión.

TDC y TDCI
A la entrada del codificador las muestras de la imagen se agrupan en bloques de 8 × 8 pixels, desplazando sus valores de enteros sin signo en el rango
[0, 2p − 1] a enteros con signo en el rango [−2p−1 , 2p−1 − 1] e introduciéndolos
en la TDC directa. A la salida del descodificador, la TDC inversa (TDCI)
genera bloques de 8 × 8 muestras para formar la imagen reconstruida.
La TDC está muy relacionada con la transformada discreta de Fourier .
Cada bloque de 8 × 8 se considera como una señal discreta función de dos
coordenadas espaciales, x e y. La salida de la TDC es un vector de 64
componentes que constituye el espectro de frecuencias de la señal de entrada
también de 64 muestras. La ecuación para la TDC 8 × 8 es




7 X
7
X
(2x + 1)uπ
(2y + 1)vπ 
1
f (x, y) cos
cos
F (u, v) = C(u)C(v) 
4
16
16
x=0 y=0

y la correspondiente a la transformada inversa 8 × 8 es
"

7 X
7
1 X
(2y + 1)vπ
(2x + 1)uπ
f (x, y) =
cos
C(u)C(v)F (u, v) cos
4 u=0 v=0
16
16

#

88

CAPÍTULO 5. COMPRESIÓN DE IMÁGENES

donde
1
√
para u, v = 0
2
C(u), C(v) = 1 en otro caso
C(u), C(v) =

El coeficiente con frecuencia cero en ambas dimensiones F (0, 0) se denomina coeficiente de componente continua y los restantes 63 coeficientes son
las componentes alternas de la señal. Como las intensidades de una imagen
varı́an normalmente de forma muy lenta de un punto a otro, el paso de aplicar la TDC proporciona la base para conseguir la compresión de datos al
concentrar la mayor parte de la señal de entrada en las frecuencias bajas.
Para un bloque 8 × 8 de una imagen tı́pica la mayor parte de las frecuencias
tienen amplitud cero o cercana a cero y no necesitan ser codificadas.
En el descodificador la TDCI invierte el paso anterior cogiendo los 64
coeficientes de la TDC (que en ese punto han sido cuantizados, como veremos
luego) y reconstruye una señal de 64 puntos. Si la TDC y la TDCI pudieran
calcularse con precisión absoluta y si los coeficientes no se cuantizaran, como
se explica a continuación, la señal original podrı́a recuperarse exactamente.
En un principio, por tanto, la TDC no introduce pérdidas en la codificación,
simplemente transforma las muestras a un dominio en el que puedan ser
codificadas más eficientemente.
Aunque no entraremos en más detalles, el hecho de que la TDC utilice
funciones trascendentes implica que ninguna instrumentación fı́sica puede
calcularla con precisión absoluta. Esto abre la puerta a numerosos algoritmos
que intentan computar la TDC con la mayor precisión posible.
Cuantización
Cada uno de los 64 coeficientes de la TDC se cuantiza uniformemente
según una tabla de cuantización de 64 elementos que se proporciona como
entrada al codificador. El propósito de la cuantización es conseguir la compresión de los datos al no representar los coeficientes con mayor precisión
de la que es necesario para conseguir la calidad de imagen deseada.
Pese a manejar un vector de 64 coeficientes, la cuantización empleada
por el método JPEG se puede considerar como una cuantización escalar (y
no vectorial) sobre cada componente de forma individual. El cuanto utilizado es mayor para las componentes de frecuencias más altas ya que éstas
juegan un papel mucho menos importante que las frecuencias más bajas. La
cuantización es la principal fuente de pérdidas en los codificadores basados
en la TDC.

5.5. EL ESTÁNDAR JPEG
F(0,0)

89
F(0,7)

F(7,7)

Figura 5.3: La reordenación en zigzag de los coeficientes de la TDC junta los
coeficientes de frecuencias altas y facilita su posterior compresión debido a que su
valor, al menos en imágenes de tono continuo, será mayoritariamente cero.

La cuantización se define como la división de cada coeficiente de la TDC
por su correspondiente cuanto, seguida de un redondeo al entero más cercano:
·
¸
F (u, v)
F Q (u, v) = redondeo
Q(u, v)
donde Q es la matriz de cuantización y Q(u, v) es el cuanto a aplicar al elemento de coordenadas (u, v). Aunque no es obligatorio, Q suele ser simétrica.
Los cuantos serán pequeños en la parte superior izquierda (bajas frecuencias) y grandes en la inferior derecha (altas frecuencias). Con ello muchos
de los coeficientes de frecuencias altas se hacen nulos y son más sencillos de
codificar, a la vez que los de bajas frecuencias permanecen casi inalterados.
Un cuanto con valor 1 proporciona el resultado más preciso; sin embargo,
aún ası́ se producen pérdidas debido a que los coeficientes exactos de la TDC
no suelen ser enteros.
El valor de F Q es normalizado por el tamaño del cuanto. La descuantización es la función inversa
0

F Q (u, v) = F Q (u, v) · Q(u, v)

Ordenación en zigzag
Finalmente, los coeficientes cuantizados se reordenan en la secuencia
en zigzag mostrada en la figura 5.3. Este orden facilita la codificación por
entropı́a del próximo paso al situar los coeficientes de baja frecuencia antes
de los de frecuencias altas (que son probablemente nulos).

90

CAPÍTULO 5. COMPRESIÓN DE IMÁGENES

Esto permite un mecanismo adicional de compresión sin pérdidas, ya que
las largas ristras de ceros pueden reducirse considerablemente si almacenamos únicamente su longitud en lugar de la sucesión completa. Para cada
coeficiente no nulo de la TDC se almacena el número de ceros que lo preceden, el número de bits que se necesitan para representar la amplitud del
número y la amplitud en sı́.
Tras este paso los coeficientes de la TDC cuantizados dejan de verse
como tales y, al colocar correlativos los coeficientes de todos los bloques
de la imagen, obtenemos una secuencia intermedia de sı́mbolos que serán
codificados por el siguiente (y último) paso.

Codificación por entropı́a
Como último paso de la compresión JPEG se obtiene una reducción
adicional del volumen de los datos sin pérdidas al codificar los coeficientes
cuantizados más compactamente según sus propiedades estadı́sticas mediante uno de estos dos métodos de codificación por entropı́a: codificación de
Huffman o codificación aritmética. Estas técnicas de codificación se basan
en utilizar un número pequeño de bits para aquellos sı́mbolos que aparecen con mayor frecuencia (probabilidad) en un canal, asignando códigos con
longitudes mayores que la del sı́mbolo original a los sı́mbolos ocasionales.2

5.6.

Compresión basada en wavelets

La compresión basada en wavelets constituye uno de los campos de investigación en compresión de señales que mayor atención está recibiendo en
los últimos años. Por tratarse de una tecnologı́a nueva con apenas una década de desarrollos es aún demasiado pronto para juzgar su eficiencia, pero los
resultados obtenidos hasta ahora son muy prometedores.
Este tipo de compresión se basa en la teorı́a de los wavelets y más concretamente en la transformada discreta con wavelets. Un conocimiento mı́nimo
de esta teorı́a es necesario para poder profundizar en esta técnica de compresión. Sin embargo, incluso una introducción al tema no puede realizarse
en un par de párrafos por lo que se ha preferido incluirla en un apéndice. El
lector puede ahora abordar el estudio del apéndice B o bien conformarse con
saber que la compresión basada en wavelets utiliza una transformada que, al
igual que la transformada discreta del coseno estudiada en 5.5, proporciona
2
Un estudio profundo de las técnicas de codificación matemática puede encontrarse en
el libro de J. Rifà y Ll. Huguet, Comunicación digital, Editorial Masson, 1991.

5.6. COMPRESIÓN BASADA EN WAVELETS

91

Compresor

Transformada
wavelet

Cuantizador

Transformada
wavelet inversa

Descuantizador

Codificador

Descodificador

Descompresor

Figura 5.4: El esquema general de las técnicas de compresión basadas en wavelets es muy similar al de las basadas en la transformada discreta del coseno. La
transformada con wavelets, sin embargo, parece dotar de mayor potencia a estos
métodos con respecto a los basados en la TDC.

un gran número de coeficientes cercanos a cero de los que puede prescindirse
sin grandes pérdidas en la calidad de la señal reconstruida.
Se han sugerido en los últimos años un gran número de esquemas de
compresión de imágenes basados en wavelets. Todos se ajustan a lo mostrado
en la figura 5.4.
Como puede verse en la figura, las etapas de este esquema de compresión
son muy similares a las mostradas en la figura 5.2 para el estándar JPEG.
La diferencia fundamental está en el tipo de transformada utilizada. En
aquel caso se utilizaba la transformada discreta del coseno y aquı́ la base del
método es la transformada discreta con wavelets (TDW).
La TDW sobre la señal bidimensional que es la imagen se lleva a cabo
mediante una transformada unidimensional, como la discutida en el apéndice B, aplicada sobre la secuencia obtenida al recorrer por filas o columnas la
imagen. Según el tipo de barrido efectuado sobre la imagen distinguiremos
entre TDW horizontal y TDW vertical . Por lo demás, ambas transformaciones son idénticas entre sı́ e iguales a la TDW.
Para poder capturar mejor la evolución de las intensidades de la imagen,
en cada nivel de la codificación subbanda se aplica alternadamente la TDW
horizontal o vertical a la subimagen procedente del nivel anterior. En este
caso, además, no es necesario iterar el algoritmo de la codificación subbanda

92

CAPÍTULO 5. COMPRESIÓN DE IMÁGENES

hasta el final. El número de niveles del algoritmo depende de varios factores
entre los que se encuentran la cantidad de compresión requerida, el tamaño
de la imagen original y la longitud de los filtros utilizados.
Al igual que en la TDC, la codificación elimina (hace cero) aquellos coeficientes con magnitudes pequeñas sin crear una distorsión significativa en
la imagen reconstruida. Una forma de eliminar los coeficientes con valores
pequeños es aplicando una función umbral
(

Tt (x) =

0 si x < t
x en otro caso

a la matriz de coeficientes de cada nivel. La cantidad de compresión obtenida (y la calidad de la imagen) puede controlarse variando el umbral t.
Para obtener mayor compresión pueden cuantizarse los coeficientes no nulos
mediante cuantización escalar como la descrita en 5.4.
Para terminar, a las muestras cuantizadas se les aplica algún esquema de
compresión sin pérdidas por entropı́a como los vistos al final de la sección 5.5.

5.7.

Compresión fractal

La aplicación de la teorı́a de los sistemas de funciones iteradas del capı́tulo 4 a la compresión de imagen se ha convertido en uno de los campos de
estudio más fértiles dentro de la codificación de imágenes. Los capı́tulos
anteriores han sentado las bases para un conocimiento profundo de los fundamentos de la geometrı́a fractal. En este capı́tulo, por otra parte, se han
presentado diferentes esquemas de compresión de imagen que compiten en
rendimiento con la compresión basada en SFI. El lector puede dar ahora el
salto hacia la transformada fractal.

5.8.

Comparación de los esquemas de compresión

Es difı́cil comparar diferentes programas o técnicas de compresión, ya
que aparecen problemas en como mı́nimo dos frentes distintos. En primer
lugar los artı́culos suelen presentar sus resultados sobre imágenes diferentes
(incluso aunque a veces parezcan la misma). En segundo lugar, a la hora de mostrar en papel las imágenes comprimidas muchos mecanismos de
impresión impiden que puedan apreciarse con claridad determinadas caracterı́sticas.

5.8. COMPARACIÓN DE LOS ESQUEMAS DE COMPRESIÓN

93

A todo esto hay que unir los problemas derivados de la falta de una medida del error cometido en la compresión que reproduzca adecuadamente la
percepción visual humana de la calidad de la imagen, como ya se discutió en
el apartado 5.2. Pese a ello, en esta sección estudiaremos, con una pretensión
meramente orientativa, los resultados proporcionados por tres programas
distintos basados en las principales técnicas de compresión de imagen con
pérdidas discutidas en este trabajo: wavelets, JPEG y transformada fractal.
Estos resultados se han tomado de un proyecto de la Universidad de Waterloo3 que pretendı́a servir de referente continuo en cuanto a la evaluación
de los distintos esquemas de compresión de imagen con pérdidas existentes.
Sin embargo, el último estudio que allı́ aparece data de julio de 1995 lo que lo
deja un tanto anticuado. Con todo, los datos de artı́culos recientes muestran
que la situación relativa hoy dı́a entre los distintos métodos es aparentemente similar a la de este estudio, por lo que finalmente lo utilizaremos aquı́ a
falta de uno posterior.
Los programas prototipo considerados para cada técnica son los indicados a continuación:
JPEG Programa Image Incorporated 3.1 de Iterated Systems con codificación de Huffman.
Wavelets Programas Codtree y Decdtree 7.01 de Amir Said y Willian
Pearlman con codificación aritmética.
Fractales Programas Enc y Dec 0.03 de Yuval Fisher [FIS 95] con un árbol
cuadricular de 4 niveles y codificación de Huffman.
Se ha indicado el tipo de codificación por entropı́a utilizado por cada
programa, ya que debe tenerse en cuenta que aquellos programas que utilizan codificación aritmética tienen ventaja con respecto a los que no. La
codificación aritmética, desafortunadamente, es una tecnologı́a patentada.
La codificación de Huffman está ahı́ para todo el que quiera cogerla.
Antes de mostrar los resultados de la comparativa, debe tenerse muy
claro que aquı́ no se trata de encontrar el mejor programa de compresión
de imágenes con pérdidas. De momento no existe ningún candidato que
destaque lo suficiente sobre el resto y posiblemente nunca existirá, ya que son
muchos los factores que entran en juego a la hora de evaluar un programa de
compresión y algunos de ellos no tienen un equivalente en otros programas.
Por citar algunos:
3
Las muestras y datos utilizados en esta sección pueden obtenerse directamente a través
de la dirección http://links.uwaterloo.ca/bragzone.base.html.

94

CAPÍTULO 5. COMPRESIÓN DE IMÁGENES

(a) JPEG

(b) wavelet

(c) fractal

Figura 5.5: Una versión generalizada del pasatiempo de descubrir las diferencias.
La imagen de la colina con un ratio de compresión de 10 bajo cada una de las tres
técnicas consideradas en esta sección.

ratio de compresión
calidad de la imagen comprimida
velocidad de compresión
velocidad de descompresión
tipo de imágenes que puede tratar
patentes y royalties
portabilidad
facilidad de uso e instrumentación
Por otra parte, debe evitarse extrapolar hacia el futuro, pues que un
determinado codificador sea hoy mejor que otro no quiere decir que lo vaya
a seguir siendo mañana. Cada tecnologı́a se encuentra en un estadio de
desarrollo diferente: algunas son muy nuevas y aún tienen que ser explotadas;
otras son tan maduras que se han desarrollado multitud de mecanismos
de optimización para ellas. Lo que sı́ es cierto es que, por ser JPEG un
estándar internacional, muy buena tendrá que ser una técnica alternativa
de compresión para lograr desbancarlo.

Ejemplos y resultados
La figura 5.5 muestra la imagen de la colina comprimida con un ratio de
compresión de 10 con las tres técnicas a estudio. Puede observarse cómo la

5.8. COMPARACIÓN DE LOS ESQUEMAS DE COMPRESIÓN
50 3

3
+
2

wavelet
fractal
JPEG

45
40

95

3
RSRM 35
2
+3
(dB)
30
2 33
+

3333
2
+
+2
2
++
3333 33333
22
+22+2+3+
33
2 + + +
2
2

25
20
15

0

20

40

60
80
100
Ratio de compresión

33

120

3
+
140

160

Figura 5.6: Compresión y RSRM obtenidas con cada uno de los tres programas a
estudio sobre la imagen de la colina.

35

2

30
25
20
RECM

15
10
5
0

wavelet 3
fractal +
JPEG 2

2
2
+
2+ + + +
+
2
+
33333
2
++
22 33333333
+
2
+
22333
+
233
+
23
3
3
0

20

40

+
3

33

60
80
100 120
Ratio de compresión

140

160

Figura 5.7: Compresión contra RECM en la imagen de la colina. Esta representación es una alternativa a la de la figura 5.6 en la que se utilizaba la RSRM.

calidad visual de las imágenes es totalmente aceptable siendo difı́cil distinguir una de otra. Esta observación se corrobora en las curvas ratio-distorsión
de la gráfica 5.6 donde se aprecia cómo para ratios de compresión bajos (entre 0 y 10) las RSRM proporcionadas por cada técnica son casi iguales. Esta
circunstancia aparece también en el resto de curvas ratio-distorsión de esta
sección.

96

CAPÍTULO 5. COMPRESIÓN DE IMÁGENES

(a) JPEG

(b) wavelet

(c) fractal

Figura 5.8: La imagen del pájaro bajo un ratio de compresión de 40 con cada
método.

50

wavelet 3
3
fractal +
233
JPEG 2
++
2 33
3
2
+++ 333
2+
3
2++ 33333 3
33333
22+ + +
3
++
22
3
3
+
+ + ++ ++ 3
2
+ ++
2
2

45
40
RSRM 35
(dB)
30
25
20
15

2
0

20

40

60
80
100
Ratio de compresión

120

140

160

Figura 5.9: Compresión y RSRM obtenidas sobre la imagen del pájaro con cada
uno de los tres esquemas de compresión considerados.

La figura 5.6 muestra, además, cómo el estándar JPEG disminuye rápidamente su calidad de compresión al aumentar el ratio de compresión. Esta
pérdida de calidad es menos visible en el caso de las compresiones mediante
wavelets y fractales,4 aspecto éste reflejado también en las siguientes curvas
ratio-distorsión.
Para que el lector pueda comparar la forma de presentar la información
de una curva ratio-distorsión en la que se muestre la RECM con una en la
que se considere la RSRM, la figura 5.7 muestra la misma información que
4
En cualquier caso debe tenerse en cuenta de nuevo que al movernos por valores más
bajos de la RSRM, pequeñas diferencias en ésta suponen amplias diferencias de calidad.

5.8. COMPARACIÓN DE LOS ESQUEMAS DE COMPRESIÓN

(a) JPEG

(b) wavelet

97

(c) fractal

Figura 5.10: El puente comprimido con los tres programas a estudio con un ratio
de compresión de 32.

34
32 3
30 +
2
3
28
23
+
RSRM 26
223333
+
(dB) 24
3
++
2+
2+ 333333
333333
++
2
22
3
2
+
20
2
18
16

0

20

40

2

60
80
100
Ratio de compresión

wavelet
fractal
JPEG

33

120

3
+
2

+

140

3

160

Figura 5.11: Compresión y RSRM de la imagen del puente comprimida según las
tres técnicas de esta sección.

la gráfica 5.6 pero considerando la RECM.
La figura 5.8 muestra el resultado de comprimir la imagen del pájaro con
un ratio de compresión de 40. En estos niveles la técnica JPEG distorsiona
ya claramente la imagen. Los resultados de la compresión fractal son ciertamente aceptables, pese a que se pierde algún detalle en las garras y las
plumas de la cabeza. La compresión con wavelets es ligeramente superior y
muestra una calidad sorprendente para el elevado nivel de compresión utilizado. Las curvas ratio-distorsión de la figura 5.9 aportan más información
acerca de la codificación de la imagen del pájaro. La RSRM se mueve en
valores superiores a los mostrados en el caso de la colina debido a que la
imagen del pájaro tiene, en general, sus rasgos principales a escalas mayores.

98

CAPÍTULO 5. COMPRESIÓN DE IMÁGENES

Pese a todas las advertencias realizadas anteriormente, es importante
resaltar una vez más que debe evitarse una generalización prematura del
comportamiento de los métodos. La figura 5.10 es prueba de ello. En ella se
muestran las codificaciones de la imagen del puente con un ratio de compresión de 32. A pesar de ser este ratio inferior al utilizado con el pájaro,
el mayor nivel de detalle de la imagen hace que la calidad empeore ostensiblemente. Las curvas ratio-distorsión de esta imagen se muestran en la
figura 5.11.
Se ha pretendido mostrar aquı́ una breve visión del comportamiento de
los esquemas de compresión basados en la TDC, en wavelets y en fractales.
En los siguientes capı́tulos se realizará un análisis más detallado centrado en
la transformada fractal. Debido a los pobres resultados que suelen obtenerse
con cualquiera de estos métodos con imágenes artificiales, todas las muestras
de este trabajo serán imágenes naturales.

Capı́tulo 6

La transformada fractal
Una de las aplicaciones más innovadoras de la geometrı́a fractal y en particular de los sistemas de funciones iteradas es la compresión de imagen. Para
proporcionar una visión lo más amplia posible, el tema anterior abordó enfoques alternativos para esta compresión. Aquı́, siguiendo particularmente
a [FIS 92], [FIS 95], [SAU 96a] y [LU 97], veremos cómo compactar fractalmente una imagen en escala de grises; en 5.3 se explicó cómo generalizar un
sistema de este tipo a imágenes en color.

6.1.

Historia y fundamentos

En el capı́tulo 4 se estudió la teorı́a de los sistemas de funciones iteradas
(SFI) desarrollada por Hutchinson y extendida por Barnsley y se sugirió su
uso para la compresión de imágenes. La codificación de la hoja de helecho
en unos cuantos coeficientes lleva a preguntarse si no será posible obtener
codificaciones reducidas similares para cualquier imagen. De hecho, en 1987
Barnsley y sus colegas especularon con ratios de compresión de 10000:1 y
con la posibilidad de transmitir vı́deo en tiempo real a través de la lı́nea
telefónica convencional.
Sin embargo, este enfoque basado en SFI presenta un problema obvio:
los fractales que genera un SFI poseen la propiedad de la autosemejanza, es
decir, están formados por copias convenientemente transformadas de sı́ mismos. En el caso de una imagen de un pato, por ejemplo, uno deberı́a poder
observar patitos distorsionados por todo su plumaje, lo cual no es evidentemente una suposición muy natural. Los primeros intentos para adaptarse
a esta caracterı́stica de los SFI no produjeron resultados muy alentadores.
Ası́ estaban las cosas cuando en 1989 un estudiante de Barnsley, Arnaud Jac99

100

CAPÍTULO 6. LA TRANSFORMADA FRACTAL

Figura 6.1: La idea clave de Jacquin fue considerar una imagen como formada por
copias de partes de sı́ misma, abandonando el enfoque global de anteriores intentos.

quin, diseñó el primer sistema automático de codificación fractal y dejó a
un lado el rı́gido enfoque basado en SFI globales.
La idea de Jacquin es a primera vista muy simple. En lugar de considerar
una imagen como formada por copias de sı́ misma (bajo las transformaciones apropiadas), ahora una imagen estará formada por copias de partes de
sı́ misma. En una postal veraniega es difı́cil que un trozo de una nube se parezca a la postal completa, pero sı́ que parece posible encontrar otra sección
de alguna nube o de otro elemento de la imagen que se parezca al trozo de
nube. Las figuras 6.1 y 6.2 muestran regiones de una imagen que son similares a diferentes escalas y bajo una transformación apropiada. El enfoque
general consiste en subdividir la imagen mediante una partición (en el caso
más sencillo en regiones cuadradas de tamaño fijo) y encontrar para cada
región resultante otra parecida en la imagen. Este esquema se conoce como
sistemas de funciones iteradas particionadas (SFIP) o locales (SFIL). Al
proceso de obtener el SFIP asociado a una imagen le llamaremos transformada fractal.
Los resultados anteriores significaron el inicio de una prolı́fica sucesión
de investigaciones, que llega hasta nuestros dı́as, para determinar numerosos
aspectos todavı́a abiertos de la transformada fractal entre los que pueden
considerarse la forma de particionar la imagen (segmentación), el mecanismo
para determinar la correspondencia entre regiones, la extensión a codificación de vı́deo o la reducción de la complejidad computacional, principalmente la temporal. Algunas posibilidades para los elementos anteriores se
discutirán más adelante.

6.2. MODELO DE IMAGEN

101

Figura 6.2: Otra muestra de regiones similares en una imagen bajo una transformación apropiada.

6.2.

Modelo de imagen

Para poder formalizar los desarrolllos teóricos de este capı́tulo necesitamos un modelo matemático de imagen en escala de grises. Aunque existen
modelos más complejos, para nuestros propósitos trabajaremos con el espacio ∆ = {τ : I 2 → I} de imágenes cuadradas de lado 1, donde I 2 es el
cuadrado unidad con valores en el rango I = [0, 1]. La función τ da el nivel
de gris de cada punto de la imagen.1 La generalización a imágenes de otros
tamaños es directa.
También necesitaremos en la siguiente sección una métrica que nos proporcione la distancia entre dos imágenes. Aunque en la práctica se consideran
métricas como la raı́z del error cuadrático medio (discutida en el página 81),
es más fácil demostrar las propiedades de contractividad y convergencia con
métricas menos elaboradas como la del supremo,
dsup (τ1 , τ2 ) =

sup |τ1 (x, y) − τ2 (x, y)|

(x,y)∈I 2

donde la resta se entiende como resta acotada. En cualquier caso, y aunque
no lo demostremos, los resultados posteriores se cumplen con muchas otras
métricas, en concreto con el error cuadrático medio.
Un resultado de gran importancia para los desarrollos presentados a continuación, y que aquı́ no probaremos, es que el espacio (∆, dsup ) es completo.
1
Serı́a más apropiado I 2 → R para que queden bien definidas las sumas y restas de
imágenes, pero mantendremos esta notación.

102

6.3.

CAPÍTULO 6. LA TRANSFORMADA FRACTAL

Sistemas de funciones iteradas particionadas

Nuestro objetivo es repetir la técnica de generación de fractales mediante SFI, pero desde una perspectiva más amplia. Buscaremos un conjunto
de transformaciones f1 , . . . , fn , las agruparemos bajo una transformación
F = ∪fi , mostraremos que bajo ciertas condiciones F es contractiva, deduciremos que tiene un atractor asociado e intentaremos escoger F tal que éste
aproxime suficientemente una imagen dada.
Definición 6.1 Sea X un espacio métrico completo y sea Di ⊂ X con
i = 1, . . . , n. Un sistema de funciones iteradas particionadas (SFIP)2 es
una colección de aplicaciones contractivas fi : Di → X con i = 1, . . . , n.
Sean D1 , . . . , Dn y R1 , . . . , Rn subconjuntos de I 2 que denominaremos
dominios y rangos, respectivamente. Es habitual confundir un subconjunto
de I 2 con la función definida sobre dicho subconjunto. Por lo tanto, aunque
llamemos dominio a Di y rango a Ri , el dominio y el rango real son los
productos cartesianos Di × I y Ri × I.
Sea v1 , . . . , vn : I 3 → I 3 un conjunto de aplicaciones. Definamos fi como
la restricción
fi = vi |Di ×I
Las aplicaciones f1 , . . . , fn conforman el SFIP. La idea es que el dominio
de fi esté restringido, aunque, aun ası́, su forma puede ser muy general. La
transformación fi opera sobre τ haciendo fi (τ ) = fi (x, y, τ (x, y)) e interpretamos el resultado como una imagen sobre I 2 . Para que todo vaya bien
debemos imponer la siguiente restricción adicional sobre los fi :
Definición 6.2 Se dice que las transformaciones f1 , . . . , fn enlosan I 2 si
para toda τ ∈ ∆,
n
[

fi (τ ) ∈ ∆

i=1

Cuando aplicamos fi a la parte de la imagen τ ∩ (Di × I) sobre Di el
resultado es una subimagen sobre un rango que denominaremos Ri .3 El en2
Lo bueno serı́a poder utilizar el teorema del punto fijo (como hicimos con los SFI)
para definir un único punto fijo del SFIP. Pero de forma general esto no es posible, ya que
al estar restringidos los dominios, el punto inicial es importante: de no tener cuidado nos
podemos quedar tras una iteración con un conjunto vacı́o. No entraremos en más detalles,
ya que en el caso que nos ocupa (la codificación de imágenes) no tendremos este problema.
3
En este caso diremos que Di cubre Ri .

6.3. SISTEMAS DE FUNCIONES ITERADAS PARTICIONADAS

103

losado implica que I 2 = ∪ni=1 Ri . Además, para simplificar nuestra discusión
asumiremos que los Ri son disjuntos.
Al imponer la condición de enlosado sobre las transformaciones de un
SFIP, estamos cerca de poder codificar una imagen, pero antes debemos
determinar la forma de que F = ∪ni=1 fi sea contractiva.
Definición 6.3 Si f : R3 → R3 es una aplicación con f (x, y, z1 ) =
(x0 , y 0 , z10 ) y f (x, y, z2 ) = (x0 , y 0 , z20 ), entonces diremos que f es contractiva z si existe un número real positivo s < 1 tal que
d(z10 , z20 ) ≤ s · d(z1 , z2 )
donde d es la distancia euclı́dea habitual, y si, además, x0 e y 0 son independientes de z1 o de z2 , para todo x, y, z1 , z2 .
Si consideramos transformaciones que presenten contractividad z, es sencillo demostrar la contractividad de un SFIP.
Proposición 6.1 Si f1 , . . . , fn son contractivas z, entonces
F =

n
[

fi

i=1

es contractiva en ∆ con la métrica del supremo.
Demostración
de fi ; entonces

Sea s = máx si , donde si es la razón de contractividad z

dsup (F (τ1 ), F (τ2 )) = sup{ |F (τ1 )(x, y) − F (τ2 )(x, y)| : (x, y) ∈ I 2 }
= sup{componente z de |fi (x, y, τ1 (x, y))
− fi (x, y, τ2 (x, y))| : (x, y) ∈ Di , i = 1, . . . , n}
≤ sup{si |τ1 (x, y) − τ2 (x, y)| : i = 1, . . . , n}
≤ sup{s |τ1 (x, y) − τ2 (x, y)|}
≤ s · sup{|τ1 (x, y) − τ2 (x, y)|}
≤ s · dsup (τ1 , τ2 )
2
Nótese cómo la independencia respecto a z de las coordenadas x e y de
fi se ha utilizado para pasar de la igualdad a la primera desigualdad. Hemos

104

CAPÍTULO 6. LA TRANSFORMADA FRACTAL

demostrado que si escogemos fi de forma que sea contractiva en el eje z, por
ejemplo si fi es de la forma














x
ui
x
ai bi 0

 

 

fi  y  =  ci di 0   y  +  vi 
oi
z
0 0 si
z

(6.1)

con si < 1, entonces F = ∪fi es contractiva bajo dsup . La condición de
enlosado nos asegura que al evaluar F = ∪fi obtendremos una imagen (función) otra vez. Esto es necesario para poder iterar F . La contractividad de
F en (∆,dsup ) determina un único punto fijo en ∆ al ser (∆,dsup ) un espacio
métrico completo (teorema del punto fijo).
La transformación 6.1 puede descomponerse en dos partes diferentes
según su forma de actuar sobre la imagen:
una transformación geométrica
Ã

gi

x
y

!

Ã

=

ai bi
ci di

!Ã

x
y

!

Ã

+

ui
vi

!

donde ai , bi , ci , di , ui y vi representan traslaciones, giros, reflejos y homotecias como los mostrados en 4.2.
una transformación que controla la escala de grises
mi (z) = si · z + oi
donde s es el escalado del contraste y o el ajuste de brillo.
La transformación geométrica transforma el dominio Di al tamaño y
posición exactos de Ri por lo que la distancia entre los dos bloques en el
sentido geométrico es cero.4 Es la distancia entre los valores de gris la que
hay que minimizar.
Ahora ya tenemos las claves para codificar una imagen. Dada una colección de aplicaciones contractivas z formada por f1 , . . . , fn que enlosan I 2 ,
sabemos que F = ∪fi define un atractor τA en el espacio de imágenes ∆. A
partir de F podemos obtener la imagen asociada de manera similar a como
procedimos en el caso de los SFI, esto es, tomando una imagen cualquiera
τ0 ∈ ∆ e iterando F (τ0 ), F (F (τ0 )) = F 2 (τ0 ), . . . La cuestión inversa, esto es, encontrar un sistema F tal que su atractor asociado aproxime una
4

En el caso más sencillo ai = di = 1/2 y bi = ci = 0 con lo que los dominios han de
tener un tamaño doble al de los rangos y los valores de vi y ui simplemente trasladan Di
hasta el bloque ocupado por Ri .

6.4. CUANTIZACIÓN VECTORIAL Y CODIFICACIÓN FRACTAL 105
imagen dada τ (la obtención exacta de τ no será en general posible) se
puede afrontar considerando el teorema del collage discutido en la página
71, y encontrando, por tanto, dominios D1 , . . . , Dn y sus correspondientes
transformaciones f1 , . . . , fn tales que
τ ≈ F (τ ) =

n
[

fi (τ )

i=1

En resumen el proceso de codificación consiste en particionar I 2 en un
conjunto de rangos5 Ri . Para cada Ri se busca un dominio Di ⊂ I 2 y una
transformación fi : Di × I → I 3 tal que fi (τ ) esté tan cerca de τ ∩ (Ri × I)
como sea posible, esto es, tal que la distancia
d(τ ∩ (Ri × I), fi (τ ))
se minimice.6 Por supuesto, las fi que definen F deben elegirse de forma que
F sea contractiva.7
Ahora podrı́amos explicar cómo segmentar la imagen y cómo determinar
los coeficientes señalados anteriormente para cada fi . Sin embargo, lo dejaremos para una sección posterior. En la siguiente ofreceremos básicamente
gran parte de la información presentada aquı́ pero desde otro punto de vista.
Abandonamos los SFI particionadas para considerar la compresión fractal
como una variante de la cuantización vectorial discutida en 5.4.

6.4.

Cuantización vectorial y codificación fractal

El esquema básico de la compresión fractal de imágenes es muy similar
a la cuantización vectorial con eliminación de media y ganancia de forma
presentada en la página 84. La principal diferencia entre ambos es que en la
CV se utiliza un libro de códigos fijo que debe estar disponible para el descodificador, mientras que en la codificación fractal el libro de códigos es virtual
(no se almacena explı́citamente) y está formado por regiones de la imagen
original. Lo anterior parece una contradicción, ya que es precisamente labor
5
Aunque los términos rango y dominio están ampliamente extendidos, otros autores
(veáse [LU 97], por ejemplo) prefieren utilizar región destino y región de referencia.
6
Ya se ha dicho que son posibles muchas otras métricas distintas a la del supremo,
que por otra parte no es lo suficientemente precisa al calcular la distancia en base a un
único punto. En concreto, aquı́ podrı́a utilizarse para d el error cuadrático medio como se
considerará más adelante.
7
No entraremos en detalles, pero la condición de que F sea contractiva puede relajarse
por la de que sea eventualmente contractiva . Una aplicación f es eventualmente contractiva si existe un valor n tal que f n es contractiva. Existen versiones del teorema del punto
fijo y del teorema del collage para este tipo de aplicaciones [FIS 95, pp. 36 y 52].

106

CAPÍTULO 6. LA TRANSFORMADA FRACTAL

del descodificador restituir la imagen original con lo que no tendrá acceso al
libro de códigos. Cabe preguntarse, por tanto, cómo puede el descodificador
reconstruir la imagen original si ésta se codifica por bloques considerados
como copias escaladas de otros bloques de la imagen más bloques con un
valor de gris constante.8
La respuesta al interrogante anterior debe estar clara para el lector familiarizado con el teorema del punto fijo. Aun ası́ presentamos a continuación
la brillante discusión disponible en [SAU 96a].
Ejemplo Vamos a codificar, por simplificar un poco las cosas, un
número real en lugar de una región de una imagen, por ejemplo π =
3,14159 . . . Supongamos que los libros de códigos para la escala y el desplazamiento son s ∈ {0, 0,25, 0,5, 0,75} y o ∈ {0,0, 0,4, 0,8, 1,2, 1,6, 2,0}.
El libro de códigos de formas tiene un único número: el propio número
π. Si obtenemos todos los posibles valores de sπ + o con s y o obtenidos
de los libros de códigos anteriores, veremos que s = 0,75 y o = 0,8 dan
la mejor aproximación a π:
s · π + o = 0,75 · π + 0,8 = 3,1561 . . .
Con esto, el codificador puede pasar la siguiente información al descodificador: el número original es aproximadamente 0.75 veces él mismo
más 0.8. Hay muchos números que satisfacen esta descripción. Sin más
información el descodificador podrı́a determinar cualquiera de ellos. Sin
embargo, uno de ellos es un número x especial, aquél que es exactamente 0.75 veces él mismo más 0.8, esto es,
x = 0,75x + 0,8
Al resolver esta ecuación obtenemos x = 3,2, que se interpretarı́a como
el valor descodificado. La resolución de x = 0,75x+0,8 es sencilla, pero
al tratar con imágenes con miles de pixels, el correspondiente sistema
de ecuaciones es tan grande que no puede resolverse directamente, sino
sólo por iteración. Esto puede comprobarse con nuestro simple ejemplo.
Si definimos un operador T : R → R como T x = 0,75x + 0,8, entonces
la información que el codificador pasa al descodificador puede verse
como π ≈ T π y debe obtenerse el punto fijo de la ecuación x = T x.
Dado un valor inicial arbitrario x0 , aplicamos iterativamente T para
obtener
x1 = T x0 , x2 = T x1 , x3 = T x2 , . . .
Por ejemplo, con x0 = 0 obtenemos x1 = 0,8, x2 = 1,4, x3 =
1,85, . . . , x20 = 3,192 . . . y x30 = 3,1995 . . . Esta secuencia converge
8

El lector que no lo haya hecho ya, debe abordar el estudio de la sección 5.4, especialmente de la parte dedicada a la cuantización vectorial con eliminación de media y ganancia
de forma.

6.4. CUANTIZACIÓN VECTORIAL Y CODIFICACIÓN FRACTAL 107
al punto fijo 3,2 conocido como atractor del operador T . Este resultado no es coincidencia. Como vimos, si el factor de escalado en valor
absoluto es menor que la unidad, esto es, |s| < 1, el teorema del punto
fijo nos asegura la convergencia al único punto tal que x = T x.

Las propiedades mostradas en el ejemplo anterior también se cumplen
en el caso de imágenes. El codificador trabaja de una forma similar a la CVEMGF. Aquı́, sin embargo, el libro de códigos de formas no se proporciona
a priori como resultado de algún algoritmo de diseño. En su lugar, el libro
de códigos de formas está constituido por bloques extraı́dos de la misma
imagen a codificar. Por lo tanto, estos bloques no se normalizan para que
tengan media cero y varianza uno como se hacı́a en el caso de la CV-EMGF.
Cada imagen, en definitiva, posee su propio libro de códigos.
Ejemplo Supongamos que la imagen se segmenta en bloques de 4 × 4
pixels, lo que hemos denominado antes rangos. Cada rango R debe
aproximarse como R ≈ sD + o1 donde D es un bloque 4 × 4 del
libro de códigos de formas. Cualquier dominio de tamaño 8 × 8 de
la imagen se reduce mediante submuestreo de sus pixels al tamaño
deseado de 4 × 4 pixels y se añade al libro de códigos. Para una imagen
de 512×512 este proceso genera un libro de códigos bastante grande con
(512 − 8 + 1)2 = 255025 bloques. Para reducir el número de bloques
a una cantidad más manejable pueden considerarse separaciones de
más de un pixel entre los dominios como se discutirá en el siguiente
capı́tulo.

El codificador debe resolver por tanto el siguiente problema: para cada
rango encontrar la mejor aproximación R ≈ sD + o1. A los coeficientes s y o
se les denomina escalado y desplazamiento. Para obtener los valores óptimos
para s, o y D deben evaluarse en principio todos los bloques D posibles y
determinar para cada uno de ellos los mejores coeficientes s y o. En el ejemplo
unidimensional anterior con π obtuvimos todas las combinaciones posibles
sobre los libros de códigos de s y o y escogimos la mejor. Aunque este
enfoque podrı́a aplicarse igualmente con bloques de imágenes, su elevado
coste computacional temporal lo hace inviable. Afortunadamente, existen
formas de evitar estas evaluaciones y se discutirán en el siguiente apartado.
Los valores obtenidos para s y o se cuantizan, normalmente mediante
cuantización escalar uniforme, obteniendo unos valores redondeados s̄ y ō.
Sea E(R, D) la función que devuelve la diferencia entre dos regiones del
mismo tamaño de una imagen. Un codificador fractal básico con bloques de
dimensión fija tendrı́a el siguiente esquema genérico:

108

CAPÍTULO 6. LA TRANSFORMADA FRACTAL

1. Segmentación de la imagen. Dividir la imagen a codificar en bloques de
tamaño fijo, por ejemplo, 4 × 4. Los bloques resultantes se denominan
rangos Ri .
2. Libro de códigos de dominios. Recorrer la imagen para crear una lista
de dominios cuyo tamaño es el doble del de los rangos. Promediando
grupos de cuatro pixels, reducir el tamaño de los dominios para que
concuerde con el de los rangos.
3. Búsqueda. Para cada rango R obtener una aproximación lo más buena
posible R ≈ sD + o1 siguiendo estos pasos:
a) Para cada dominio Di calcular los coeficientes s y o que mejor
aproximan a R, cuantizarlos si procede y utilizando los coeficientes cuantizados s̄ y ō calcular el error E(R, Di ).
b) Entre todos los bloques Di encontrar aquel Dk con menor error
E(R, Dk ) = mı́ni E(R, Di ).
c) Mostrar el código fractal del bloque R formado por s̄, ō y el ı́ndice
k que identifica al bloque óptimo Dk .
Como ya hemos visto el conjunto de códigos fractales resultante del algoritmo anterior, denominado modelo fractal, no permite al descodificador
la obtención inmediata de una aproximación de la imagen original, ya que se
trata, en realidad, de la descripción de un operador. Ası́, como vimos en el
ejemplo con π, podemos desarrollar las operaciones indicadas por el modelo
fractal sobre cualquier imagen inicial τ0 para obtener una nueva imagen T τ0 .
Si repetimos el proceso sobre la nueva imagen de forma iterativa la secuencia
resultante
τ1 = T τ0 , τ2 = T τ1 , τ3 = T τ2 , . . .
converge a un atractor que aproximará la imagen original siempre que T sea
contractiva, esto es, cuando |s| < 1. El teorema del collage motiva la minimización del error E(R, Dk ) en el codificador para que la imagen obtenida
tras un número suficiente de iteraciones en el descodificador aproxime en la
mayor medida posible a la original.

6.5.

Obtención de los coeficientes de los códigos
fractales

La determinación de unos valores adecuados para el escalado y el desplazamiento es un aspecto crucial para reducir la distorsión de la imagen
descodificada. Aunque existen otras aproximaciones, aquı́ consideraremos

6.5. OBTENCIÓN DE LOS COEFICIENTES DE LOS CÓDIGOS FRACTALES109
únicamente dos formas de obtener estos coeficientes. La primera [SAU 96a]
se basa en el método de los mı́nimos cuadrados, mientras que la segunda
[LU 97] reduce el número de calculos necesarios sacrificando algo de calidad
en los valores obtenidos.

Método de los mı́nimos cuadrados
Consideremos dos bloques R y D con n pixels de intensidades
r1 , r2 , . . . , rn y d1 , d2 , . . . , dn . Si trabajamos con el error cuadrático al realizar
la selección de los mejores coeficientes, esto es,
E(R, D) =

n
X

(s · di + o − ri )2

i=1

podemos obtener los valores de s y o que minimizan R igualando a cero las
derivadas parciales respecto a ambos para obtener:
Pn

s=

Pn
Pn
i=1 di ri ) − ( i=1 di )( i=1 ri )
P
P
n ni=1 d2i − ( ni=1 di )2

n(

y

Ã

n
n
X
1 X
o=
ri − s
di
n i=1
i=1

(6.2)

!

En ese caso el error cuadrático es
"

Ã

!

Ã

n
n
n
n
n
X
X
X
X
1 X
E(R, D) =
ri2 + s s
d2i − 2
di ri + 2o
di + o on − 2
ri
n i=1
i=1
i=1
i=1
i=1

!#

P

Si el denominador de la ecuación 6.2 es cero, entonces s = 0 y o = ni=1 ri /n.
En este caso no es necesario almacenar la información sobre el dominio pues
éste es indiferente.

Método del escalado constante
Aunque muchas de las subexpresiones de las ecuaciones anteriores pueden evaluarse sólo una vez al comienzo del codificador y almacenarse convenientemente en tablas, la obtención de los valores óptimos de s y o no
deja de precisar una gran cantidad de operaciones. Una solución subóptima
consiste en considerar que el escalado es un valor constante (por ejemplo,
s = 3/4) y calcular el desplazamiento o que minimice E(R, D) a partir de
esta premisa.

110

CAPÍTULO 6. LA TRANSFORMADA FRACTAL

En este caso, si R̄ y D̄ representan las intensidades medias de los bloques
R y D, es decir,
Pn
ri
R̄ = i=1
n
y
Pn
di
D̄ = i=1
n
entonces se toma o = R̄ − D̄. Aunque de esta forma nos ahorramos tener
que almacenar s para cada rango porque su valor es constante, la calidad
de la imagen descodificada se resiente, especialmente para ratios altos de
compresión.

6.6.

Compactación de los códigos fractales

Determinar el cuanto a utilizar para los valores del desplazamiento o y el
escalado s de cada código fractal es equivalente a establecer el número de bits
con los que se almacenará cada uno de ellos. Los resultados óptimos [FIS 95,
p. 63] suelen obtenerse con 5 bits para s y 7 para o, aunque valores de 4
y 6, respectivamente, también proporcionan resultados aceptables. También
podrı́a ser factible considerar la codificación adaptativa de estos coeficientes,
ya que sus distribuciones presentan una estructura bastante regular [FIS 95,
p. 63].
Por otra parte, no es necesario guardar la posición de cada rango Ri , pues
éstos pueden ser conocidos por el descodificador si el codificador utiliza algún
orden determinado en el almacenamiento de sus códigos fractales asociados
(por ejemplo, mediante un barrido por filas). La información sobre el dominio
Di , sin embargo, no es redundante y para referenciarlo se usarán dlog2 N e
bits, donde N es el tamaño de la lista de dominios considerada al codificar.
El codificador necesitará, por tanto, de una rutina que empaquete adecuadamente cada coeficiente según el número de bits requerido. En el otro
lado, evidentemente, el descodificador deberá desempaquetar correctamente
los datos anteriores.

6.7.

Ejemplos

En esta sección se presentan los resultados obtenidos sobre la imagen
del pájaro con un compresor desarrollado por el autor mezclando el código
sugerido por [LU 97] y [FIS 95] con aportaciones propias. El codificador
se basa en el método del escalado constante discutido antes, considerando

6.7. EJEMPLOS

111

Figura 6.3: El pájaro comprimido con rangos de 4 × 4 con un ratio de compresión
de 5.11.

Figura 6.4: El pájaro comprimido con rangos de 8 × 8 con un ratio de compresión
de 19.6.

s = 3/4. Además el programa obtiene valores para o en el rango [−128, 127]
y los almacena con 8 bits por lo que no hace uso de la cuantización escalar.
La imagen del pájaro tiene dimensiones 256 × 256 con lo que el número
de dominios está limitado entre 0 y (2562 − 1) = 65535 y son necesarios 16
bits para representarlos.9 En esta imagen, por tanto, son necesarios 16 +
8 = 24 bits por código fractal. Si consideramos una partición en bloques
cuadrados de n × n, con n = 3, 4, 5, 6, 7, 8, . . ., los ratios de compresión serán
9

En realidad el número de dominios es ligeramente menor y depende del tamaño de los
rangos, pero seguirán siendo necesarios 16 bits al menos que los rangos sean mayores de
76 × 76 pixels.

112

CAPÍTULO 6. LA TRANSFORMADA FRACTAL

Figura 6.5: El pájaro comprimido con rangos de 16×16 con un ratio de compresión
de 77.37.

Figura 6.6: El pájaro comprimido con rangos de 32×32 con un ratio de compresión
de 293.9.

de 3,0, 5,33, 8,33, 12,0, 16,33, 21,33, . . ., respectivamente.
Las figuras de la 6.3 a la 6.6 muestran el resultado de comprimir la
imagen del pájaro con distintos tamaños de rangos. La tabla 6.1 presenta
los ratios de compresión de cada una de estas figuras junto a las medidas de
su distorsión y el tiempo empleado para su codificación.
En la figura 6.7 se muestra la imagen inicial y la obtenida por el descodificador tras distintas iteraciones sobre el modelo fractal. Sin ningún tratamiento adicional al presentado en este capı́tulo, la convergencia se produce
habitualmente en algún lugar entre las 10 y las 20 iteraciones, aunque este

6.7. EJEMPLOS
Rangos
4×4
8×8
16 × 16
32 × 32

113

Figura
6.3
6.4
6.5
6.6

Tiempo (seg)
241.5
159.4
126.3
100.0

Compresión
5.11
19.6
77.37
293.9

RECM
3.18
6.75
12.19
20.37

RSRM
38.08
31.54
26.41
21.95

Cuadro 6.1: El efecto del tamaño de bloque de los rangos en el tiempo de compresión, el ratio de compresión y las medidas de distorsión de la imagen. Los tiempos se
midieron en una máquina con procesador Pentium II a 233 MHz bajo Linux. Los ratios de compresión no corresponden exactamente a los razonados en el texto debido
a la presencia de una cabecera en el fichero comprimido y a algunos bits adicionales
incluidos por el codificador al estar diseñado para un esquema más complejo que el
comentado en este capı́tulo.

(a) i = 0

(b) i = 1

(c) i = 2

(d) i = 3

Figura 6.7: Resultado de la descodificación de la imagen del pájaro con rangos de
4 × 4 tras i iteraciones. Como imagen inicial se utiliza una imagen lisa con todos
sus pixels con valor de gris constante 128.

valor depende de la imagen y de la complejidad (en el sentido de interdependencias) del modelo fractal generado.

Capı́tulo 7

Mejoras en la codificación
fractal
En el capı́tulo anterior se describió la transformada fractal en su versión
más sencilla y primitiva. Si todo lo conocido sobre la compresión fractal
de imágenes se redujera a este esquema básico, no estarı́amos hablando de
una tecnologı́a prometedora y probablemente estos capı́tulos no tendrı́an
sentido. Pero el estado actual de la codificación fractal llega mucho más
lejos. Durante los últimos años se han publicado numerosos trabajos que
superan, en cierta forma, algunos de los múltiples problemas que plantea el
enfoque básico de la transformada fractal.
Aquı́ sólo consideraremos algunas ideas ampliamente difundidas para
optimizar en diversos sentidos tanto la compresión como la descompresión
fractal. Ampliaciones adicionales pueden encontrarse en la bibliografı́a, en
concreto [SAU 96a] presenta un repaso detallado a muchas de estas innovaciones. En la red [FRE 97] recopila una gran cantidad de material sobre el
tema.

7.1.

Segmentación de la imagen

La partición de la imagen en rangos cuadrados de tamaño fijo es la forma
más sencilla de afrontar el problema de la segmentación de la imagen. Este
enfoque, sin embargo, adolece de ser independiente de las caracterı́sticas de
la imagen considerada. La consideración de una partición adaptativa tiene
una gran cantidad de ventajas porque en las imágenes suele haber regiones
homogéneas que se pueden cubrir aceptablemente con bloques grandes y
regiones con grandes contrastes que precisan bloques más pequeños para
115

116

CAPÍTULO 7. MEJORAS EN LA CODIFICACIÓN FRACTAL

obtener una calidad determinada.

Árboles cuadriculares
La primera idea explorada en el contexto de la codificación fractal fue la
considerar dominios cuadrados de distintos tamaños (por ejemplo, de 4, 8 y
16 pixels de ancho). De esta forma surge con naturalidad el uso de árboles
cuadriculares en los que cada nodo tiene exactamente cuatro descendientes.
Al contrario que en la codificación con bloques de tamaño fijo, en este caso
es necesario pasar cierta información sobre el árbol cuadricular subyacente
al descodificador.
Con el uso de rangos de tamaño variable es posible diseñar un codificador
que proporcione resultados variables. El usuario puede indicar el nivel de
primacı́a de la calidad de la imagen sobre el ratio de compresión mediante un
factor de tolerancia del error ε. El codificador va dividiendo recursivamente
la imagen hasta que se alcanza este criterio como sigue:

1. Definir una tolerancia ε para el error E(R, D) y un valor máximo y
mı́nimo para el tamaño de los rangos. A continuación dividir la imagen
en rangos del tamaño máximo.
2. Crear una pila de rangos e inicializarla metiendo en ella los rangos de
tamaño máximo.
3. Mientras la pila no esté vacı́a, hacer:
a) Sacar el rango de la cima de la pila y buscar el dominio del tamaño
correspondiente que proporcione la mejor aproximación R ≈ sD+
o1 y el menor error E(R, D).
b) Si E(R, D) < ε o si el tamaño del rango es igual al del mı́nimo
tamaño permitido, entonces mostrar el código fractal correspondiente.
c) Si no, subdividir R en cuatro cuadrantes e introducirlos en la pila.

Variando el valor de ε es posible obtener codificaciones con distintos
ratios de compresión y distintos errores respecto a la imagen original. La
figura 7.1 representa el árbol cuadricular resultante de dos codificaciones
con criterios distintos.

7.1. SEGMENTACIÓN DE LA IMAGEN

117

Figura 7.1: Resultados de dos compresiones con árboles cuadriculares. A la izquierda se muestra la imagen del barco con calidad media (30.11 dB) y ratio de
compresión medio (13.95). A la derecha la misma imagen con mejor calidad (33.19
dB) y menor ratio de compresión (7.93). Bajo cada una de ellas se muestra su
plantilla asociada, que refleja la estructura del árbol cuadricular utilizado.

Partición HV
En la segmentación horizontal-vertical [FIS 95, p. 119 y ss.] una imagen rectangular se divide bien horizontal o bien verticalmente para generar
dos nuevos rectángulos. La subdivisión se repite de forma recursiva hasta
que se alcanza un determinado criterio de tolerancia como en el caso de
los árboles cuadriculares. El punto de corte se determina según la uniformidad del bloque a dividir de manera que se evita la restricción de partir la
imagen siempre por determinadas posiciones fijas, además de aumentar las
posibilidades de que distintos rectángulos posean estructuras similares.

118

CAPÍTULO 7. MEJORAS EN LA CODIFICACIÓN FRACTAL

La variedad de formas de los rangos implica una mayor complejidad en
el diseño del codificador. Sin embargo, a pesar del incremento de espacio necesario para poder almacenar este tipo de partición, muchos experimentos
demuestran que las curvas ratio-distorsión se ven considerablemente mejoradas respecto al uso de árboles cuadriculares.

Partición triangular
Para superar el inconveniente de los dos métodos anteriores al restringir
la orientación de las aristas, [DAV 95] propone el uso de triángulos, que se
pueden adaptar mucho mejor a las caracterı́sticas de la imagen y reducir
el efecto de bloque. Mediante el conocido algoritmo de triangulación de
Delaunay se va refinando sucesivamente una partición triangular según los
valores de gris de sus triángulos. En una fase posterior se reduce el número de
códigos fractales obtenidos fusionando pares de triángulos si el cuadrilatero
resultante es convexo y si ambos triángulos poseen una distribución de grises
similar.

Partición genética
Para determinar una partición adecuada de la imagen se han propuesto también algoritmos basados en computación evolutiva. En [SAU 96b] se
considera un rango como un conjunto de pequeños bloques de la imagen
conectados. Cada población está formada por Np configuraciones, esto es,
Np particiones cada una con su lista de códigos fractales. En cada paso
de evolución se generan σ hijos que heredan las particiones de sus padres,
excepto por la fusión de dos rangos vecinos aleatorios. De entre todos los
descendientes se seleccionan los mejores para la población de la siguiente
generación basándose en el error E(R, D). Una codificación compacta de la
estructura resultante no es trivial. Una posibilidad es almacenar el recorrido
por el borde de cada rango indicando en cada paso qué dirección se toma
(giro a la izquierda, giro a la derecha o seguir recto).

7.2.

Transformación geométrica de los dominios

En la página 104 mostramos que la aplicación que transforma un dominio
Di en un rango Ri puede descomponerse en una transformación espacial y en
otra que actúa sobre los valores de las intensidades del dominio. Centrándonos ahora en aquélla, cabe preguntarse de qué forma influyen los coeficientes

7.2. TRANSFORMACIÓN GEOMÉTRICA DE LOS DOMINIOS

119

ai , bi , ci y di de la ecuación 6.1 en el resultado de la codificación fractal.1
Lo habitual en la práctica totalidad de las implementaciones de algoritmos de compresión fractal es considerar ai = di = 1/2 y bi = ci = 0, lo
que lleva al uso de dominios con un tamaño doble al de los rangos. Otra
posibilidad es utilizar dominios con tamaño triple al de los rangos mediante
los valores ai = di = 1/3.
Pero la cuestión realmente importante es si es posible utilizar dominios
y rangos del mismo tamaño haciendo ai = di = 1. En principio, ninguna de
las proposiciones del tema anterior requerı́an tal condición y esta elección
parece la más natural a primera vista. Aunque algunas evaluaciones experimentales [SAU 96a, p. 15] parecen indicar que el error es mayor en este caso,
el verdadero problema es que, si no se guarda cuidado, una region puede terminar referenciándose a sı́ misma lo que va contra el principio de identificar
diferentes objetos similares en la imagen. Al utilizar dominios con el doble
de tamaño este problema queda automáticamente resuelto. Con todo, si se
evitan las autorreferencias (a nivel de una o varias regiones), el uso de dominios y rangos del mismo tamaño [LU 97, p. 128] puede ser beneficioso, ya
que suele ser habitual en una imagen encontrar dos objetos similares a la
misma escala.
También es una práctica habitual agrandar la lista de dominios incluyendo bloques obtenidos rotando cada dominio valores múltiplos de 90 grados
y haciendo esto mismo sobre su versión reflejada (simétrica). Las ocho posibles isometrı́as resultantes son las mostradas en la tabla 7.1. De esta forma
la lista de dominios es ocho veces más grande y es de esperar que mejoren
las curvas ratio-distorsión. Debe tenerse en cuenta, por otra parte, que la
información sobre la isometrı́a utilizada (3 bits) debe pasarse también al
descodificador con lo que se incrementa el número de bits necesarios para
cada código fractal. En [SAU 96a, p. 15] se sugiere que la complejidad que
la consideración de las isometrı́as introduce en el algoritmo no está justificada. Entre ambos extremos, [LU 97, p. 125] propone utilizar sólo las cuatro
primeras isometrı́as de la tabla 7.1.
La tabla 7.2 refleja el error obtenido al codificar la imagen del tucán
considerando distintos números de isometrı́as y un árbol cuadricular de dos
niveles (8 × 8 y 4 × 4). La figura 7.2 muestra cómo queda la imagen del tucán
codificada únicamente sobre la transformación identidad. Nótese cómo, al
menos en este caso, parece razonable el uso exclusivo de la forma identidad,
ya que la enorme ganancia en términos de velocidad compensa la leve pérdida
de calidad obtenida.

1

Los coeficientes ui y vi simplemente trasladan el dominio a la posición del rango por
lo que no existen alternativas posibles a la hora de escoger sus valores.

120

CAPÍTULO 7. MEJORAS EN LA CODIFICACIÓN FRACTAL
Índice
0

Isometrı́a

Ã

identidad
Ã

1

simetrı́a respecto a x
Ã

2

simetrı́a respecto a y
Ã

3

rotación de π
Ã

4

simetrı́a respecto a la primera diagonal
Ã

5

giro de π/2
Ã

6

giro de 2π/3
Ã

7

simetrı́a respecto a la segunda diagonal

Matriz
+1/2 0
0 +1/2
−1/2 0
0 +1/2
+1/2 0
0 −1/2
−1/2 0
0 −1/2
0 +1/2
+1/2 0
0 −1/2
+1/2 0
0 +1/2
−1/2 0
0 −1/2
−1/2 0

!

!

!

!

!

!

!

!

Cuadro 7.1: Las ocho isometrı́as pueden deducirse de las ecuaciones dadas en 4.2.
Las transformaciones 1, 2, 4 y 7 son combinaciones de 0, 3, 5 y 6 con una inversión.

Núm. isometrı́as
8
4
1

Tiempo (seg)
955.9
486.9
142.0

Compresión
13.39
13.2
14.69

RECM
3.38
3.44
3.69

RSRM
37.55
37.39
36.77

Cuadro 7.2: El efecto del número de isometrı́as consideradas al codificar la imagen
del tucán. Para la compresión se utilizó un árbol cuadricular de dos niveles sin
clasificación previa de los dominios. El ratio de compresión ligeramente superior de
la última fila se debe al hecho de no tener que almacenar la información sobre la
isometrı́a asociada a cada código fractal cuando se usa exclusivamente la identidad.

7.3.

Postprocesamiento

Al codificar cada rango independientemente no se puede garantizar que
las transiciones entre pixels adyacentes situados en los bordes de las regiones

7.3. POSTPROCESAMIENTO

121

Figura 7.2: El tucán con un ratio de compresión de 14.69 sin utilizar transformaciones adicionales sobre los dominios.

sean suaves. El ojo es sensible a estas discontinuidades, incluso cuando no son
muy abruptas. Una manera de reducir la aparición de este efecto de mosaico
es postprocesando la imagen. Una posibilidad [FIS 95, p. 61] es modificar,
una vez descodificada la imagen, los pixels situados en las fronteras de los
rangos mediante un promedio ponderado de sus valores. Si los valores de
gris a ambos lados de la frontera son a y b, entonces el valor de a se sutituye
por
w1 a + w2 b
w1 + w2

Aunque son valores totalmente heurı́sticos, w2 ≈ (1/3)w1 suele proporcionar los mejores resultados. Aun ası́, la mejora de la calidad es casi imperceptible, como se muestra en la figura 7.3, donde se indica el error sobre una
codificación de la imagen del pájaro con y sin este tipo de postprocesamiento.
Como se ve la mejora es inferior a 1/2 dB.
Para reducir el efecto de bloque se ha propuesto también [DUG 96] el uso
de rangos solapados. La codificación presenta entonces cierta redundancia
en las zonas de solapamiento. El descompresor promedia los valores dados
por cada rango suavizando el aspecto de la imagen y reduciendo el error al
disponer de varios códigos fractales para algunos pixels. Como contrapartida,
el rendimiento de la compresión se reduce al tener que tratar más de una
vez algunas zonas de la imagen.

122

CAPÍTULO 7. MEJORAS EN LA CODIFICACIÓN FRACTAL

(a) RECM=6.44, RSRM=31.95

(b) RECM=6.78, RSRM=31.54

Figura 7.3: Descodificación del pájaro con postproceso (arriba) y sin él (abajo).
Los pesos usados en el postprocesamiento fueron w1 = 3 y w2 = 1. El ratio de
compresión es de 23.73.

7.4.

Clasificación de los dominios

Durante la codificación fractal deben explorarse un gran número de dominios para cada rango de la imagen. Esta exploración es la causa principal
del elevado coste temporal del algoritmo. Uno de los caminos más utilizados para reducir este coste es el de la clasificación de los dominios. En los
métodos de clasificación los dominios se agrupan independientemente antes
del inicio real de la codificación en un número predefinido de clases. Para
un rango determinado sólo se busca su dominio asociado en la clase que le
corresponde. Existen una gran variedad de algoritmos que siguen esta idea.

7.4. CLASIFICACIÓN DE LOS DOMINIOS

123

A continuación mostramos cuatro por orden creciente de complejidad.

Estructura de dominios
En su trabajo original Jacquin clasificó los dominios según sus caracterı́sticas geométricas distinguiendo sólo tres tipos de bloques [SAU 94b]:
Bloques suaves Las intensidades varı́an levemente a lo largo del bloque.
Bloques con aristas Presentan un cambio abrupto de intensidad, por
ejemplo, debido al borde de un objeto de la imagen.
Bloques intermedios Contienen variaciones de intensidad mayores que
los bloques suaves, pero sin presentar un gradiente tan pronunciado
como en el caso de los bloques con aristas.
Los bloques intermedios son los que cobijan normalmente las texturas
existentes en la imagen. Los rangos que se clasifican como bloques suaves
pueden aproximarse correctamente a través del desplazamiento o y no es
necesario buscar un dominio para ellos (en este caso, s = 0). Por lo tanto,
en este esquema sólo existen dos clases, en una de las cuales debe buscarse
para cada rango que no sea un bloque suave.

Vectores de caracterı́sticas
Otra sencilla forma de clasificar los dominios a partir de su estructura,
muy ligada a la anterior, se basa en el establecimiento de vectores de caracterı́sticas [COL 96]. Consideremos un dominio D cuadrado formado por las
intensidades d1 , d2 , . . . , dn y definamos su nivel de gris medio como
D̄ =

n
1X
di
n i=1

Para un rango cuadrado R el nivel medio de gris R̄ puede definirse de
forma análoga. Consideremos entonces la media anterior de cada bloque y
la media de sus cuatro cuadrantes no solapados (izquierda superior, derecha
superior, izquierda inferior y derecha inferior) que denotaremos por Ai con
i = 0, 1, 2, 3. Con estos valores podemos definir un vector de caracterı́sticas
para dominios ω = {ωi , i = 0, . . . , 3} como sigue:
(

ωi =

1
0

:
:

si Ai > D̄
si Ai ≤ D̄

124

CAPÍTULO 7. MEJORAS EN LA CODIFICACIÓN FRACTAL

y análogamente en el caso de rangos. Lo anterior permite 16 vectores (realmente 15) de estructura distintos. Durante la codificación sólo se realizará la
comparación con aquellos dominios que poseen la misma estructura que el
rango actual.

Reordenación de cuadrantes
Una clasificación más elaborada [FIS 95, p. 57] divide, al igual que la
anterior, los rangos y dominios cuadrados en cuatro cuadrantes. Para cada
cuadrante se calcula la intensidad media de los pixels Ai y las varianzas
Vi (i = 0, . . . , 3). Es fácil ver que siempre puede reorientarse (mediante
giros y reflejos) cualquier rango o dominio de manera que las intensidades
promediadas estén ordenadas de una de las tres formas siguientes:
Clase principal 1: A0 ≥ A1 ≥ A2 ≥ A3
Clase principal 2: A0 ≥ A1 ≥ A3 ≥ A2
Clase principal 3: A0 ≥ A3 ≥ A1 ≥ A2
Una vez reorientado el bloque, existen 24 (factorial de 4) formas diferentes de ordenar las varianzas, que definen 24 subclases para cada clase
principal. De esta forma podemos considerar bien 3 clases distintas, bien
72.2

Agrupamiento de Heckbert
Aunque la clasificación por reordenación de cuadrantes considera la división de los dominios en un número relativamente grande de clases, éste
todavı́a puede impedir la codificación en entornos en los que la velocidad
sea el factor primordial. En este apartado consideraremos un método que
permite aumentar casi indefinidamente el número de clases consideradas.
El algoritmo de agrupamiento (clustering) de Heckbert [LU 97, pp. 179
y ss.] divide un conjunto de vectores en M grupos (clusters) realizando sucesivas divisiones mediante hiperplanos perpendiculares a alguna dirección.
Describiéndolo por encima, el algoritmo se basa en elegir el grupo actual con
mayor cantidad de vectores, encontrar la dirección en la que el grupo tiene
su mayor extensión y dividirlo mediante un hiperplano perpendicular a esa
dirección en el valor que hace los tamaños de los dos grupos generados lo
2

Si el valor del escalado s es negativo, deben reconsiderarse las ordenaciones anteriores.
Por lo tanto, cada dominio se clasifica con dos orientaciones, una orientación para valores
de s positivos y otra para valores negativos.

7.4. CLASIFICACIÓN DE LOS DOMINIOS

125

más parecidos posible. Este proceso se repite hasta que se obtiene el número
de grupos deseados.
Consideremos ahora el conjunto de dominios de la forma
D = d1 , d2 , . . . , dn de una imagen como un conjunto finito de vectores x ∈ Ω ∈ Rn y sea M el número deseado de grupos. El algoritmo
de Heckbert define la clasificación a través de una secuencia de M − 1
divisiones. Cada división se define mediante cuatro parámetros (k, k 0 , i, v),
donde el k-ésimo grupo se divide por el i-ésimo hiperplano en el valor v en
dos grupos, el grupo k y el grupo k 0 para xi < v y xi ≥ v, respectivamente.
En resumen, el algoritmo divide del k-ésimo grupo al k 0 -ésimo en la variable
i-ésima sobre el valor v. El criterio para determinar las particiones es el
siguiente:

Inicialización Inicialmente tenemos un único grupo:
1. Sea C00 = Ω.
Primera división En la primera división s = 1:
1. Elegir el grupo k1 = 0.
2. Encontrar la dirección i1 con mayor extensión, esto es,
(

máx {xi1 − yi1 } = máx

0≤i<n

x,y∈C00

)

máx {xi − yi }

x,y∈C00

3. Encontrar el valor de división por la mitad v1 para que
¯
¯
¯
0¯
card
C
¯
0¯
¯=
¯card{x ∈ C00 : xi1 < v1 } −
¯
2 ¯
¯
¯
¯
card C00 ¯¯
¯
0
ı́nf ¯card{x ∈ C0 : xi1 < v} −
¯
v∈R ¯
2 ¯

4. Dividir el grupo en dos:
C01 = {x ∈ C00 : xi1 < v1 } y

C11 = {x ∈ C00 : xi1 ≥ v1 }

Resto de divisiones En las siguientes divisiones s = 2, 3, . . . , n − 1:
1. Elegir el grupo más grande ks (si hay más de uno, escoger cualquiera de ellos), esto es,
card Cks−1
= máx card Cks−1
s
0≤k<s

126

CAPÍTULO 7. MEJORAS EN LA CODIFICACIÓN FRACTAL
2. Encontrar la dirección is con la mayor extensión, esto es,



máx {xis − yis } = máx

x,y∈Cks−1
s




máx {xi − yi }

0≤i<n x,y∈C s−1
ks



3. Encontrar el valor de división por la mitad vs tal que
¯
¯
¯
¯
card Cks−1
¯
¯
s−1
s
¯card{x ∈ Cks : xis < vs } −
¯=
¯
¯
2
¯
¯
s−1 ¯
¯
card
C
¯
¯
k
s
ı́nf ¯card{x ∈ Cks−1
: xis < v} −
¯
s
¯
¯
v∈R
2

en dos:
4. Dividir el grupo Cks−1
s
Ckss = {x ∈ Cks−1
: xis < vs } y
s

Css = {x ∈ Cks−1
: xis ≥ vs }
s

y establecer los otros grupos a los mismos que antes:
Cks = Cks−1 ,

para 0 ≤ k < s

y

k 6= ks

A través de la sucesión de hiperplanos puede conocerse el grupo asociado
a cualquier vector x del conjunto Ω. En concreto, puede obtenerse el grupo
de dominios asociado a cada rango de la imagen y realizar la búsqueda
exclusivamente sobre ellos.
En la tabla 7.3 puede observarse cómo a partir de cierto punto el incremento del número de grupos no lleva asociado un incremento substancial en
el tiempo de codificación. Esto es debido a que la clasificación preliminar de
los dominios consume la práctica totalidad del tiempo. En particular, este
tiempo representa más del 90 % del total a partir de un número de grupos
superior a 256.

7.5.

Compresión sustituyente

Una vez empaquetados los bits que definen cada código fractal y almacenados de forma contigua en un fichero puede intentarse su compresión
mediante técnicas como las comentadas en la página 90 (entropı́a, codificación aritmética y otros). Existe, sin embargo, un problema y es que los
programas existentes suelen operar a nivel de bytes con lo que pueden no
apreciar una regularidad existente en los datos si el tamaño de cada código
fractal no es múltiplo de 8 bits. Parece necesario, por tanto, adaptar los
algoritmos para que ajusten su dinámica a la de los códigos fractales.

7.6. INDEPENDENCIA DE LA RESOLUCIÓN
Núm. Grupos
1
2
4
8
16
32
64
128
256
512
1024
2048

Tiempo (seg)
1393.0
771.6
419.0
238.7
147.9
101.6
79.4
67.9
62.4
59.6
58.6
58.5

RECM
3.14
3.37
3.64
3.89
4.23
4.54
4.76
5.07
5.30
5.64
5.85
6.25

127
RSRM
38.18
37.58
36.9
36.33
35.59
34.99
34.59
34.02
33.64
33.1
32.78
32.21

Cuadro 7.3: Efecto del número de grupos del algoritmo de Heckbert sobre la
imagen de las flores. Los tiempos corresponden a un sistema Pentium II 233 MHz
con Linux. Se utilizaron rangos de 4 × 4 y cuatro isometrı́as. El ratio de compresión
resultante fue de 4.7.

En concreto, distintos ficheros de códigos fractales comprimidos mediante
el algoritmo LZ77 de Lempel-Ziv (programa gzip) no lograban reducir el
tamaño de los ficheros más allá del 6 %.

7.6.

Independencia de la resolución

Una caracterı́stica particular de las imágenes comprimidas fractalmente,
y que distingue especialmente a esta técnica de las otras técnicas de compresión, es el denominado zoom o ampliación fractal. Las imágenes codificadas
mediante la transformada fractal se describen exclusivamente como el punto
fijo de un determinado operador y no presentan referencias a ninguna escala
particular de la imagen ni a ningún tamaño determinado en pixels. Por ello,
un código fractal puede descodificarse a cualquier resolución, generando detalles en todas las escalas. Este es uno de los aspectos que, pese a algunas
crı́ticas, justifica considerar este método como fractal.
Evidentemente los detalles generados al descodificar una imagen a escalas
mayores son artificiales y no tienen mucho que ver con los existentes en el
mundo real cuando fue tomada, pero debido al carácter autorreferencial del
modelo, estos detalles son relativamente consistentes con la apariencia global
de los objetos y presentan una estructura más natural que las imágenes
ampliadas simplemente por duplicación de pixels o interpolación.

128

CAPÍTULO 7. MEJORAS EN LA CODIFICACIÓN FRACTAL

Figura 7.4: A la izquierda parte de la imagen del barco descodificada a cuatro
veces su tamaño y a la derecha la imagen original agrandada 4 veces mediante la
técnica de duplicación.

Si tenemos un código fractal asociado a un rango Ri de tamaño n ×
n con origen en las coordenadas (xi , yi ) y perteneciente a una imagen de
tamaño x̄ × ȳ, el código fractal correspondiente para generar una imagen de
tamaño 2x̄ × 2ȳ con el rango Ri de tamaño 2n × 2n se obtiene simplemente
sustituyendo el origen del rango por (2xi , 2yi ).
La figura 7.4 muestra las diferencias entre la forma clásica de ampliación
y el zoom fractal con el faro que aparece a lo lejos en la imagen del barco.
La independencia de la resolución se ha utilizado algunas veces como

7.7. MEJORA DE LA RESOLUCIÓN

129

justificación de ciertos resultados muy cercanos a la publicidad engañosa.
Supongamos que codificamos una determinada imagen, obtenemos un ratio
de compresión de 12 y descodificamos la imagen comprimida con un factor
de ampliación de 2, es decir, a un tamaño doble del de la original. Algunos
autores justifican el siguiente razonamiento: la imagen descodificada tiene
4 veces más pixels que la original, por lo que su ratio de compresión es de
12 · 4 = 48. El lector ya está prevenido.

7.7.

Mejora de la resolución

Muy ligado a la técnica de la sección anterior se encuentra la mejora
de resolución [LU 97, p. 218]. Mediante esta técnica una imagen de poca
resolución puede codificarse fractalmente y descodificarse a continuación a
mayor resolución generando una versión mejorada.

7.8.

Aceleración de la compresión

En esta sección veremos algunas ideas para acelerar la compresión fractal
mediante técnicas que no utilizan clasificación de dominios.

Búsqueda local
Ocurre a menudo que el dominio con que se empareja un rango se encuentra relativamente cerca de éste. Esta situación es comprensible, ya que
la distribución de los objetos por una imagen suele determinar que regiones
similares se encuentren cercanas al pertenecer ambas al mismo objeto. Esta
hipótesis, sin embargo, no tiene que ocurrir (y de hecho no ocurre) en todas
las imágenes, ni siquiera en todos los rangos de una determinada imagen.
Sin embargo, esto da la idea de utilizar un algoritmo de compresión que favorezca los códigos fractales con dominios locales al rango correspondiente.
El algoritmo de compresión que incorpore esta idea [LU 97, pp. 117 – 122]
puede realizar una búsqueda local sobre un reducido número de dominios
cercanos al rango actual, además de sobre la lista completa de domimios.
Las ventajas de este enfoque es que permite utilizar muchos menos bits para
referenciar al dominio óptimo si éste se encontró en la búsqueda local.3 Es
3

Puede añadirse a cada código fractal un bit que indique si la información sobre el
dominio es la de un dominio local o global. Si una gran cantidad de referencias a lo largo
de la imagen son locales, se reducirá el ratio de compresión sin ninguna pérdida de calidad.

130

CAPÍTULO 7. MEJORAS EN LA CODIFICACIÓN FRACTAL

más, llegado el caso puede desestimarse la realización de la búsqueda global
si el mejor dominio encontrado localmente proporciona un error aceptable.
Es posible también cierta ganancia en velocidad, ya que [LU 97] sugiere el
uso exclusivo de la forma identidad de la tabla 7.1 para la búsqueda local.

Búsqueda del vecino más cercano
El tiempo de la codificación en la compresión fractal de imágenes es
considerablemente elevado. Si el número de dominios manejados es N , entonces el tiempo invertido en la búsqueda para cada rango es lineal con N ,
es decir, O(N ). Las técnicas de clasificación de dominios vistas en 7.4 pretenden reducir este tiempo mediante la división en grupos de los dominios
según determinadas caracterı́sticas. De esta forma en cada búsqueda sólo se
evalúan los dominios de un determinado grupo. Sin embargo, este enfoque
sólo reduce el factor de proporcionalidad de la complejidad, que sigue siendo
O(N ).
En [SAU 95] se demuestra que el problema de encontrar pares óptimos
de rangos y dominios es equivalente al de búsqueda del vecino más cercano
en un espacio euclı́deo adecuado formado por vectores de caracterı́sticas de
los dominios y de los rangos. El problema de encontrar el vecino más cercano
puede ser resuelto con las estructuras de datos y los algoritmos adecuados
en O(log N ), con lo que se obtendrá una clara ventaja sobre la búsqueda
lineal, mayor cuanto mayor sea el número de dominios N .
Consideremos cada rango Ri como un vector Ri ∈ Rn y, análogamente,
cada dominio Di (convenientemente submuestreado) como un vector del
mismo espacio, Di ∈ Rn . Sea e = √1d (1, . . . , 1) ∈ Rn un vector unitario con
el mismo número de componentes y definamos
φ(x) =

x − hx, eie
kx − hx, eiek

donde x ∈ Rn y h·, ·i denota el producto escalar en Rn con lo que kxk =
p
hx, xi.

Ahora puede demostrarse que la minimización del error cuadrático medio
E(R, Di ), con i = 1, . . . , N , es equivalente a la búsqueda del vecino más
cercano de φ(R) ∈ Rn en el conjunto de los 2N vectores ±φ(Di ) ∈ Rn .
Como ya se ha dicho, este problema puede resolverse con coste O(log N ).
A la hora de realizar una implementación práctica del algoritmo deben
tenerse en cuenta algunas cosas:
No pueden obviarse los requerimientos espaciales de manejar los vecto-

7.8. ACELERACIÓN DE LA COMPRESIÓN

131

res de caracterı́sticas al trabajar con listas de miles de dominios. Para
limitar este espacio pueden submuestrearse los dominios y los rangos
a una dimensión moderada, lo que implica que el método anterior sólo
podrá dar resultados aproximados, pero no exactos.
Otra alternativa para reducir la complejidad espacial es considerar en
el algoritmo de búsqueda del vecino más cercano ı́ndices de rangos y
dominios en lugar de los vectores en sı́. Con este enfoque las componentes de intensidad de cada bloque se obtienen accediendo directamente
a la imagen mediante una función que devuelva el bloque asociado a
un determinado ı́ndice.
Para un determinado rango no todos los dominios de la lista de dominios son admisibles, en el sentido de que el factor de escalado s
obtenido mediante el método de los mı́nimos cuadrados puede no garantizar la contractividad de la transformación (en general, si |s| ≥ 1).
Para considerar esta posibilidad, no se realiza la búsqueda del vecino
más cercano, sino de los k vecinos más cercanos,4 quedándonos con
aquél que nos asegure la convergencia del modelo.

Distancia entre dominios
En la página 107 se señaló que una forma de reducir el número de regiones de la lista de dominios es considerando que los orı́genes de dominios
consecutivos (en el orden establecido con un barrido por lı́neas) no están
situados en pixels adyacentes, sino que existe una separación l > 1 entre
ellos. De esta manera el número de comparaciones se reduce por un factor l
y con ello el tiempo necesario para la codificación, aunque es de esperar que
a costa de una pérdida de la calidad al existir el riesgo de saltarnos alguna
región importante.
La tabla 7.4 cuantifica estas pérdidas sobre la imagen del perro pastor
en la que valores pequeños de l no deberı́an alterar excesivamente la distorsión de la imagen codificada. La codificación usa exclusivamente el esquema
básico del capı́tulo anterior. La figura 7.5 muestra la imagen codificada con
dominios con orı́genes separados 128 pixels. Debe tenerse cuidado con generalizar estos resultados, ya que debido al gran parecido de todas las regiones
de esta imagen era de esperar poca influencia del valor de l.5 En general, el
valor de l es mucho más importante en la mayor parte de las imágenes y no
debe mantenerse muy por encima de 4.
4

El problema de los k vecinos más cercanos también es resoluble en tiempo logarı́tmico.
Aun ası́ los resultados son espectaculares: el tiempo de codificación logra reducirse a
la centésima parte con una reducción de 5 dB.
5

132

CAPÍTULO 7. MEJORAS EN LA CODIFICACIÓN FRACTAL
Separación
1
2
3
4
5
6
7
8
16
32
64
128

Tiempo (seg)
78.9
38.8
25.8
19.9
15.5
13.0
11.2
9.9
5.0
2.7
1.4
0.7

RECM
3.32
3.47
3.62
3.69
3.9
3.87
3.86
3.92
4.2
4.55
5.12
5.87

RSRM
37.69
37.31
36.97
36.79
36.32
36.39
36.39
36.27
35.67
34.98
33.95
32.75

Cuadro 7.4: Efecto de la separación entre los origenes de los dominios con la
imagen del perro pastor. Los tiempos corresponden a un sistema Pentium II 233
MHz con Linux. Se utilizaron rangos de 4 × 4 y el ratio de compresión resultante
fue de 5.81.

Figura 7.5: Considerando sólo 256 dominios la imagen del perro pastor se codifica
tan bien como se muestra aquı́ en tan sólo 0.7 segundos. El ratio de compresión es
5.87 y la RSRM 32.75.

7.9.

Aceleración de la descompresión

Aunque la diferencia entre los tiempos necesarios para la codificación y
la descodificación fractal es abismal (en beneficio de esta última), en apli-

7.10. ENFOQUES HÍBRIDOS

133

caciones en las que la velocidad de descodificación es vital (por ejemplo,
vı́deo en tiempo real) se hace necesario encontrar métodos más rápidos que
el clásico. Se han propuesto distintos métodos [SAU 96a, p. 45] que incluso
evitan en parte la iteración del esquema tradicional.

Encadenamiento de pixels
Una de las técnicas de descodificación rápida [LU 97, p. 207] es la conocida como encadenamiento de pixels, aplicable cuando el ajuste del tamaño de
los dominios al de los rangos se realiza mediante submuestreo y no mediante
el promediado que hemos considerado hasta ahora.
Aunque no vamos a entrar en demasiados detalles, el método parte del
hecho de que para cada pixel de la imagen existe un único pixel, llamado
pixel de dominio asociado, que llega hasta él mediante la transformación
fractal correspondiente. Si conociésemos el valor de este pixel de dominio,
el valor correspondiente del pixel destino podrı́a derivarse de un solo paso.
Considerando este pixel de dominio como un nuevo pixel destino, podemos
encontrar para él otro nuevo pixel de dominio. Siguiendo este procedimiento,
podemos construir una cadena de pixels. Esta cadena se detiene cuando
choca con un punto que ya está en la cadena o con un pixel de valor conocido
que ya fue calculado en una cadena anterior.
Si el pixel es conocido, podemos generar los valores de todos los pixels de
la cadena siguiéndola hacia atrás y aplicando los ajustes de brillo y contraste
definidos por cada código fractal.
En el otro caso, el valor del último pixel de la cadena se obtiene saltando
hacia atrás un número suficiente de pixels, estableciendo el valor del pixel
final al gris 128 y deshaciendo el camino hasta dar valor al último pixel de
la cadena. Desde este momento podemos dar valor al resto de pixels de la
cadena como antes. Si s = 3/4, entonces basta saltar 20 pixels hacia atrás
para que el error cometido sobre el último pixel de la cadena sea despreciable.

7.10.

Enfoques hı́bridos

La unión hace la fuerza y son los enfoques hı́bridos los que en la actualidad acaparan las lı́neas más prometedoras de investigación [LU 97, pp. 41
– 45], especialmente los que aunan de diversas formas la teorı́a fractal con
los wavelets.
Los aspectos analizados en este capı́tulo no cubren en absoluto todas las

134

CAPÍTULO 7. MEJORAS EN LA CODIFICACIÓN FRACTAL

alternativas al enfoque básico de codificación fractal y se han presentado
como pequeña muestra de la increible evolución experimentada por estos
desarrollos en apenas una década. Los próximos años verán, sin duda, la
aparición de innovadoras técnicas fractales que mejorarán nuestra visión del
mundo en todas sus escalas.

Apéndice A

Medida de conjuntos
A menudo resulta necesario determinar el tamaño de un fractal para
poder establecer su similitud con algún otro. Existen distintos números asociados con los fractales que nos permiten compararlos. Estos números, denominados normalmente dimensiones fractales, son un intento de cuantificar
nuestra idea subjetiva acerca de la densidad con que un fractal ocupa el
espacio al que pertenece.
La más importante de tales dimensiones es la dimensión de Hausdorff,
aunque su dificultad de instrumentación en la práctica lleva a considerar
desarrollos alternativos. Aun ası́, el camino hasta la obtención de una medida satisfactoria pasó por el rechazo de ciertas métricas que se mostraban
impotentes con los conjuntos fractales a pesar de su consolidada eficacia
con la mayor parte de los conjuntos clásicos. De todo esto se hablará en
este apéndice. Los conceptos a continuación presentados pueden accederse
también a través de [GUZ 93] y [BAR 93a].

A.1.

La medida de Lebesgue

La medida de conjuntos de puntos en la recta, en el plano o en el espacio
tiene tras de sı́ una larga historia. Ya Euclides consideró el cálculo de áreas
de rectángulos y triángulos. Para otras regiones limitadas por lı́neas rectas
se procedı́a a su triangulación. Si la región está limitada por curvas, la
triangulación no es posible y se hace necesario recurrir entonces al método
de exahusción propuesto por Arquı́medes y que es, básicamente, una versión
del concepto actual de paso al lı́mite en el que se van considerando sucesivos
triángulos de área cada vez menor para rellenar adecuadamente la región.
135

136

APÉNDICE A. MEDIDA DE CONJUNTOS

Costó dos mil años abandonar la tradición griega de la triangulación
que requerı́a grandes dosis de ingenio y habilidad para cada caso particular.
En la década de 1890, Peano y Jordan consideraron un método basado en
enlosados que, aun siendo mucho mejor que el de triangulación, producı́a
resultados erróneos con cierto tipo de conjuntos.
No estaba claro cómo habı́a que proceder para encontrar una definición
satisfactoria de área hasta que Emile Borel sugirió la idea de usar enlosados
formados por infinitos rectángulos.
La idea de Borel fue desarrollada por Henri Lebesgue llegándose ası́ a
la definición de lo que hoy dı́a se conoce como medida de Lebesgue y que
generaliza la idea de longitud, área y volumen en la recta, plano y espacio,
respectivamente. Exponemos la definición en Rn , aunque puede ser más
sencillo pensar en lo siguiente que n = 2.
Consideremos la familia < de los hipercubos (rectángulos) R de Rn de
la forma
[a1 , b1 ) × · · · × [an , bn ) = R
Definimos el volumen V (R) de un rectángulo R como
V (R) = (b1 − a1 ) . . . (bn − an )
Dado ahora E ⊂ Rn consideraremos todos los recubrimientos (enlosaS
dos) {Ri } de E, es decir, tales que E = ∞
i=1 Ri y definimos
n

L (E) = ı́nf

(∞
X

)

V (Ri ) : {Ri } es un recubrimiento de E

i=1

Pueden demostrarse las siguientes proposiciones:
a) Ln (∅) = 0
b) Ln (A) ≤ Ln (B) si A ⊂ B
c) para cualquier familia numerable {Ek }, k = 1, 2, . . ., de subconjuntos
de Rn se verifica
Ã
!
Ln

∞
[

k=1

Ek

∞
X

≤

Ln (Ek )

k=1

propiedad denominada subaditividad .
A Ln se le denomina medida exterior de Lebesgue. Sin embargo, no es
cierto siempre que dado un E tal que E ⊂ Rn se verifique
Ln (E − B) + Ln (E ∩ B) = Ln (E)

para todo B ⊂ Rn

A.2. PROBLEMA DEL ÁREA

137

es decir, que la medida de un conjunto sea siempre igual a la suma de las
medidas de dos partes complementarias. Cuando dicha igualdad es cierta
para todo B ⊂ Rn diremos que el conjunto E es medible Lebesgue, hecho
que afortunadamente ocurrı́a para los conjuntos más usuales hasta que los
fractales comenzaron a ser conjuntos usuales.
Si E es medible Lebesgue, Ln (E) es una medida del conjunto E que
se llama medida de Lebesgue. La clase de los conjuntos medibles Lebesgue
abarca a una clase de conjuntos muy amplia.

A.2.

Problema del área

Si bien el problema de medir el área de una región plana queda razonablemente resuelto con la definición de la medida de Lebesgue, el problema
de medir el área de una superficie no contenida en un plano presenta serias dificultades, debido a la rigidez que impone el tener que recubrir bien
con rectángulos, bien con cubos. Si la superficie es suave (diferenciable), el
problema lo resolvı́a la geometrı́a diferencial que Gauss habı́a iniciado en el
primer tercio del siglo XIX, pero en el caso de una superficie menos regular
no se disponı́a de una opción alternativa.
Por si fuera poco, la insuficiencia de la medida de Lebesgue se hizo aún
más evidente a comienzos del siglo XX con la aparición de los primeros
conjuntos fractales.

A.3.

Dimensión

Hay varios conceptos matemáticos diferentes que responden al nombre
de dimensión de un conjunto geométrico. Uno de ellos, el de dimensión
topológica, hace alusión a la forma de ocupar el espacio que tiene el conjunto.
Ası́, tanto a una curva diferenciable, como a la curva de Koch, o a la de
Peano, se les asigna dimensión topológica igual a uno, y a un punto, a los
puntos racionales de la recta real y al conjunto de Cantor se les asigna
dimensión cero.
Pero este tipo de dimensiones plantean un serio problema pues resultan
ser poco finas al otorgar la misma dimensión a un único punto que a un
conjunto no numerable de puntos como es el conjunto de Cantor. La medida de Lebesgue Ln (E) proporciona análogas consecuencias al considerar
siempre dimensiones enteras n = 1, 2, 3 . . . Este planteamiento llevó a tomar
en consideración la introducción de categorı́as intermedias o dimensiones

138

APÉNDICE A. MEDIDA DE CONJUNTOS

fraccionarias.

A.4.

Dimensión de homotecia

Una manera de asignar una dimensión fraccionaria a un conjunto parte
de una propiedad de la dimensión que sirve a Platón en su diálogo Menón
para demostrar la doctrina de la reminiscencia a partir de la creencia mı́tica
en la preexistencia y transmigración del alma. Sócrates mediante el método
mayéutico consigue que un esclavo iletrado de Menón recupere de sı́ mismo
el conocimiento acerca de cómo cuando el lado de un cuadrado se duplica
se obtiene una figura equivalente a cuatro, y no a dos, de los cuadrados
iniciales. Si en lugar de un cuadrado se tratara de un cubo, el factor de
multiplicación serı́a de ocho y no de cuatro.
Nos encontramos, por tanto, con un rango distintivo de cada dimensión.
Precisándolo más, supongamos que una figura de dimensión entera d puede
ser descompuesta en n copias a escala r de sı́ misma (piénsese en el ejemplo
del cuadrado de Socrates donde d = 2, r = 1/2 y n = 4). Entonces es fácil
ver que n = (1/r)d o tomando logaritmos que
d=

log n
log n
=
log(1/r)
− log r

Si tomamos esta fórmula como definición del valor de la dimensión de cualquier figura que pueda ser descompuesta en copias a escala de sı́ misma, obtenemos una manera cómoda de asignar una dimensión a algunos conjuntos
fractales clásicos. El conjunto de Cantor E, por ejemplo, puede descomponerse en dos copias de sı́ mismo a escala 1/3 o en cuatro copias de sı́ mismo
a escala 1/9 lo que nos da
dim(E) =

log 2
log 4
=
= 0,63092975 . . .
log 3
log 9

Este método es de muy fácil aplicación, pero exige que el conjunto a
medir pueda ser descompuesto en copias de sı́ mismo a escala, lo cual es una
propiedad muy especı́fica de ciertos conjuntos. Sin embargo, la idea puede
aprovecharse para calcular la dimensión de cualquier tipo de conjunto.

A.5.

Medida de Haussdorf

Dado un conjunto E ⊂ Rn y un número real positivo δ, diremos que una
familia {Ai }, i = 1, 2, . . ., de subconjuntos de Rn es un recubrimiento-δ de

A.6. DIMENSIÓN DE HAUSDORFF

139

E, si la unión de tales conjuntos contiene a E, es decir,
E=

∞
[

Ai

i=1

y el diámetro de cada uno de los miembros del recubrimiento es menor o
igual que δ
|Ai | ≤ δ ∀ i = 1, 2, . . .
Dado un conjunto E ⊂ Rn y un número real s > 0, definimos
Hδs (E)

= ı́nf

(∞
X

)
s

|Ai | : {Ai } es recubrimiento-δ de E

i=1

número1 que mide el tamaño-s del conjunto E, es decir, ignorando las irregularidades de E que tienen tamaño menor que δ.
Si hacemos tender δ a cero, iremos apreciando irregularidades de tamaño
cada vez menor. Además, si δ → 0, Hδs (E) aumenta, pues es un ı́nfimo
tomado cada vez sobre una clase más restringida de recubrimientos y, por
tanto, existe el lı́mite
H s (E) = lı́m Hδs (E)
δ→0

que puede ser finito o infinito. Al número H s (E) se le conoce como medida
s-dimensional de Hausdorff.
Puede demostrarse que la definición es equivalente si se supone que los
recubrimientos están formados por abiertos, por cerrados o por convexos,
debido a que tales restricciones no alteran las sumas de los diámetros de los
conjuntos del mismo.
Puede también demostrarse que H s (E) tiene las propiedades que se exigen para ser una medida y que, en general, si s = n es entero y E es
medible Lebesgue, entonces H n (E) es proporcional a la medida de Lebesgue
n-dimensional Ln (E).

A.6.

Dimensión de Hausdorff

La medida s–dimensional de Hausdorff como función de s tiene un comportamiento especial. Su rango está formado por uno, dos o tres valores.
Estos posibles valores son cero, un número finito e infinito.
1
Para una definición precisa de los conceptos de ı́nfimo y supremo puede consultarse
la página 38.

140

APÉNDICE A. MEDIDA DE CONJUNTOS

Teorema A.1 Sea n un número entero positivo y sea E un subconjunto
acotado de Rn . Sea H s (E) la medida s–dimensional de Hausdorff tal como
se definió más arriba con 0 ≤ s < ∞. Entonces existe un único número real
0 ≤ DH ≤ n tal que
(
s

H (E) =

∞
0

si s < DH
si s > DH

El único número real DH que cumple el teorema anterior se conoce como
la dimensión de Hausdorff del conjunto E y se escribe también como DH (E).
Evidentemente, los conjuntos clásicos conservan su dimensión clásica bajo
DH , pero los conjuntos fractales están ahora mucho mejor caracterizados y
podemos intentar dar una definición precisa de ellos.
Es en exceso simplista afirmar que un fractal es aquel conjunto con dimensión fraccionaria. Algunos fractales tienen dimensión entera, por lo que
la afirmación anterior es incluso errónea. Un fractal serı́a cualquier conjunto
cuya dimensión de Hausdorff sea mayor estrictamente que su dimensión topológica. Con ello incluso la curva de Hilbert se considerarı́a fractal, ya que
su dimensión de Hausdorff es 2 y su dimensión topológica es 1.

A.7.

Dimensión fractal

Aunque el concepto de dimensión de Hausdorff es esencial en la geometrı́a
fractal, su definición es relativamente compleja y en la práctica se utilizan
otras definiciones de dimensión que resultan, además, de gran valor a la
hora de determinar empı́ricamente la dimensión de series de datos obtenidas
del mundo real, y que suelen coincidir con la dimensión de Hausdorff en
los casos más interesantes. La más extendida dentro de esta categorı́a es
la denominada dimensión fractal. En este apartado restringiremos nuestro
estudio a la dimensión de conjuntos compactos.
Definición A.1 Sea A ∈ H(Rn ) un conjunto compacto y no vacı́o de Rn .
Sea N (A, ²) el menor número de bolas cerradas de diámetro ² > 0 necesarias
para cubrir A. Si existe
½

D = lı́m

²→0

log N (A, ²)
log(1/²)

¾

entonces D se denomina dimensión fractal de A. Se escribirá también D =
D(A) para indicar que A tiene dimensión fractal D.

A.7. DIMENSIÓN FRACTAL

141

Ejemplo Consideremos la curva de Koch discutida en la página 8.
Se necesita una bola de diametro 1 para cubrir todo el conjunto, 4
bolas de diámetro 1/3, 16 bolas de diámetro 1/9, 64 bolas de diámetro
1/27... En general, son necesarias 4n bolas de diámetro (1/3)n para
cubrir la curva de Koch. Su dimensión fractal será, por tanto,
D = lı́m

n→∞

log 4n
log 4
=
= 1,261859507 . . .
log 3n
log 3

lo que indica que la curva de Koch está más cerca de ser una curva que
un área (nótese que para que el diámetro tienda a cero, n ha de tender
a infinito).

Ya hemos dicho que en muchas ocasiones la dimensión fractal y la dimensión de Hausdorff coinciden. Más concretamente puede demostrarse el
siguiente teorema.
Teorema A.2 Sea n un entero positivo y sea A un subconjunto de Rn . Si
D(A) denota la dimensión fractal de A y DH (A) la dimensión de Hausdorff
de A, entonces
0 ≤ DH (A) ≤ D(A) ≤ n
El siguiente teorema simplifica aún más el cálculo de la dimensión fractal
al permitir la sustitución de la variable continua ² por una variable discreta.
Teorema A.3 (Teorema de recuento por cajas) Sea A ∈ H(Rn ) un
conjunto compacto y no vacı́o de Rn . Cubramos Rn mediante cajas cuadradas cerradas con lados de longitud (1/2)m . Sea Nm (A) el número de cajas
de lado de longitud (1/2)m que intersectan con A. Si
½

D = lı́m

m→∞

log Nm (A)
log 2m

¾

entonces A tiene dimensión fractal D.
La aplicación del teorema es tan sencilla como situar una malla sobre el
conjunto a medir y contar el número de cajas en las que hay algún punto
del conjunto. Calculemos mediante este método la dimensión fractal del
triángulo de Sierpinski.
Ejemplo Consideremos el triángulo de Sierpinski S de la figura A.1.
Puede verse como N1 (S) = 3, N2 (S) = 9, N3 (S) = 27 y, en general,
Nn (S) = 3n para n = 1, 2, . . .

142

APÉNDICE A. MEDIDA DE CONJUNTOS

Figura A.1: Se necesitan 3n cajas cerradas de lado (1/2)n para cubrir el triángulo
de Sierpinski. Se deduce por tanto que su dimensión fractal es log 3/ log 2. Figura
tomada de [BAR 93a].

El teorema A.3 implica que
½
¾
log Nn (A)
D(S) = lı́m
n→∞
log 2n
¾
½
log 3
log 3n
= lı́m
=
= 1,584962501 . . .
n
n→∞
log 2
log 2

Aunque es posible utilizar los métodos analı́ticos anteriores para el cálculo de la dimensión sobre cualquier tipo de fractal, la dificultad de hacerlo
puede ser casi insuperable. Aunque la curva de Koch o el triángulo de Sierpinski se domestican fácilmente, conjuntos como los de Julia o el de Mandelbrot, vistos en el capı́tulo 1, son otra historia. Si existen técnicas analı́ticas
para calcular su dimensión, todavı́a no han sido descubiertas. No obstante,
es posible utilizar técnicas experimentales. La más sencilla de tales técnicas
consiste en registrar varios valores de Nm (A) y representar los resultados
en un gráfico con log Nm (A) en el eje vertical y log 2m en el eje horizontal. La pendiente de la curva de regresión que mejor se adapte a los puntos
representados será una aproximación de la dimensión fractal del objeto.
Las definiciones dadas en este apéndice para la dimensión no cubren la
multitud de distintas definiciones existentes hoy dı́a. Aun ası́ pueden considerarse un buen punto de partida para estudios más profundos.

A.7. DIMENSIÓN FRACTAL

143

Ante el asombro de Menón por los conocimientos demostrados por su
siervo, Sócrates le explica cómo “éste se ha de comportar de la misma manera
con cualquier geometrı́a y con todas las demás disciplinas”. Quizá de haber
escrito hoy dı́a sus Diálogos, Platón habrı́a hecho que el esclavo razonara a
continuación la evolución a diferentes escalas de algún conjunto insigne de
la geometrı́a fractal.

Apéndice B

La teorı́a de los wavelets
Aunque la transformada de Fourier es probablemente la transformada
más utilizada, no es la única. Existen multitud de transformadas distintas,
cada una con sus ventajas e inconvenientes, usadas en ingenierı́a y matemáticas: la transformada de Hilbert, la transformada de Fourier a corto plazo, las
distribuciones de Wigner o la transformada con wavelets, por poner algunos
ejemplos.
Debido a su utilidad para la compresión de señales, nos centraremos
aquı́ en la transformada con wavelets, principalmente siguiendo a [POL 97].
Aunque se mostrarán algunos de los conceptos más importantes, es necesario
que el lector conozca los fundamentos de la transformada de Fourier y de la
representación de señales en el dominio de la frecuencia.1

B.1.

Limitaciones de la transformada de Fourier

La transformada de Fourier (TF) descompone una señal en funciones
exponenciales complejas de diferentes frecuencias:
X(f ) =
x(t) =

Z ∞

−∞
Z ∞
−∞

x(t)e−2πjf t dt
X(f )e−2πjf t df

(transformada de Fourier)
(transformada inversa de Fourier)

donde t es el tiempo, f la frecuencia, x denota la señal en el dominio del
tiempo y X es la señal en el dominio de la frecuencia.
1

Existen multitud de referencias sobre la transformada de Fourier por lo que cualquier
trabajo sobre tratamiento de señales deberı́a servir. Un buen libro, centrado en las señales
discretas, es Discrete-Time Signal Processing de Alan V. Oppenheim y Ronald W. Schafer,
Prentice–Hall, 1989.

145

146

APÉNDICE B. LA TEORÍA DE LOS WAVELETS
3

x(t) = cos 4πt + cos 16πt + cos 32πt

2
1

x(t) 0
-1
-2
-3

0

0.1

0.2

0.3

0.4

0.5

t
Figura B.1: La señal cos 4πt + cos 16πt + cos 32πt es estacionaria en el sentido de
que sus componentes frecuenciales se distribuyen a lo largo de toda la señal.

Según la ecuación de Euler
ejα = cos α + j · sen α
por lo que la ecuación de la transformada de Fourier multiplica la señal original por una expresión compleja formada por senos y cosenos de frecuencia f
y posteriormente integra este producto. Si el resultado de esta integración
para un cierto valor de f es grande, diremos que la señal x(t) tiene una
componente espectral dominante en la frecuencia f .
A pesar de su enorme utilidad, la TF presenta ciertas limitaciones cuando
se aplica a señales no estacionarias.2 Supongamos, por ejemplo, que tenemos
dos señales diferentes, ambas con las mismas componentes espectrales, pero
con una diferencia: una de las señales tiene tres componentes frecuenciales
en todo el tiempo (puede tratarse, por ejemplo, de la señal mostrada en la
figura B.1) y la otra tiene las mismas tres componentes frecuenciales, pero
en tiempos distintos (tal como la señal mostrada en la figura B.2). Aunque
las señales son completamente diferentes, su TF es la misma.
Cuando se necesita la localización en el tiempo de las componentes espectrales por estar tratando una señal no estacionaria, es necesaria una
transformada que nos dé la representación tiempo-frecuencia de la señal.
Una de tales transformadas es la transformada de Fourier a corto plazo,
2

En las señales estacionarias todas las componentes de frecuencia existentes en la señal
aparecen a lo largo de toda su duración; en las señales no estacionarias, por otra parte, la
frecuencia cambia constantemente a lo largo del tiempo.

B.2. LA TRANSFORMADA DE FOURIER A CORTO PLAZO

147

1

x(t) 0

-1

0

0.5

1

1.5

2

2.5

3

t
Figura B.2: La señal mostrada no es estacionaria debido a que se trata de la
concatenación de tres ondas finitas de distintas frecuencias que no se solapan en el
tiempo. En este caso la señal está definida como cos 32πt en el intervalo [0,1], como
cos 16πt en [1,2] y como cos 4πt en el intervalo [2,3].

que nos proporciona una representación de la señal en tiempo y frecuencia
simultáneamente. La transformada con wavelets se desarrolló para superar
algunos problemas de resolución de la transformada de Fourier a corto plazo.

B.2.

La transformada de Fourier a corto plazo

Para superar el problema de la TF con señales no estacionarias, podemos
considerar que una señal no estacionaria es estacionaria localmente, es decir,
en pequeños intervalos de tiempo. Este enfoque llevó a la formulación de la
denominada transformada de Fourier a corto plazo (TFCP). En ella la señal
se divide en segmentos lo suficientemente pequeños como para que pueda
asumirse un comportamiento estacionario en ellos. Con este fin se elige una
ventana w(t) de ancho finito que se va deslizando sobre la señal:
Z

T F CPxw (t0 , f )

=

t

[x(t) · w∗ (t − t0 )] · e−2πjf t dt

El término w∗ (t − t0 ) representa el conjugado de la función ventana desplazada t0 unidades de tiempo a la derecha. Como puede verse, la TFCP
no es más que la TF de la señal previamente multiplicada por una función
ventana. Para cada t0 y f se obtiene un coeficiente.
Por lo tanto, estamos obteniendo una representación real de la señal en

148

APÉNDICE B. LA TEORÍA DE LOS WAVELETS

tiempo y frecuencia. La TFCP es bidimensional (en un eje aparece la frecuencia y en el otro el tiempo) o tridimensional, si consideramos la amplitud.
Sin embargo, hay un problema nada despreciable en la formulación anterior. Un problema que tiene sus raı́ces en el principio de incertidumbre de
Heisenberg.
El principio de incertidumbre, formulado originalmente por Heisenberg,
afirma que no pueden conocerse simultáneamente el momento y la posición
de una partı́cula en movimiento. Aplicándolo a nuestro estudio, no podemos
conocer qué componentes espectrales existen en un determinado instante de
tiempo. Lo más que podemos hacer es investigar qué componentes espectrales existen en un cierto intervalo de tiempo, lo cual es un problema de
resolución.3
En la TF no existe problema de resolución en el dominio de la frecuencia,
es decir, sabemos exactamente qué frecuencias existen en la señal; de manera
similar, no existe un problema de resolución en el dominio del tiempo, ya que
conocemos el valor de la señal en cada instante de tiempo. Sin embargo, la
resolución temporal en la TF y la resolución en frecuencias en el dominio del
tiempo son nulas, ya que no tenemos información sobre ellas. Lo que da la
resolución perfecta en frecuencias es el hecho de que la ventana utilizada en
la TF, la función e−2πjf t , tiene duración infinita. En la TFCP la ventana es
de longitud finita, por lo que perdemos la resolución en frecuencias perfecta
y obtenemos una resolución en frecuencias más pobre a costa de una mejor
resolución temporal.
Nuestro dilema puede esquematizarse como: ventana estrecha =⇒ buena
resolución temporal y resolución en frecuencias pobre; ventana ancha =⇒
buena resolución en frecuencias y resolución temporal pobre. La figura B.3
muestra la TFCP de una señal, similar a la de la figura B.2 pero con cuatro frecuencias distintas en lugar de tres, en la que se utilizó una ventana
estrecha.4 Puede verse cómo en el dominio de la frecuencia cada pico cubre
un rango de frecuencias y no un único valor. En el dominio del tiempo, sin
embargo, los cuatro picos están bien separados unos de otros.
3

El principio de incertidumbre afirma que no es posible reducir arbitrariamente a la
vez la resolución en el tiempo (∆x) y la resolución en frecuencia (∆ω) debido a que su
producto está acotado inferiormente por la desigualdad de Heisenberg
∆x ∆ω ≥

1
2

Esta desigualdad implica que debemos sacrificar la resolución en el tiempo por la resolución
en frecuencias o viceversa.
4
Por motivos que ahora no entraremos a analizar, la TFCP es siempre simétrica. Puede,
por tanto, descartarse la mitad de la transformada sin que se pierda ninguna información.

B.3. ANÁLISIS MULTIRRESOLUCIÓN

149

Figura B.3: La transformada de Fourier a corto plazo de una señal realizada con
una ventana estrecha proporciona una muy buena resolución temporal, pero una
resolución en frecuencias relativamente pobre. Figura tomada de [POL 97].

La transformada con wavelets resuelve hasta cierto punto el dilema. La
TFCP da una resolución fija durante todo el tiempo, mientras que la transformada con wavelets da una resolución variable basándose en que las frecuencias altas se resuelven mejor en el tiempo y las frecuencias bajas se
resuelven mejor en la frecuencia.

B.3.

Análisis multirresolución

Aunque los problemas de resolución en tiempo y frecuencia son el resultado de un fenómeno fı́sico (el principio de incertidumbre de Heisenberg) y
existen independientemente de la transformada utilizada, es posible analizar
una señal mediante un enfoque alternativo denominado análisis multirresolución. El análisis multirresolución analiza la señal con diferentes resoluciones en diferentes frecuencias. A diferencia de la TFCP, cada componente
espectral no se trata de la misma forma.
El análisis multirresolución está diseñado para dar una buena resolución
temporal y una resolución en frecuencias pobre para las frecuencias altas y
una buena resolución en frecuencias junto a una resolución temporal pobre
para las frecuencias bajas. Este enfoque tiene especial sentido cuando la

150

APÉNDICE B. LA TEORÍA DE LOS WAVELETS

señal a analizar tiene componentes frecuenciales altas durante breves periodos de tiempo y componentes frecuenciales bajas durante periodos largos.
Afortunadamente, en la práctica, casi todas las señales son de este tipo.
La transformada con wavelets basa su funcionamiento en el análisis multirresolución.

B.4.

La transformada continua con wavelets

La transformada continua con wavelets fue desarrollada a comienzos de
los años ochenta como alternativa a la TFCP para superar el problema de
la resolución. Aunque guarda un cierto parecido con la TFCP, en el caso de
la transformada continua con wavelets el ancho de la ventana va cambiando
conforme se calcula la transformada para cada componente espectral. La
transformada continua con wavelets (TCW) se define como
T CWxψ (τ, s)

=

Ψψ
x (τ, s)

1
=p
|s|

µ

Z

x(t)ψ

∗

t−τ
s

¶

dt

donde τ es el parámetro de traslación, s es el parámetro de escala y ψ(t) es
la función de transformación conocida como onda madre o wavelet madre.
El término wavelet 5 significa en inglés onda pequeña. Lo de onda se
refiere al hecho de que esta función es oscilatoria. El término madre se refiere
a que las ventanas con distintos anchos que se utilizan en la transformada
se derivan de una única función. En otras palabras, la onda madre es un
prototipo para generar el resto de ventanas.
La traslación τ está relacionada, como en la TFCP, con el desplazamiento
de la ventana por la señal. Aquı́, sin embargo, no hay un parámetro para
la frecuencia. En su lugar se utiliza el parámetro de escala s que se define
como la inversa de la frecuencia.
El parámetro de escala es similar a la escala utilizada en los mapas. Al
igual que en los mapas, una escala alta corresponde a una vista global de la
señal sin mucho detalle y una escala baja corresponde a una vista detallada.
De igual forma, en términos de frecuencia, las frecuencias bajas (escalas
altas) corresponden a una información global de la señal, mientras que las
frecuencias altas (escalas bajas) corresponden a una información detallada
de la señal que dura normalmente un breve instante de tiempo.
5

Este es el único término no traducido al castellano en esta obra, lo cual se debe a la
dificultad de encontrar una traducción agradable y al poco tiempo de vida de esta transformada, que impide la existencia de un equivalente ampliamente aceptado en castellano.

B.4. LA TRANSFORMADA CONTINUA CON WAVELETS

151

Figura B.4: Señal no estacionaria utilizada para mostrar el aspecto de la transformada continua con wavelets. La frecuencia de la señal va decreciendo conforme
aumenta el tiempo. Su transformada se muestra en la figura B.5. Figura tomada de
[POL 97].

Matemáticamente hablando, el escalado comprime o dilata una señal.
Las escalas altas correspoden a señales dilatadas o expandidas y las escalas
bajas a señales comprimidas. Por tanto, todas las ventanas utilizadas en
la TCW son versiones desplazadas y dilatadas (o comprimidas) de la onda
madre.
p

La multiplicación por 1/ |s| en la expresión anterior se utiliza para
normalizar las energı́as en cada escala. La transformada con wavelets proporciona, en definitiva, un punto del plano escala-desplazamiento para cada
escala y para cada instante de tiempo.
Veamos ahora un ejemplo del aspecto de una transformada con wavelets.
La figura B.5 es la TCW de la señal no estacionaria de la figura B.4. Recordemos que la escala debe interpretarse como la inversa de la frecuencia
por lo que las zonas de la gráfica de la figura B.5 con escala cercana a cero
corresponden realmente a las frecuencias más altas. En cualquier caso, los
valores de los ejes no deben tenerse en cuenta, ya que están normalizados,
pero sı́ puede observarse cómo se refleja que las frecuencias más altas de la
señal (bajas escalas) aparecen primero y cómo, conforme nos desplazamos
por el eje del desplazamiento, la señal presenta frecuencias cada vez más
bajas (escalas altas), lo cual está acorde con la representación de la señal de

152

APÉNDICE B. LA TEORÍA DE LOS WAVELETS

Figura B.5: TCW de la señal de la figura B.4. La transformada refleja la presencia
de las frecuencias a lo largo de la señal. Conforme se aumenta el tiempo (desplazamiento), aparecen más componentes en las escalas altas, es decir, en las frecuencias
bajas. Figura tomada de [POL 97].

la figura B.4.
Una de las funciones más utilizadas para la onda madre es la función de
sombrero mejicano que se define como la segunda derivada de la gaussiana
−t2
1
w(t) = √
e 2σ2
2π σ

que es
1
ψ(t) = √
2π σ 3

Ã

e

−t2
2σ 2

Ã

!!

t2
−1
σ2

y que aparece representada en la figura B.6.

Resolución en tiempo y frecuencia
Para ver cómo puede interpretarse la resolución en tiempo y frecuencia,
podemos observar la figura B.7. Cada caja de la figura corresponde a un
valor de la transformada con wavelets en el plano tiempo-frecuencia. Nótese
que las cajas tienen área no nula, lo que implica que no puede conocerse
el valor de un punto particular en el plano tiempo-frecuencia. Todos los

B.4. LA TRANSFORMADA CONTINUA CON WAVELETS

153

0.04

σ = −2,2

0.03
0.02

ψ(t) 0.01
0
-0.01
-0.02
-10

-8

-6

-4

-2

0

t

2

4

6

8

10

Frecuencia

Figura B.6: Una de las ondas madre utilizada con mayor frecuencia en el cálculo
de la TCW es la función de sombrero mejicano.

Tiempo

Figura B.7: Este diagrama es habitual a la hora de analizar la forma de la resolución en tiempo y frecuencia proporcionada por la transformada con wavelets.
El principio de incertidumbre obliga a que el área de todas las cajas sea la misma,
pero la transformada con wavelets varı́a sus dimensiones para atender de distinta
manera a las distintas frecuencias de la señal.

puntos de este plano que caen en una caja se representan por un valor de la
transformada con wavelets.
Aunque el alto y ancho de las cajas cambia, el área es constante. Es decir,
cada caja representa una parte igual del plano tiempo-frecuencia, pero con
diferentes proporciones de tiempo y frecuencia. Esta área no puede reducirse debido al principio de incertidumbre de Heisenberg. Véase cómo para
frecuencias bajas las cajas son menos altas (lo que corresponde a mejores re-

154

APÉNDICE B. LA TEORÍA DE LOS WAVELETS

soluciones en frecuencias, ya que hay menos ambigüedad para determinar el
valor exacto de la frecuencia), pero tienen una anchura mayor (lo que corresponde a una resolución temporal más pobre, ya que hay mayor ambigüedad
para determinar el valor del tiempo exacto). Para frecuencias mayores decrece el ancho de las cajas, es decir, mejora la resolución temporal, y aumenta
su altura, con lo que se empobrece la resolución en frecuencias.

La transformada inversa
La TCW es una transformada reversible siempre que se cumpla la ecuación B.1 dada más abajo. Afortunadamente, éste no es un requerimiento
muy restrictivo. La reconstrucción es posible mediante
1
x(t) = 2
cψ

Z Z
s τ

1
Ψψ
x (τ, s) 2 ψ
s

µ

t−τ
s

¶

dτ ds

donde cψ es una constante que depende de la onda madre utilizada. El éxito
en la reconstrucción depende de que esta constante, llamada constante de
admisibilidad , satisfaga la siguiente condición de admisibilidad:
v
u
¯
¯2
u Z
¯
¯
u
∞ ¯ψ̂(ξ)¯
t
dξ
cψ = 2π
−∞

|ξ|

<∞

(B.1)

donde ψ̂(ξ) es la tranformada de Fourier de ψ(t). La ecuación B.1 implica
que ψ̂(0) = 0 que, por una propiedad de la TF, es equivalente a
Z

ψ(t)dt = 0
Esta última ecuación no es muy restrictiva, ya que hay muchas funciones
que se pueden utilizar como onda madre cuya integral es cero. Para ello la
función debe ser oscilatoria. La función sombrero mejicano de la página 152
es una de las muchas que cumple la condición de admisibilidad.

B.5.

La transformada discreta con wavelets

La idea de la transformada discreta con wavelets (TDW) es la misma
que en la transformada continua. Utilizando filtros digitales se obtiene una
representación tiempo-escala (frecuencia) de una señal digital. La TCW se
calculaba variando la escala de la ventana de análisis, desplazando la ventana
en el tiempo, multiplicando por la señal e integrando sobre todo el tiempo.

B.5. LA TRANSFORMADA DISCRETA CON WAVELETS

155

En el caso discreto se utilizan filtros de diferentes frecuencias de corte para
analizar la señal a diferentes escalas. La señal se pasa por una serie de
filtros de paso alto para analizar las frecuencias altas y por una serie de
filtros de paso bajo para analizar las frecuencias bajas. La resolución de la
señal se cambia mediante operaciones de filtrado y la escala se cambia con
operaciones de submuestreo y supermuestro.
Submuestrear una señal es reducir la tasa de muestreo, eliminando algunas muestras de la señal. Por ejemplo, para submuestrear por dos una señal
puede eliminarse una de cada dos muestras consecutivas. El submuestreo
por un factor n reduce n veces el número de muestras de la señal.
Supermuestrear una señal es incrementar la tasa de muestreo de la señal,
añadiéndole nuevas muestras. Por ejemplo, para supermuestrear por dos una
señal puede añadirse entre cada dos muestras una nueva, normalmente un
cero o un valor interpolado. El supermuestreo por un factor n incrementa el
número de muestras de la señal por un factor n.
En lo siguiente representaremos una secuencia por x(n) donde n es un
entero. Además, debe tenerse en cuenta que en las señales discretas la frecuencia se expresa en términos de radianes y que el ancho de banda de
cualquier secuencia es de π rad/s.
Veamos ahora cómo actúa la TDW. En primer lugar la señal original
x(n) se pasa por un filtro de media banda de paso alto g(n) y por uno de
paso bajo h(n). El filtrado de una secuencia por un filtro digital de respuesta
impulsiva h(n) se expresa matemáticamente como la convolución
y(n) = x(n) ∗ h(n)
=

X

x(k)h(n − k)

k

=

X

h(k)x(n − k)

k

donde y(n) es la secuencia a la salida del filtro.
Tras el filtrado pueden eliminarse la mitad de las muestras según el principio de Nyquist, ya que la máxima frecuencia de la señal (frecuencia de
corte) es ahora de π/2 rad/s y no de π rad/s. Puede, por tanto, submuestrearse la señal por dos tomando una muestra sı́ y otra no.
La descomposición anterior reduce a la mitad la resolución temporal, ya
que la señal completa está caracterizada ahora por la mitad de las muestras.
Sin embargo, la resolución en frecuencias se ha duplicado, ya que el ancho
de banda de la señal es la mitad del ancho de banda anterior.
Este procedimiento, también conocido como codificación subbanda, se

156

APÉNDICE B. LA TEORÍA DE LOS WAVELETS
f = [0, π]

x (n)

g(n)

h(n)

f = [π/2, π]

f = [0, π/2]
2

2

Coeficientes
nivel 1
g(n)

h(n)

f = [π/4, π/2]

f = [0, π/4]
2

2

Coeficientes
nivel 2
g(n)

h(n)

Figura B.8: El algoritmo de codificación subbanda puede verse como la aplicación
de una cascada de filtros de paso de media banda seguidos de submuestreadores
por dos. El ancho espectral de la señal en cada nivel se representa mediante f .

repite sucesivamente sobre la salida del filtro de paso bajo hasta obtener
una secuencia de longitud dos. Los primeros pasos del procedimiento se
muestran en la figura B.8.
Finalmente, la TDW de la señal original se obtiene concatenando todos
los coeficientes desde el primer nivel de descomposición. La TDW tendrá el
mismo número de coeficientes que la señal original.
Las frecuencias más destacadas de la señal original aparecerán con amplitudes grandes en la región de la transformada que incluya esas frecuencias en
particular. La diferencia entre esta transformada y la transformada discreta
de Fourier es que no hemos perdido la localización temporal de esas frecuencias. Sin embargo, la resolución de esta localización temporal dependerá del
nivel en que aparezca la frecuencia. Si la información principal de la señal
aparece en las frecuencias altas, como ocurre muy a menudo, la localización
temporal de estas frecuencias será más precisa, ya que vienen caracterizadas
por un mayor número de muestras. Si la información principal de la señal

B.5. LA TRANSFORMADA DISCRETA CON WAVELETS

157

aparece sólo en las frecuencias bajas, su localización temporal no será muy
precisa, ya que se utilizan pocas muestras para representar la señal en esas
frecuencias. Este procedimiento ofrece, en definitiva, una buena resolución
temporal en las frecuencias altas y una buena resolución en frecuencias para
frecuencias bajas. Como ya hemos dicho, este comportamiento es adecuado
para tratar la mayor parte de las señales encontradas en la práctica.
Las bandas de frecuencias que no sean muy prominentes en la señal
original tendrán amplitudes muy bajas y podremos prescindir de esa parte
de la TDW casi sin perder información. Esta es la idea base en todos los
sistemas de compresión basado en wavelets como el discutido en el capı́tulo 5.
Nótese que, debido a los sucesivos submuestreos por dos, la longitud de
la señal debe ser potencia de dos o, al menos, un múltiplo de una potencia
de dos, para que este esquema sea eficiente.

Filtros utilizados en la TDW
Una propiedad importante de la TDW es la relación entre las respuestas
impulsivas de los filtros de paso alto y paso bajo. Ambos están relacionados
por
g(L − 1 − n) = (−1)n h(n)

(B.2)

donde g(n) es el filtro de paso alto, h(n) el filtro de paso bajo y L es la longitud del filtro. Las operaciones de filtrado y submuestreo de la transformada
pueden expresarse como
yalta (n) =

X

x(k) · g(−k + 2n)

(B.3)

x(k) · h(−k + 2n)

(B.4)

k

ybaja (n) =

X
k

Si los filtros utilizados cumplen la condición B.2, la transformada inversa
es muy sencilla y se limita a reconstruir la señal original siguiendo el procedimiento explicado anteriormente en orden inverso. Las señales de cada
nivel se supermuestrean por dos, se pasan por los filtros de análisis g̃(n) y
h̃(n) (de paso alto y paso bajo, respectivamente) y, finalmente, se suman.
Los filtros de análisis y los de sı́ntesis son idénticos entre sı́, excepto por
estar invertidos uno respecto al otro:
g̃(n) = g(−n)
h̃(n) = h(−n)

158

APÉNDICE B. LA TEORÍA DE LOS WAVELETS
Por lo tanto, la reconstrucción de la señal es para cada nivel
x(n) =

X

[yalta (k) · g(−n + 2k) + ybaja (k) · h(−n + 2k)]

(B.5)

k

Las ecuaciones B.3, B.4 y B.5 son la piedra angular de todo codificador/descodificador basado en wavelets.
El desarrollo anterior se llevó a cabo suponiendo que los filtros eran ideales. Aun ası́, aunque no es posible diseñar filtros ideales, sı́ pueden encontrarse filtros que permitan una reconstrucción perfecta bajo ciertas condiciones.
Unos de los más utilizados son los filtros de Daubechies, cuyos coeficientes
h(n) son
h0 =
0,332670552950
h1 =
0,806891509311
h2 =
0,459877502118
h3 = −0,135011020010
h4 = −0,085441273882
h5 =
0,035226291882
Los coeficientes de los otros filtros pueden obtenerse a partir de éstos mediante las expresiones dadas anteriormente.
Ya se ha comentado la utilidad de la TDW para la compresión de señales.
Su aplicación detallada a la compresión de imágenes se discute en la sección 5.6.

Apéndice C

Imágenes originales
Este apéndice muestra el original de las imágenes utilizadas a lo largo de
la obra. Para cada una se indica brevemente los aspectos esenciales que una
buena codificación deberı́a preservar. Evidentemente, con independencia de
la técnica de compresión utilizada, estos detalles se irán perdiendo conforme
aumente el ratio de compresión.

Figura C.1: Aparecen numerosas zonas de baja resolución y escaso contraste,
especialmente en la zona del follaje. Las vallas del puente generan un gradiente alto
en sus bordes. El tamaño de la imagen es 256 × 256.

159

160

APÉNDICE C. IMÁGENES ORIGINALES

Figura C.2: El cuerpo del pájaro muestra una textura agradable rodeada por
cortes abruptos en las fronteras entre las plumas blancas y negras y entre el cuerpo y
el fondo. Las garras, el pico y los ojos son estructuras cruciales para una codificación
detallada. El fondo aumenta su luminosidad gradualmente desde la esquina inferior
derecha a la superior izquierda, pero sin ser completamente liso. Tamaño 256 ×
256.

161

Figura C.3: Similar a la figura C.1. Destaca la presencia de texturas de escala
media ası́ como la ausencia de grandes áreas uniformes. La variedad cromática de
las fachadas de las casas debe capturarse adecuadamente. Por otra parte, la persona
que sube por la cuesta podrı́a fácilmente confundirse con el fondo. Tamaño 256 ×
256.

Figura C.4: Similar a la figura C.2. Se trata de una imagen con numerosas transiciones suaves, aunque es importante capturar correctamente los cambios en el color
del plumaje y del pico. Tamaño 372 × 279.

162

APÉNDICE C. IMÁGENES ORIGINALES

Figura C.5: Presenta numerosos detalles a baja escala, como los mástiles o las
texturas de los barcos, sobre un cielo suave. Las letras que configuran el nombre
del barco deberı́an ser atrapadas adecuadamente. Tamaño 512 × 512.

163

Figura C.6: Una de las imágenes más sencillas de codificar fractalmente debido a
la similitud entre todas las regiones de la imagen. Tamaño 256 × 256.

Figura C.7: Dos planos bien diferenciados. En primer plano las flores con escaso
contraste. Detrás, las hojas presentan detalles a menor escala. Tamaño 460 × 345.

Bibliografı́a
[ALI 91]

M. Ali, T. G. Clarkson, “Fractal image compression”. Proc.
1st Seminar on Information Technology and its Applications
(ITA’91), Markfield Conf. Centre, Leicester, U.K., 29 Sept.,
1991. También disponible vı́a [FRE 97].
Un artı́culo de los primeros tiempos de la compresión fractal de imágenes en el que se muestran algunas técnicas (sin experimentos que las
evalúen) para intentar resolver el problema inverso, esto es, obtener
automáticamente el SFI que genera una imagen cualquiera. El texto propone expresar las transformaciones que conforman un SFI en
forma compleja de manera que el problema se traslade al dominio de
los momentos complejos. Se sugiere, entonces, el método de recocido
simulado (simulated annealing) para encontrar el SFI más cercano a
una imagen o a un segmento de ella.

[BAR 93a] M. Barnsley, Fractals Everywhere, Second Edition, Academic
Press, 1993.
La enciclopedia de los sistemas de funciones iteradas y obra fundamental para el desarrollo de la compresión fractal. Aunque no se enfrenta
decididamente al tema de la codificación de imágenes (aspecto éste
presentado en posteriores obras del autor), expone exhaustivamente
la teorı́a y las implicaciones prácticas de los SFI desde la topologı́a
de los espacios métricos, el teorema del collage y la dimensión fractal
hasta los conjuntos de Julia o de Mandelbrot (desde la perspectiva de
los SFI) y los SFI recurrentes.

[BAR 93b] J. Barrallo Calonge, Geometrı́a fractal: algorı́tmica y representación, Anaya Multimedia, 1993.
El principal mérito de este libro es la enorme cantidad de programas
en C que incluye. Casi sin matemáticas logra introducir prácticamente
todas las facetas de estudio actuales sobre los fractales. El capı́tulo 13

165

166

BIBLIOGRAFÍA
discute los sistemas de funciones iteradas y el capı́tulo 14 la generación
de curvas fractales mediante gramáticas.

[BLA 94]

J. Blanc–Talon, “Inference of grammars from fractal sets: the
inner structure”. En Grammatical Inference, L. Erlbaum Assoc.
Publisher, 1994, Simon Lucas Ed.
Los sistemas D0L son uno de los mecanismos más sencillos para generar estructuras fractales además de poseer propiedades topológicas
bien conocidas. El artı́culo repasa la situación del momento de la generación sintáctica de fractales, principalmente a través del estudio
de los sistemas D0L y de las gramáticas independientes del contexto.
A continuación presenta un método para el problema inverso, es decir, la inferencia de sistemas D0L a partir de conjuntos de datos con
distribución fractal y su generalización a la inferencia de gramáticas
independientes del contexto.

[CIE 97]

L. Ciepliński, C. Jȩdrzejek, T. Major, “Acceleration of a fractal
image compression by fast nearest-neighbor search”. Fractals,
vol. 5, supplementary issue, 1997.
Sigue la idea de [SAU 95] de acelerar la compresión mediante la
búsqueda multidimensional del vecino más cercano en un espacio de
proyecciones. Los autores proponen y estudian empı́ricamente el algoritmo de eliminación parcial de la distorsión para la búsqueda. También se sugieren algunas mejoras adicionales como buscar sólo en la
mitad del espacio de proyecciones o postprocesar la imagen mediante
el suavizado de los bordes de las regiones destino.

[COL 96]

J. Colvin, “Iterated function systems and fractal image compression”. Disponible en http://hamnetcenter.com/jeffc/
fractal.html.1
Una introducción muy sencilla a los SFI y a la codificación fractal de
imágenes mediante SFI particionados. Un breve tutorial recomendado
para una primera aproximación al tema.

[DAV 95]

F. Davoine, J. Svensson, J.-M. Chassery, “A mixed triangular
and quadrilateral partition for fractal image coding”. IEEE Int.
Conf. on Image Processing (ICIP’95). También disponible vı́a
[FRE 97].
Presenta un esquema para la segmentación de una imagen, basado en

1

Debe tenerse en cuenta que las referencias disponibles en la red existı́an cuando se
preparó este trabajo: en el momento de acceder a ellas pueden haber cambiado de lugar o
incluso desaparecido.

BIBLIOGRAFÍA

167

triángulos y cuadrilateros, y los correspondientes algoritmos de compresión y descompresión fractal. Los triángulos obtenidos mediante
triangulación de Delaunay permiten reducir el efecto de bloque en la
imagen, pero, debido a que su elevado número puede afectar al ratio
de compresión excesivamente, se procede a continuación a agrupar
triángulos vecinos para formar cuadrilateros convexos.

[DED 98]

Ľ. Dedera, J. Chmúrny, “A parallel approach to image decoding in the fractal image block coding scheme”. Disponible vı́a
[FRE 97].
Artı́culo sencillo que propone un modelo alternativo para el algoritmo
de descodificación de imágenes comprimidas mediante SFI particionados. Este nuevo modelo utiliza una especie de red neuronal recurrente
que necesita el mismo número de iteraciones que el algoritmo iterativo clásico. Este enfoque, sin embargo, facilita la paralelización del
proceso.

[DUG 96]

J.-L. Dugelay, E. Polidori, S. Roche, “Iterated function systems
for still image processing”. Institut EURECOM. Disponible en
http://www.eurecom.fr/~image.
Breve artı́culo que propone dos lı́neas de desarrollo bien diferenciadas.
Por una parte sugiere el uso de rangos no disjuntos en la codificación
de imágenes para mejorar el zoom fractal clásico. Por otro lado se estudia un control jerárquico de acceso a imágenes codificadas mediante
la transformada fractal.

[FIS 92]

Y. Fisher, “Fractal Image Compression”. Siggraph’92 Course Notes. Disponible también vı́a http://inls.ucsd.edu/y/
Fractals.
Una revisión sencilla sobre los matices básicos de la compresión fractal de imágenes. Tras analizar los fundamentos de los SFI con ayuda
de la metáfora de la fotocopiadora de reducciones múltiples, analiza
el algoritmo mı́nimo de codificación de imágenes en escala de grises
mediante SFI particionados. Posteriormente se amplı́a este esquema
proponiendo la obtención de rangos mediante partición con árboles
cuadriculares, partición HV o partición triangular.

[FIS 95]

Y. Fisher (ed.), Fractal Image Compression: Theory and Application, Springer Verlag, 1995.
Una recopilación de artı́culos de diversos autores acerca de la codificación fractal. Es una de las referencias indispensables sobre el tema.
Los tres primeros capı́tulos pueden considerarse una introducción pormenorizada a los SFI, a los SFI recurrentes, a los SFI particionados y

168

BIBLIOGRAFÍA
a la compresión fractal de imágenes, especialmente mediante árboles
cuadriculares. Siguen varios artı́culos en los que se presentan distintas formas de aproximación al tema como pueden ser la codificación
basada en arquetipos, en resultados del álgebra lineal, en cuantización vectorial o en autómatas finitos ponderados. Los apéndices son
casi tan interesantes como el resto del libro; en ellos aparece código
de ejemplo en C para un compresor/descompresor basado en árboles
cuadriculares, ası́ como aspectos todavı́a abiertos o poco investigados
de la compresión fractal. Estos últimos constituyen una valiosa fuente
para nuevos proyectos.

[FRE 97]

Una gran colección de artı́culos sobre compresión fractal
de imágenes puede encontrarse en ftp://ftp.informatik.
uni-freiburg.de/documents/papers/fractal.
Este es uno de los puntos más interesantes para encontrar referencias
actualizadas sobre el tema; cada mes se incorporan varios artı́culos
en formato postscript. Además, se dispone de una extensa bibliografı́a
con muchos más artı́culos de los disponibles en lı́nea.

[FRI 94]

C. Frigaard, J. Gade, T. Hemmingsen, T. Sand, “Image compression based on a fractal theory”. Institute for Electronic Systems, Aalborg University, Denmark, 1994. También disponible
vı́a [FRE 97].
Revisa el esquema original de compresión fractal propuesto por Jacquin, ampliando y mejorando la rutina de clasificación de bloques al
asociar a cada rango un vector de caracterı́sticas continuo y comparar
cada rango únicamente con aquellos dominios cuyos vectores asociados están próximos en el espacio de caracterı́sticas. El texto termina
ofreciendo resultados comparativos entre el algoritmo propuesto y el
estándar JPEG.

[GAI 97]

Jean-loup Gailly, “Frecuently asked questions about compression”. FAQ de comp.compression, Sept. 1997. Disponible en la
dirección ftp://rtfm.mit.edu/pub/usenet/news.answers/
compression-faq.
La respuesta a las preguntas más habituales sobre compresión de datos. Entre muchos otros temas aborda la compresión fractal de imágenes.

[GHA 96]

M. Gharavi-Alkhansari, T. S. Huang, “Generalized image coding using fractal-based methods”. PCS’94, Sacramento, Sept.
94. También disponible vı́a [FRE 97].
Un método distinto al básico de compresión fractal que convierte a

BIBLIOGRAFÍA

169

éste en un caso particular de aquél. Cada bloque se aproxima mediante una combinación lineal de bloques escogidos de un banco de
bloques base. Este banco está formado por a) un conjunto de bloques
tomados de la misma imagen que el bloque a aproximar (con la misma
escala o bien resultantes de submuestrear la imagen) y b) un conjunto
de bloques fijos suministrados inicialmente al codificador. El artı́culo
muestra métodos para escoger los bloques del banco de bloques de
manera que disminuya el error del collage.

[GUZ 93]

M. de Guzmán, M. A. Martı́n, M. Morán, M. Reyes, Estructuras fractales y sus aplicaciones, Labor, 1993.
El primer libro español que aborda el tema a un nivel matemático
profundo. El texto está escrito con un lenguaje claro y abierto y contiene varios programas en BASIC para explorar algunas estructuras
fractales. El capı́tulo 2 discute ampliamente la dimensión de Hausdorff, el capı́tulo 4 aborda la teorı́a de conjuntos autosemejantes de
Hutchinson y el capı́tulo 5 se centra totalmente en los sistemas de
funciones iteradas, incluidos el teorema del collage y la generación de
animaciones mediante SFI.

[HIL 94]

M. L. Hilton, B. D. Jawerth, A. Sengupta, “Compressing still
and moving images with wavelets”. Multimedia Systems, Vol.
2, No. 3. También disponible en ftp://ftp.math.scarolina.
edu:21/pub/wavelet/papers/varia/tutorial.
Explica de manera sencilla la transformada con wavelets y su aplicación a la compresión de imágenes y vı́deo. Pese a su aparente sencillez
contiene información suficiente para poder desarrollar un codificador
de andar por casa. Además, el artı́culo muestra algunos resultados de
la comparación entre el estándar JPEG y varios esquemas de codificación con wavelets.

[KOM 95]

J. Kominek, “Algorithm for fast fractal image compression”.
Proceedings of SPIE, Volume 2419, 1995. También disponible
vı́a [FRE 97].
Este texto propone una técnica para reducir la complejidad temporal
de la compresión fractal. La idea básica es normalizar las intensidades
de los pixels de los rangos para que tengan media y varianza fija y
utilizar, entonces, un árbol r (una extensión de los árboles b) para acceder eficientemente a los dominios estructuralmente cercanos a cada
rango.

[LU 97]

N. Lu, Fractal Imaging, Academic Press, 1997.
Una revisión acutualizada y didáctica de la incursión fractal en la

170

BIBLIOGRAFÍA
compresión de imágenes. Con claridad no exenta de rigor aborda el
modelo fractal de una imagen básico ampliándolo posteriormente mediante el estudio detallado de técnicas de partición, transformaciones
espaciales y de intensidad, búsqueda mediante clasificación o descompresión rápida. El texto dedica también capı́tulos a la compresión con
wavelets o transformada del coseno, compresión de vı́deo o imágenes
en color y codificación mediante entropı́a. Aunque contiene abundante código en C, éste no es totalmente funcional debido a la presencia
de errores, tipográficos normalmente aunque no siempre. De cualquier
forma el estudio detallado de los desarrollos teóricos permite encontrar
con cierta facilidad estos fallos. Se deja leer y se aprende bastante.

[LUT 95]

E. Lutton, J. Lévy Véhel, G. Cretin, P. Glevarec, C. Roll, “Mixed IFS: resolution of the inverse problem using genetic programming”. Institut National de Recherche en Informatique et
en Automatique. Disponible en http://www-syntim.inria.
fr/fractales.
Se enfrenta a la resolución del problema inverso para SFI con un
enfoque muy distinto al de [ALI 91]. El artı́culo considera SFI no afines (SFI mixtos) con distintos operadores y constantes, representando
cada transformación mediante un árbol. La búsqueda del SFI mixto
que mejor aproxima un determinado conjunto se lleva a cabo mediante un esquema de programación genética sobre estos árboles. Estudia,
además, el ajuste de los distintos parámetros del algoritmo evolutivo
y la forma de asegurar la contractividad del sistema obtenido.

[POL 97]

R. Polikar, “The wavelet tutorial”. Disponible en http://www.
public.iastate.edu/~rpolikar/WAVELETS/WTpart1.html.
Un tutorial en lı́nea sobre la teorı́a de los wavelets cargado de imágenes
muy útiles para comprender intuitivamente sus fundamentos. Describe con un enfoque muy ameno y educativo la noción del espectro de
frecuencias de una señal, la transformada de Fourier clásica y a corto
plazo y la transformada con wavelets tanto continua como discreta.

[RUH 97]

M. Ruhl, H. Hartenstein, D. Saupe, “Adaptative partitionings
for fractal image compression”. IEEE Int. Conf. on Image Processing (ICIP’97). También disponible vı́a [FRE 97].
Repasa brevemente el algoritmo evolutivo de segmentación presentado en [SAU 96b] y entra en algunos detalles para una implementación
eficiente no discutidos allı́. El artı́culo continúa con nuevos resultados
y su confrontación con los obtenidos mediante particiones HV.

BIBLIOGRAFÍA

171

[SAU 94a] D. Saupe, R. Hamzaoui, “A guided tour of the fractal image
compression literature”. Technical Report 58, Institut für Informatik, July 94. Una primera versión aparece en las notas del
SIGGRAPH’94. También disponible vı́a [FRE 97].
Este artı́culo presenta un repaso cronológico a la evolución de la compresión fractal de imágenes mediante el análisis de la mayor parte de
los trabajos sobre el tema aparecidos hasta julio de 1994, fecha de edición del artı́culo. Los trabajos más relevantes se presentan, además,
mediante sus abstracts originales. Finalmente, se incluye una extensa bibliografı́a, la mayor publicada hasta ese momento y la principal
contribución del artı́culo según los autores.

[SAU 94b] D. Saupe, R. Hamzaoui, “Complexity reduction methods for
fractal image compression”. IMA Conf. Proc. on Image Processing; Mathematical Methods and Applications, Sept. 94, J. M.
Blackledge (ed.), Oxford University Press, 1997. También disponible vı́a [FRE 97].
Recorre distintos métodos para la aceleración de la búsqueda en la codificación fractal como la clasificación por reordenación de cuadrantes,
la búsqueda del vecino más cercano o distintas técnicas de agrupamiento adaptativo. Incluye una pequeña comparación entre todos los
métodos analizados.

[SAU 95]

D. Saupe, “Accelerating fractal image compression by multidimensional nearest neighbor search”. Proceedings DCC’95 Data Compression Conference, J. A. Storer, M. Cohn (eds.), IEEE
Computer Society Press, March 1995. También disponible vı́a
[FRE 97].
La fase más costosa de la compresión fractal de imágenes es la búsqueda de la región de referencia más parecida (en términos de escalado y
desplazamiento) entre todas las posibles regiones de referencia de la
imagen. Si el número de éstas es N , el tiempo de cada búsqueda lineal
es O(N ). El uso de un algoritmo de clasificación (clustering) inicial
únicamente reduce el factor de proporcionalidad de la complejidad,
pero no su orden. El artı́culo demuestra que la búsqueda secuencial
sobre las regiones de referencia (o una clase de ellas) puede sustituirse
por una búsqueda multidimensional del vecino más cercano, tarea ésta
que puede llevarse a cabo con varios algoritmos de coste O(log N ). El
autor muestra resultados prometedores de una instrumentación mediante arboles kd.

[SAU 96a] D. Saupe, R. Hamzaoui, H. Hartenstein, “Fractal image compression: an introductory overview”. En Fractal Models for Ima-

172

BIBLIOGRAFÍA
ge Synthesis, Compression, and Analysis, D. Saupe, J. Hart
(eds.), ACM SIGGRAPH’96 Course Notes. También disponible
vı́a [FRE 97].
El artı́culo de cabecera de la compresión fractal de imágenes, además
de ser una fuente indispensable junto a [FIS 92] para una primera
aproximación al tema. Analiza de forma excelente la compresión fractal a través de su similitud con la cuantización vectorial con eliminación de media y ganancia de forma. Además, aborda temas avanzados como la reducción del coste temporal, la partición adaptativa o
métodos hı́bridos, haciendo un recorrido por la literatura existente.
Imprescindible.

[SAU 96b] D. Saupe, M. Ruhl, “Evolutionary fractal image compression”.
ICIP-96 IEEE International Conference on Image Processing,
Lausanne, Sept. 1996. También disponible vı́a [FRE 97].
Este artı́culo muestra cómo particionar una imagen en rangos mediante computación evolutiva. El algoritmo se inicia con una partición en
bloques atómicos (por ejemplo, bloques de 4 × 4) y sucesivas generaciones unen pares de ellos para obtener un nuevo rango. Se discuten
también cuatro métodos para codificar eficientemente la partición finalmente obtenida y se presentan resultados que demuestran que el
algoritmo evolutivo supone una gran mejora con ratios de compresión
altos sobre el tradicional esquema basado en árboles cuadriculares.

[SAU 97]

D. Saupe, R. Hamzaoui, “A Bibliography on Fractal Image
Compression”. Disponible en [FRE 97].
La bibliografı́a (sólo la bibliografı́a) aparecida en [SAU 94a] se actualiza regularmente en este documento para incorporar los últimos
trabajos sobre compresión fractal de imágenes. Aunque no es una bibliografı́a comentada, es una buena referencia para seguirle la pista a
los numerosos trabajos que se van publicando sobre el tema.

[SVR 95]

E. R. Vrscay, “A hitchhiker’s guide to fractal-based function
approximation and image compression”. Disponible en http:
//links.uwaterloo.ca/hitchiker.html.
La mayor parte de esta introducción está dedicada a presentar los SFI
sobre funciones u : [0, 1] −→ R+ en lugar de sobre conjuntos, esto
es, sobre imágenes unidimensionales en escala de grises y no sobre las
imágenes habituales (bidimensionales). Ası́ se presenta, principalmente, la aplicación de los SFI a la aproximación de funciones, aunque los
resultados se extrapolan en los últimos párrafos de manera directa a
la compresión de imágenes.

BIBLIOGRAFÍA
[TAY 97]

173

M. C. Taylor, “Frecuently asked questions about fractals”. FAQ
de sci.fractals, Sept. 1997. Disponible en ftp://rtfm.mit.
edu/pub/usenet/news.answers/sci/fractals-faq.
La respuesta a las preguntas más habituales sobre fractales. Su valor
estriba en el gran número de referencias bibliográficas y de Internet
que aporta. Entre otros temas aborda los sistemas de funciones iteradas y la compresión fractal.

[WAL 91]

G. K. Wallace, “The JPEG still picture compression standard”.
Communications of the ACM, April 1991. También disponible
en ftp://ftp.uu.net/graphics/jpeg/wallace.ps.gz.
Aunque no pretende sustituir la necesidad de acceder a la especificación ISO completa a la hora de realizar una implementación, es una
excelente introducción al estándar JPEG para compresión de imágenes. Además de las nociones básicas sobre la compresión mediante la
transformada discreta del coseno, aborda detalladamente los cuatro
modos de operación del estándar: codificación secuencial, progresiva,
sin pérdidas y jerárquica.

Índice alfabético
adherencia, 38
alfabeto, 23
algoritmo
de tiempo de escape, 21
análisis multirresolución, 149
aplicación
continua, 39
contractiva, 39, 56
Arquı́medes, 135
atractor, 64, 68, 78, 107, 108
extraño, 11
autorreferencia, 3
autosemejanza, 3, 99
axioma, 25
Barnsley, M. F., 55
base, método, 85
Besicovitch, 2
bola
abierta, 37
cerrada, 38
Borel, Emile, 136
cadena, 23
cardioide, 18, 19
codificación
aritmetica, 90
de Huffman, 90
del color, 82
jerárquica, 86
por entropı́a, 90, 92, 93
progresiva, 86
secuencial, 86
sin pérdidas, 86
subbanda, 155
colores primarios, 82

componente
alterna de una señal, 88
continua de una señal, 88
compresión
con wavelets, 90, 93
fractal, 93
sin pérdidas, 79
condición
de abierto, 35, 49
de admisibilidad, 154
conjunto
abierto, 38, 39
acotado, 38
autosemejante, 33, 34
cerrado, 38, 39
de Cantor, 2, 3, 5, 17, 27, 35, 46,
53, 64
ternario, 6
de Julia, 15, 25, 77, 142
relleno, 16
de Mandelbrot, 142
interior, 39
invariante, 44, 45, 51
medible Lebesgue, 137
constante
de admisibilidad, 154
de Feigenbaum, 13, 19
convergencia de la descodificación,
112
crominancia, 83
cuantización
escalar, 84, 88, 92, 107
vectorial, 84
con eliminación de media y ganancia de forma, 84, 105
175

176
cuanto, 84, 88, 110
cuerpo paralelo-δ, 46
curva
de Hilbert, 2, 8, 29, 140
de Koch, 2, 8, 29, 35, 36, 53, 60,
62, 137, 141
de Peano, 2, 7, 137
ratio-distorsión, 95
curvas razón-distorsión, 82
CV, véase cuantización vectorial
CV-EMGF, véase cuantización vectorial con eliminación de media y ganancia de forma

ÍNDICE ALFABÉTICO
completo, 39, 40, 47, 55, 56,
102
esponja de Menger, 10
Fatou, Pierre, 15
Feigenbaum, 13
filtro
de Daubechies, 158
digital, 155
ideal, 158
forma normal de Chomsky, 31
funciones de Weierstrass, 9
Gauss, 137
gramáticas
estocásticas, 31
independientes del contexto, 31

descuantización, 89
diagrama de bifurcación, 12
diámetro, 38
Hermite, Charles, 1
dimensión
homotecia, 57
de Hausdorff, 2, 53, 140
H(Rn ), 47, 48
de homotecia, 138
Hutchinson, J. E., 3, 34, 55
fractal, 53, 140
topológica, 137
ı́nfimo, 38
distancia, 37, 38, 45
invarianza de escala, 3
de Hausdorff, 45, 46, 56, 64, 68, isometrı́a, 56
77
iteración generalizada de Lloyd, 84
distribuciones de Wigner, 145
dominio, 102, 105, 108
Jacquin, A., 100
Joint Photographic Expert Group,
duplicación de periodo, 12, 15
85
Jordan, 136
ecuación
JPEG, 85, 93
de Euler, 146
Julia, Gaston, 15, 16
forzada de Duffing, 14
logı́stica, 11
enlosado, 102
error
cuadrático, 109
medio, 101
escala, 150
espacio
∆, 101
de los fractales, 56
métrico, 37
compacto, 39

Lebesgue, Henri, 136
lenguaje, 24
universal, 24
libro de códigos, 84, 105
Lindenmeyer, Aristid, 24, 26
Logo, 27
luminancia, 83
métrica
del supremo, 101
Mandelbrot, Benoit, 2–4, 18

ÍNDICE ALFABÉTICO
matiz, 83
matriz de cuantización, 89
máximo, 38
medida
de Hausdorff, 34
s-dimensional, 139
de Lebesgue, 2, 7, 136, 137
exterior, 136
Menón, 138, 143
métrica, véase distancia
mı́nimo, 38
modelo
de imagen, 101
fractal, 108
módulo de una contracción, 40
morfismo, 25, 29

177
recubrimiento-δ, 139
red de recubrimientos básicos, 51
resolución, 148, 149, 152
retrato fase, 11, 14
RGB, 82, 84
RSRM, véase razón señal-ruido
máxima

saturación, 83
semejanza, 34
sensibilidad a las condiciones iniciales, 11
señal
estacionaria, 146
no estacionaria, 146, 151
SFI, véase sistema de funciones iteradas
N (A, ²), 140
SFIP, véase sistema de funciones iteNm (A), 141
radas particionadas
sı́mbolo, 23
onda madre, 150
sistema
órbita, 16
caótico, 11
D0L, 24–26, 28
palabra, véase cadena
modificado, 27
partición, 100
de autosemejanzas, 42
Peano, 136
de funciones iteradas, 62, 78, 92
Platón, 138, 143
locales, véase sistema de funprincipio
ciones iteradas particionade incertidumbre de Heisenberg,
das
148, 153
particionadas, 100, 102
de Nyquist, 155
de
semejanzas, 43, 44, 51
Prusinkiewicz, P., 29
DT0L, 25, 31
punto fijo de una contracción, 40
L, 24
Smith, A. R., 29
raı́z del error cuadrático medio, 81
Sócrates, 138, 143
ramificación, 29
sombrero mejicano, 152
rango, 102, 105, 108
subaditividad, 136
ratio de compresión, 81
subespacio métrico, 37
razón
submuestreo, 155
de contractividad, 40
sucesión
de un SFI, 62
convergente, 39
de semejanza, 34
de Cauchy, 39, 41
señal-ruido máxima, 81
RECM, véase raı́z del error cuadráti- supermuestreo, 155
co medio
supremo, 38

178

ÍNDICE ALFABÉTICO

TCW, véase transformada continua
madre, véase onda madre
con wavelets
Weierstrass, 9
TDC, véase transformada discreta
del coseno
TDW, véase transformada discreta
con wavelets
horizontal, 91
vertical, 91
teorema
de recuento por cajas, 141
de selección de Blaschke, 47
del collage, 71, 105, 108
del punto fijo, 40, 48, 63, 71, 106
teorı́a geométrica de la medida, 2
teragones, 42
TF, véase transformada de Fourier
TFCP, véase transformada de Fourier a corto plazo
transformación afı́n, 58
transformada
con wavelets, 145, 147, 149, 153
continua
con wavelets, 150
de Fourier, 145
a corto plazo, 145, 147
de Hilbert, 145
discreta
con wavelets, 90, 91, 154
de Fourier, 87
del coseno, 85, 91
del coseno inversa, 87
fractal, 92, 100
triangulación, 135
triángulo
de Sierpinski, 9, 28, 49, 53, 59,
62, 66, 69, 141
generalizado, 66
TW, véase transformada con wavelets
Universidad de Waterloo, 93
Verhulst, 11
wavelet, 150

Vocabulario bilingüe
(inglés-español)
A
adherence
adherencia
admissibility constant
constante de admisibilidad
affine transformation
transformación afı́n
alphabet
alfabeto
arithmetic coding
codificación aritmética
attractor
atractor ,
axiom
axioma
B
baseline method
método base
bifurcation diagram
diagrama de bifurcación
bounded set
conjunto acotado
box counting theorem
teorema de recuento por cajas
brightness adjustment
ajuste de brillo
C
Cantor set
conjunto de Cantor
cardioid

cardioide
Cauchy sequence
sucesión de Cauchy
chaotic dynamical system
sistema dinámico caótico
chrominance
crominancia
closed ball
bola cerrada
cluster
grupo
clustering algorithm
algoritmo de agrupamiento
codebook
libro de códigos
collage theorem
teorema del collage
compact set
conjunto compacto
compactness
compacidad
complete metric space
espacio métrico completo ,
completeness
completitud
compression ratio
ratio de compresión
continuous mapping
aplicación continua
contractive map
aplicación contractiva
contractive mapping fixed-point
theorem
179

180

VOCABULARIO BILINGÜE (INGLÉS-ESPAÑOL)

teorema del punto fijo
contractivity factor
factor de contractividad
razón de contractividad
contrast adjustment
ajuste de contraste
convergent sequence
sucesión convergente

punto fijo
fractal dimension
dimensión fractal
fractal imaging model
modelo de imagen fractal
fractal transform
transformada fractal
H

D
D0L system
sistema D0L
dequantization
descuantización
destination region
región de destino
discrete cosine transform
transformada discreta del coseno
discrete wavelet transform
transformada discreta con wavelets
distance
distancia
domain
dominio

Hausdorff dimension
dimensión de Hausdorff
Hausdorff metric
distancia de Hausdorff
high pass filter
filtro de paso alto
hue
matiz
Huffman coding
codificación de Huffman
I
IFS

SFI, véase iterated function system
image distortion rate
ratio de distorsión de la imagen
E
image template
plantilla de la imagen
entropy coding
infimum
codificación por entropı́a
ı́nfimo
escape time algorithm
intensity component
algoritmo de tiempo de escape
componente de intensidad
eventually contractive map
interior
aplicación eventualmente cnoconjunto interior
tractiva
invariant function
función invariante
F
inverse discrete cosine transform
Feigenbaum’s constant
transformada discreta del coseno
constante de Feigenbaum
inversa
Feigenbaum’s number
iterated function system
constante de Feigenbaum
fixed point
sistema de funciones iteradas

VOCABULARIO BILINGÜE (INGLÉS-ESPAÑOL)
J
Julia set
conjunto de Julia
K
Koch curve
curva de Koch
L
L system
sistema L
language
lenguaje
Lebesgue measure
medida de Lebesgue
losless compression
compresión sin pérdidas
lossy compression
compresión con pérdidas
low pass filter
filtro de paso bajo
luminance
luminancia
M
maximum
máximo
metric
métrica
metric space
espacio métrico
minimum
mı́nimo
mother wavelet
onda madre
multiresolution analysis
análisis multirresolución
O
open ball
bola abierta

181

open set condition
condición de abierto
P
partitioned iterated function
system
sistema de funciones iteradas
particionadas
peak-to-peak signal-to-noise ratio
razón señal-ruido máxima
period-dubling
duplicación de periodo
pixel chaining
encadenamiento de pixels
PSNR
RSRM, véase peak-to-peak
signal-to-noise ratio
Q
quadtree partition
partición mediante árboles cuadriculares
quantization
cuantización
quantization step
cuanto
R
range
rango
rate-distorsion curves
curvas razón-distorsión
recurrent iterated function system
sistema de funciones iteradas recurrente
reference region
región de referencia
resolution
resolución
rms error

182

VOCABULARIO BILINGÜE (INGLÉS-ESPAÑOL)

RECM, véase root mean square
enlosado
error
topological dimension
dimensión topológica
root mean square error
raı́z del error cuadrático medio
U
S
uncertainty principle
principio de incertidumbre
saturation
upsample
saturación
supermuestreo
scalar quantization
cuantización escalar
V
scale
escala
vector quantization
scale invariance
cuantización vectorial
invarianza de escala
VQ
self-reference
CV, véase vector quantization
autorreferencia
self-similarity
autosemejanza
autosimilitud
short time Fourier transform
transformada de Fourier a corto
plazo
Sierpinski triangle
triángulo de Sierpinski
similitude
semejanza
spatial component
componente espacial
strange attractor
atractor extraño
string
cadena
subband coding
codificación subbanda
subsample
submuestreo
supremum
supremo
T
threshold
umbral
tiling


